[
  {
    "title": "Safety and robustness for deep learning with provable guarantees",
    "author": "Kwiatkowska, Marta",
    "abstract": "Computing systems are becoming ever more complex, with decisions increasingly often based on deep learning components. A wide variety of applications are being developed, many of them safety-critical, such as self-driving cars and medical diagnosis. Since deep learning is unstable with respect to adversarial perturbations, there is a need for rigorous software development methodologies that encompass machine learning components. This lecture will describe progress with developing automated verification and testing techniques for deep neural networks to ensure safety and robustness of their decisions with respect to bounded input perturbations. The techniques exploit Lipschitz continuity of the networks and aim to approximate, for a given set of inputs, the reachable set of network outputs in terms of lower and upper bounds, in anytime manner, with provable guarantees. We develop novel algorithms based on feature-guided search, games, global optimisation and Bayesian methods, and evaluate them on state-of-the-art networks. The lecture will conclude with an overview of the challenges in this field.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418901",
    "comment": ""
  },
  {
    "title": "Is software engineering research addressing software engineering problems? (keynote)",
    "author": "Murphy, Gail C.",
    "abstract": "Brian Randell described software engineering as \"the multi-person development of multi-version programs\". David Parnas expressed that this \"pithy phrase implies everything that differentiates software engineering from other programming\" (Parnas, 2011). How does current software engineering research compare against this definition? Is there too much focus currently on research into problems and techniques more associated with programming than software engineering? Are there opportunities to use Randell's description of software engineering to guide the community to new research directions? In this extended abstract, I motivate the keynote, which explores these questions and discusses how a consideration of the development streams used by multiple individuals to produce multiple versions of software opens up new avenues for impactful software engineering research.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3417103",
    "comment": ""
  },
  {
    "title": "Accelerating all-SAT computation with short blocking clauses",
    "author": "Zhang, Yueling and Pu, Geguang and Sun, Jun",
    "abstract": "The All-SAT (All-SATisfiable) problem focuses on finding all satisfiable assignments of a given propositional formula, whose applications include model checking, automata construction, and logic minimization. A typical ALL-SAT solver is normally based on iteratively computing satisfiable assignments of the given formula. In this work, we introduce BASolver, a backbone-based All-SAT solver for propositional formulas. Compared to the existing approaches, BASolver generates shorter blocking clauses by removing backbone variables from the partial assignments and the blocking clauses. We compare BASolver with 4 existing ALL-SAT solvers, namely MBlocking, BC, BDD, and NBC. Experimental results indicate that although finding all the backbone variables consumes additional computing time, BASolver is still more efficient than the existing solvers because of the shorter blocking clauses and the backbone variables used in it.With the 608 formulas, BASolver solves the largest amount of formulas (86), which is 22\\%, 36\\%, 68\\%, 86\\% more formulas than MBlocking, BC, NBC, and BDD respectively. For the formulas that are both solved by BASolver and the other solvers, BASolver uses 88.4\\% less computing time on average than the other solvers. For the 215 formulas which first 1000 satisfiable assignments are found by at least one of the solvers, BASolver uses 180\\% less computing time on average than the other solvers.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416569",
    "comment": ""
  },
  {
    "title": "A predictive analysis for detecting deadlock in MPI programs",
    "author": "Huang, Yu and Ogles, Benjamin and Mercer, Eric",
    "abstract": "A common problem in MPI programs is deadlock: when two or more processes are blocked indefinitely due to a circular communication dependency. Automatically detecting deadlock is difficult due to its schedule-dependent nature. This paper presents a predictive analysis for single-path MPI programs that observes a single program execution and then determines whether any other feasible schedule of the program can lead to a deadlock. The analysis works by identifying problematic communication patterns in a dependency graph to form a set of deadlock candidates. The deadlock candidates are filtered by an abstract machine and ultimately tested for reachability by an SMT solver with an efficient encoding for deadlock. This approach quickly yields a set of high probability deadlock candidates useful for reasoning about complex codes and yields higher performance overall in many cases compared to other state-of-the-art analyses. The analysis is sound and complete for single-path MPI programs on a given input.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416588",
    "comment": ""
  },
  {
    "title": "Learning to handle exceptions",
    "author": "Zhang, Jian and Wang, Xu and Zhang, Hongyu and Sun, Hailong and Pu, Yanjun and Liu, Xudong",
    "abstract": "Exception handling is an important built-in feature of many modern programming languages such as Java. It allows developers to deal with abnormal or unexpected conditions that may occur at runtime in advance by using try-catch blocks. Missing or improper implementation of exception handling can cause catastrophic consequences such as system crash. However, previous studies reveal that developers are unwilling or feel it hard to adopt exception handling mechanism, and tend to ignore it until a system failure forces them to do so. To help developers with exception handling, existing work produces recommendations such as code examples and exception types, which still requires developers to localize the try blocks and modify the catch block code to fit the context. In this paper, we propose a novel neural approach to automated exception handling, which can predict locations of try blocks and automatically generate the complete catch blocks. We collect a large number of Java methods from GitHub and conduct experiments to evaluate our approach. The evaluation results, including quantitative measurement and human evaluation, show that our approach is highly effective and outperforms all baselines. Our work makes one step further towards automated exception handling.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416568",
    "comment": ""
  },
  {
    "title": "BuildFast: history-aware build outcome prediction for fast feedback and reduced cost in continuous integration",
    "author": "Chen, Bihuan and Chen, Linlin and Zhang, Chen and Peng, Xin",
    "abstract": "Long build times in continuous integration (CI) can greatly increase the cost in human and computing resources, and thus become a common barrier faced by software organizations adopting CI. Build outcome prediction has been proposed as one of the remedies to reduce such cost. However, the state-of-the-art approaches have a poor prediction performance for failed builds, and are not designed for practical usage scenarios. To address the problems, we first conduct an empirical study on 2,590,917 builds to characterize build times in real-world projects, and a survey with 75 developers to understand their perceptions about build outcome prediction. Then, motivated by our study and survey results, we propose a new history-aware approach, named BuildFast, to predict CI build outcomes cost-efficiently and practically. We develop multiple failure-specific features from closely related historical builds via analyzing build logs and changed files, and propose an adaptive prediction model to switch between two models based on the build outcome of the previous build. We investigate a practical online usage scenario of BuildFast, where builds are predicted in chronological order, and measure the benefit from correct predictions and the cost from incorrect predictions. Our experiments on 20 projects have shown that BuildFast improved the state-of-the-art by 47.5\\% in F1-score for failed builds.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416616",
    "comment": ""
  },
  {
    "title": "Legion: best-first concolic testing",
    "author": "Liu, Dongge and Ernst, Gidon and Murray, Toby and Rubinstein, Benjamin I. P.",
    "abstract": "Concolic execution and fuzzing are two complementary coverage-based testing techniques. How to achieve the best of both remains an open challenge. To address this research problem, we propose and evaluate Legion. Legion re-engineers the Monte Carlo tree search (MCTS) framework from the AI literature to treat automated test generation as a problem of sequential decision-making under uncertainty. Its best-first search strategy provides a principled way to learn the most promising program states to investigate at each search iteration, based on observed rewards from previous iterations. Legion incorporates a form of directed fuzzing that we call approximate path-preserving fuzzing (APPFuzzing) to investigate program states selected by MCTS. APPFuzzing serves as the Monte Carlo simulation technique and is implemented by extending prior work on constrained sampling. We evaluate Legion against competitors on 2531 benchmarks from the coverage category of Test-Comp 2020, as well as measuring its sensitivity to hyperparameters, demonstrating its effectiveness on a wide variety of input programs.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416629",
    "comment": ""
  },
  {
    "title": "Plug the database \\& play with automatic testing: improving system testing by exploiting persistent data",
    "author": "Clerissi, Diego and Denaro, Giovanni and Mobilio, Marco and Mariani, Leonardo",
    "abstract": "A key challenge in automatic Web testing is the generation of syntactically and semantically valid input values that can exercise the many functionalities that impose constraints on the validity of the inputs. Existing test case generation techniques either rely on manually curated catalogs of values, or extract values from external data sources, such as the Web or publicly available knowledge bases. Unfortunately, relying on manual effort is generally too expensive for most practical applications, while domain-specific and application-specific data can be hardly found either on the Web or in general purpose knowledge bases.This paper proposes DBInputs, a novel approach that reuses the data from the database of the target Web application, to automatically identify domain-specific and application-specific inputs, and effectively fulfill the validity constraints present in the tested Web pages. DBInputs can properly cope with system testing and maintenance testing efforts, since databases are naturally and inexpensively available in those phases. To extract valid inputs from the application databases, DBInputs exploits the syntactic and semantic similarity between the identifiers of the input fields and the ones in the tables of the database, automatically resolving the mismatch between the user interface and the schema of the database. Our experiments provide initial evidence that DBInputs can outperform both random input selection and Link, a competing approach for searching inputs from knowledge bases.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416561",
    "comment": ""
  },
  {
    "title": "Enhanced compiler bug isolation via memoized search",
    "author": "Chen, Junjie and Ma, Haoyang and Zhang, Lingming",
    "abstract": "Compiler bugs can be disastrous since they could affect all the software systems built on the buggy compilers. Meanwhile, diagnosing compiler bugs is extremely challenging since usually limited debugging information is available and a large number of compiler files can be suspicious. More specifically, when compiling a given bug-triggering test program, hundreds of compiler files are usually involved, and can all be treated as suspicious buggy files. To facilitate compiler debugging, in this paper we propose the first reinforcement compiler bug isolation approach via structural mutation, called RecBi. For a given bug-triggering test program, RecBi first augments traditional local mutation operators with structural ones to transform it into a set of passing test programs. Since not all the passing test programs can help isolate compiler bugs effectively, RecBi further leverages reinforcement learning to intelligently guide the process of passing test program generation. Then, RecBi ranks all the suspicious files by analyzing the compiler execution traces of the generated passing test programs and the given failing test program following the practice of compiler bug isolation. The experimental results on 120 real bugs from two most popular C open-source compilers, i.e., GCC and LLVM, show that RecBi is able to isolate about 23\\%/58\\%/78\\% bugs within Top-1/Top-5/Top-10 compiler files, and significantly outperforms the state-of-the-art compiler bug isolation approach by improving 92.86\\%/55.56\\%/25.68\\% isolation effectiveness in terms of Top-1/Top-5/Top-10 results.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416570",
    "comment": ""
  },
  {
    "title": "M3: semantic API migrations",
    "author": "Collie, Bruce and Ginsbach, Philip and Woodruff, Jackson and Rajan, Ajitha and O'Boyle, Michael F. P.",
    "abstract": "Library migration is a challenging problem, where most existing approaches rely on prior knowledge. This can be, for example, information derived from changelogs or statistical models of API usage.This paper addresses a different API migration scenario where there is no prior knowledge of the target library. We have no historical changelogs and no access to its internal representation. To tackle this problem, this paper proposes a novel approach (M3), where probabilistic program synthesis is used to semantically model the behavior of library functions. Then, we use an SMT-based code search engine to discover similar code in user applications. These discovered instances provide potential locations for API migrations.We evaluate our approach against 7 well-known libraries from varied application domains, learning correct implementations for 94 functions. Our approach is integrated with standard compiler tooling, and we use this integration to evaluate migration opportunities in 9 existing C/C++ applications with over 1MLoC. We discover over 7,000 instances of these functions, of which more than 2,000 represent migration opportunities.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416618",
    "comment": ""
  },
  {
    "title": "The impact of generic data structures: decoding the role of lists in the linux kernel",
    "author": "Volanschi, Nic and Lawall, Julia",
    "abstract": "The increasing adoption of the Linux kernel has been sustained by a large and constant maintenance effort, performed by a wide and heterogeneous base of contributors. One important problem that maintainers face in any code base is the rapid understanding of complex data structures. The Linux kernel is written in the C language, which enables the definition of arbitrarily uninformative datatypes, via the use of casts and pointer arithmetic, of which doubly linked lists are a prominent example. In this paper, we explore the advantages and disadvantages of such lists, for expressivity, for code understanding, and for code reliability. Based on our observations, we have developed a toolset that includes inference of descriptive list types and a tool for list visualization. Our tools identify more than 10,000 list fields and variables in recent Linux kernel releases and succeeds in typing 90\\%. We show how these tools could have been used to detect previously fixed bugs and identify 6 new ones.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416635",
    "comment": ""
  },
  {
    "title": "Pending constraints in symbolic execution for better exploration and seeding",
    "author": "Kapus, Timotej and Busse, Frank and Cadar, Cristian",
    "abstract": "Symbolic execution is a well established technique for software testing and analysis. However, scalability continues to be a challenge, both in terms of constraint solving cost and path explosion. In this work, we present a novel approach for symbolic execution, which can enhance its scalability by aggressively prioritising execution paths that are already known to be feasible, and deferring all other paths. We evaluate our technique on nine applications, including SQLite3, make and tcpdump and show it can achieve higher coverage for both seeded and non-seeded exploration.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416589",
    "comment": ""
  },
  {
    "title": "Broadening horizons of multilingual static analysis: semantic summary extraction from C code for JNI program analysis",
    "author": "Lee, Sungho and Lee, Hyogun and Ryu, Sukyoung",
    "abstract": "Most programming languages support foreign language interoperation that allows developers to integrate multiple modules implemented in different languages into a single multilingual program. While utilizing various features from multiple languages expands expressivity, differences in language semantics require developers to understand the semantics of multiple languages and their inter-operation. Because current compilers do not support compile-time checking for interoperation, they do not help developers avoid interoperation bugs. Similarly, active research on static analysis and bug detection has been focusing on programs written in a single language.In this paper, we propose a novel approach to analyze multilingual programs statically. Unlike existing approaches that extend a static analyzer for a host language to support analysis of foreign function calls, our approach extracts semantic summaries from programs written in guest languages using a modular analysis technique, and performs a whole-program analysis with the extracted semantic summaries. To show practicality of our approach, we design and implement a static analyzer for multilingual programs, which analyzes JNI interoperation between Java and C. Our empirical evaluation shows that the analyzer is scalable in that it can construct call graphs for large programs that use JNI interoperation, and useful in that it found 74 genuine interoperation bugs in real-world Android JNI applications.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416558",
    "comment": ""
  },
  {
    "title": "Assessing and restoring reproducibility of Jupyter notebooks",
    "author": "Wang, Jiawei and Kuo, Tzu-yang and Li, Li and Zeller, Andreas",
    "abstract": "Jupyter notebooks---documents that contain live code, equations, visualizations, and narrative text---now are among the most popular means to compute, present, discuss and disseminate scientific findings. In principle, Jupyter notebooks should easily allow to reproduce and extend scientific computations and their findings; but in practice, this is not the case. The individual code cells in Jupyter notebooks can be executed in any order, with identifier usages preceding their definitions and results preceding their computations. In a sample of 936 published notebooks that would be executable in principle, we found that 73\\% of them would not be reproducible with straightforward approaches, requiring humans to infer (and often guess) the order in which the authors created the cells.In this paper, we present an approach to (1) automatically satisfy dependencies between code cells to reconstruct possible execution orders of the cells; and (2) instrument code cells to mitigate the impact of non-reproducible statements (i.e., random functions) in Jupyter notebooks. Our Osiris prototype takes a notebook as input and outputs the possible execution schemes that reproduce the exact notebook results. In our sample, Osiris was able to reconstruct such schemes for 82.23\\% of all executable notebooks, which has more than three times better than the state-of-the-art; the resulting reordered code is valid program code and thus available for further testing and analysis.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416585",
    "comment": ""
  },
  {
    "title": "Verified from scratch: program analysis for learners' programs",
    "author": "Stahlbauer, Andreas and Fr\\\"{a}drich, Christoph and Fraser, Gordon",
    "abstract": "Block-based programming languages like Scratch support learners by providing high-level constructs that hide details and by preventing syntactically incorrect programs. Questions nevertheless frequently arise: Is this program satisfying the given task? Why is my program not working? To support learners and educators, automated program analysis is needed for answering such questions. While adapting existing analyses to process blocks instead of textual statements is straightforward, the domain of programs controlled by block-based languages like Scratch is very different from traditional programs: In Scratch multiple actors, represented as highly concurrent programs, interact on a graphical stage, controlled by user inputs, and while the block-based program statements look playful, they hide complex mathematical operations that determine visual aspects and movement. Analyzing such programs is further hampered by the absence of clearly defined semantics, often resulting from ad-hoc decisions made by the implementers of the programming environment. To enable program analysis, we define the semantics of Scratch using an intermediate language. Based on this intermediate language, we implement the Bastet program analysis framework for Scratch programs, using concepts from abstract interpretation and software model checking. Like Scratch, Bastet is based on Web technologies, written in TypeScript, and can be executed using NodeJS or even directly in a browser. Evaluation on 279 programs written by children suggests that Bastet offers a practical solution for analysis of Scratch programs, thus enabling applications such as automated hint generation, automated evaluation of learner progress, or automated grading.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416554",
    "comment": ""
  },
  {
    "title": "Interval change-point detection for runtime probabilistic model checking",
    "author": "Zhao, Xingyu and Calinescu, Radu and Gerasimou, Simos and Robu, Valentin and Flynn, David",
    "abstract": "Recent probabilistic model checking techniques can verify reliability and performance properties of software systems affected by parametric uncertainty. This involves modelling the system behaviour using interval Markov chains, i.e., Markov models with transition probabilities or rates specified as intervals. These intervals can be updated continually using Bayesian estimators with imprecise priors, enabling the verification of the system properties of interest at runtime. However, Bayesian estimators are slow to react to sudden changes in the actual value of the estimated parameters, yielding inaccurate intervals and leading to poor verification results after such changes. To address this limitation, we introduce an efficient interval change-point detection method, and we integrate it with a state-of-the-art Bayesian estimator with imprecise priors. Our experimental results show that the resulting end-to-end Bayesian approach to change-point detection and estimation of interval Markov chain parameters handles effectively a wide range of sudden changes in parameter values, and supports runtime probabilistic model checking under parametric uncertainty.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416565",
    "comment": ""
  },
  {
    "title": "UnchartIt: an interactive framework for program recovery from charts",
    "author": "Ramos, Daniel and Pereira, Jorge and Lynce, In\\^{e}s and Manquinho, Vasco and Martins, Ruben",
    "abstract": "Charts are commonly used for data visualization. Generating a chart usually involves performing data transformations, including data pre-processing and aggregation. These tasks can be cumbersome and time-consuming, even for experienced data scientists. Reproducing existing charts can also be a challenging task when information about data transformations is no longer available.In this paper, we tackle the problem of recovering data transformations from existing charts. Given an input table and a chart, our goal is to automatically recover the data transformation program underlying the chart. We divide our approach into four steps: (1) data extraction, (2) candidate generation, (3) candidate ranking, and (4) candidate disambiguation. We implemented our approach in a tool called UnchartIt and evaluated it on a set of 50 benchmarks from Kaggle. Experimental results show that UnchartIt successfully ranks the correct data transformation among the top-10 programs in 92\\% of the benchmarks. To disambiguate the top-ranking programs, we use our new interactive procedure, which successfully disambiguates 98\\% of the ambiguous benchmarks by asking on average fewer than 2 questions to the user.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416613",
    "comment": ""
  },
  {
    "title": "Demystifying diehard Android apps",
    "author": "Zhou, Hao and Wang, Haoyu and Zhou, Yajin and Luo, Xiapu and Tang, Yutian and Xue, Lei and Wang, Ting",
    "abstract": "Smartphone vendors are using multiple methods to kill processes of Android apps to reduce the battery consumption. This motivates developers to find ways to extend the liveness time of their apps, hence the name diehard apps in this paper. Although there are blogs and articles illustrating methods to achieve this purpose, there is no systematic research about them. What's more important, little is known about the prevalence of diehard apps in the wild.In this paper, we take a first step to systematically investigate diehard apps by answering the following research questions. First, why and how can they circumvent the resource-saving mechanisms of Android? Second, how prevalent are they in the wild? In particular, we conduct a semi-automated analysis to illustrate insights why existing methods to kill app processes could be evaded, and then systematically present 12 diehard methods. After that, we develop a system named DiehardDetector to detect diehard apps in a large scale. The experimental result of applying DiehardDetector to more than 80k Android apps downloaded from Google Play showed that around 21\\% of apps adopt various diehard methods. Moreover, our system can achieve high precision and recall.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416637",
    "comment": ""
  },
  {
    "title": "UI obfuscation and its effects on automated UI analysis for Android apps",
    "author": "Zhou, Hao and Chen, Ting and Wang, Haoyu and Yu, Le and Luo, Xiapu and Wang, Ting and Zhang, Wei",
    "abstract": "The UI driven nature of Android apps has motivated the development of automated UI analysis for various purposes, such as app analysis, malicious app detection, and app testing. Although existing automated UI analysis methods have demonstrated their capability in dissecting apps' UI, little is known about their effectiveness in the face of app protection techniques, which have been adopted by more and more apps. In this paper, we take a first step to systematically investigate UI obfuscation for Android apps and its effects on automated UI analysis. In particular, we point out the weaknesses in existing automated UI analysis methods and design 9 UI obfuscation approaches. We implement these approaches in a new tool named UIObfuscator after tackling several technical challenges. Moreover, we feed 3 kinds of tools that rely on automated UI analysis with the apps protected by UIObfuscator, and find that their performances severely drop. This work reveals limitations of automated UI analysis and sheds light on app protection techniques.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416642",
    "comment": ""
  },
  {
    "title": "Good things come in threes: improving search-based crash reproduction with helper objectives",
    "author": "Derakhshanfar, Pouria and Devroey, Xavier and Zaidman, Andy and van Deursen, Arie and Panichella, Annibale",
    "abstract": "Writing a test case reproducing a reported software crash is a common practice to identify the root cause of an anomaly in the software under test. However, this task is usually labor-intensive and time-taking. Hence, evolutionary intelligence approaches have been successfully applied to assist developers during debugging by generating a test case reproducing reported crashes. These approaches use a single fitness function called Crash Distance to guide the search process toward reproducing a target crash. Despite the reported achievements, these approaches do not always successfully reproduce some crashes due to a lack of test diversity (premature convergence). In this study, we introduce a new approach, called MO-HO, that addresses this issue via multi-objectivization. In particular, we introduce two new Helper-Objectives for crash reproduction, namely test length (to minimize) and method sequence diversity (to maximize), in addition to Crash Distance. We assessed MO-HO using five multi-objective evolutionary algorithms (NSGA-II, SPEA2, PESA-II, MOEA/D, FEMO) on 124 non-trivial crashes stemming from open-source projects. Our results indicate that SPEA2 is the best-performing multi-objective algorithm for MO-HO. We evaluated this best-performing algorithm for MO-HO against the state-of-the-art: single-objective approach (Single-Objective Search) and decomposition-based multi-objectivization approach (De-MO). Our results show that MO-HO reproduces five crashes that cannot be reproduced by the current state-of-the-art. Besides, MO-HO improves the effectiveness (+10\\% and +8\\% in reproduction ratio) and the efficiency in 34.6\\% and 36\\% of crashes (i.e., significantly lower running time) compared to Single-Objective Search and De-MO, respectively. For some crashes, the improvements are very large, being up to +93.3\\% for reproduction ratio and -92\\% for the required running time.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416643",
    "comment": ""
  },
  {
    "title": "Subdomain-based generality-aware debloating",
    "author": "Xin, Qi and Kim, Myeongsoo and Zhang, Qirun and Orso, Alessandro",
    "abstract": "Programs are becoming increasingly complex and typically contain an abundance of unneeded features, which can degrade the performance and security of the software. Recently, we have witnessed a surge of debloating techniques that aim to create a reduced version of a program by eliminating the unneeded features therein. To debloat a program, most existing techniques require a usage profile of the program, typically provided as a set of inputs I. Unfortunately, these techniques tend to generate a reduced program that is over-fitted to I and thus fails to behave correctly for other inputs. To address this limitation, we propose DomGad, which has two main advantages over existing debloating approaches. First, it produces a reduced program that is guaranteed to work for subdomains, rather than for specific inputs. Second, it uses stochastic optimization to generate reduced programs that achieve a close-to-optimal tradeoff between reduction and generality (i.e., the extent to which the reduced program is able to correctly handle inputs in its whole domain). To assess the effectiveness of DomGad, we applied our approach to a benchmark of ten Unix utility programs. Our results are promising, as they show that DomGad could produce debloated programs that achieve, on average, 50\\% code reduction and 95\\% generality. Our results also show that DomGad performs well when compared with two state-of-the-art debloating approaches.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416644",
    "comment": ""
  },
  {
    "title": "Revisiting the relationship between fault detection, test adequacy criteria, and test set size",
    "author": "Chen, Yiqun T. and Gopinath, Rahul and Tadakamalla, Anita and Ernst, Michael D. and Holmes, Reid and Fraser, Gordon and Ammann, Paul and Just, Ren\\'{e}",
    "abstract": "The research community has long recognized a complex interrelationship between fault detection, test adequacy criteria, and test set size. However, there is substantial confusion about whether and how to experimentally control for test set size when assessing how well an adequacy criterion is correlated with fault detection and when comparing test adequacy criteria. Resolving the confusion, this paper makes the following contributions: (1) A review of contradictory analyses of the relationships between fault detection, test adequacy criteria, and test set size. Specifically, this paper addresses the supposed contradiction of prior work and explains why test set size is neither a confounding variable, as previously suggested, nor an independent variable that should be experimentally manipulated. (2) An explication and discussion of the experimental designs of prior work, together with a discussion of conceptual and statistical problems, as well as specific guidelines for future work. (3) A methodology for comparing test adequacy criteria on an equal basis, which accounts for test set size without directly manipulating it through unrealistic stratification. (4) An empirical evaluation that compares the effectiveness of coverage-based testing, mutation-based testing, and random testing. Additionally, this paper proposes probabilistic coupling, a methodology for assessing the representativeness of a set of test goals for a given fault and for approximating the fault-detection probability of adequate test sets.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416667",
    "comment": ""
  },
  {
    "title": "Synthesis of infinite-state systems with random behavior",
    "author": "Katis, Andreas and Fedyukovich, Grigory and Chen, Jeffrey and Greve, David and Rayadurgam, Sanjai and Whalen, Michael W.",
    "abstract": "Diversity in the exhibited behavior of a given system is a desirable characteristic in a variety of application contexts. Synthesis of conformant implementations often proceeds by discovering witnessing Skolem functions, which are traditionally deterministic. In this paper, we present a novel Skolem extraction algorithm to enable synthesis of witnesses with random behavior and demonstrate its applicability in the context of reactive systems. The synthesized solutions are guaranteed by design to meet the given specification, while exhibiting a high degree of diversity in their responses to external stimuli. Case studies demonstrate how our proposed framework unveils a novel application of synthesis in model-based fuzz testing to generate fuzzers of competitive performance to general-purpose alternatives, as well as the practical utility of synthesized controllers in robot motion planning problems.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416586",
    "comment": ""
  },
  {
    "title": "Demystifying loops in smart contracts",
    "author": "Mariano, Benjamin and Chen, Yanju and Feng, Yu and Lahiri, Shuvendu K. and Dillig, Isil",
    "abstract": "This paper aims to shed light on how loops are used in smart contracts. Towards this goal, we study various syntactic and semantic characteristics of loops used in over 20,000 Solidity contracts deployed on the Ethereum blockchain, with the goal of informing future research on program analysis for smart contracts. Based on our findings, we propose a small domain-specific language (DSL) that can be used to summarize common looping patterns in Solidity. To evaluate what percentage of smart contract loops can be expressed in our proposed DSL, we also design and implement a program synthesis toolchain called Solis that can synthesize loop summaries in our DSL. Our evaluation shows that at least 56\\% of the analyzed loops can be summarized in our DSL, and 81\\% of these summaries are exactly equivalent to the original loop.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416626",
    "comment": ""
  },
  {
    "title": "Patching as translation: the data and the metaphor",
    "author": "Ding, Yangruibo and Ray, Baishakhi and Devanbu, Premkumar and Hellendoorn, Vincent J.",
    "abstract": "Machine Learning models from other fields, like Computational Linguistics, have been transplanted to Software Engineering tasks, often quite successfully. Yet a transplanted model's initial success at a given task does not necessarily mean it is well-suited for the task. In this work, we examine a common example of this phenomenon: the conceit that \"software patching is like language translation\". We demonstrate empirically that there are subtle, but critical distinctions between sequence-to-sequence models and translation model: while program repair benefits greatly from the former, general modeling architecture, it actually suffers from design decisions built into the latter, both in terms of translation accuracy and diversity. Given these findings, we demonstrate how a more principled approach to model design, based on our empirical findings and general knowledge of software development, can lead to better solutions. Our findings also lend strong support to the recent trend towards synthesizing edits of code conditional on the buggy context, to repair bugs. We implement such models ourselves as \"proof-of-concept\" tools and empirically confirm that they behave in a fundamentally different, more effective way than the studied translation-based architectures. Overall, our results demonstrate the merit of studying the intricacies of machine learned models in software engineering: not only can this help elucidate potential issues that may be overshadowed by increases in accuracy; it can also help innovate on these models to raise the state-of-the-art further. We will publicly release our replication data and materials at https://github.com/ARiSE-Lab/Patch-as-translation.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416587",
    "comment": ""
  },
  {
    "title": "DeepTC-enhancer: improving the readability of automatically generated tests",
    "author": "Roy, Devjeet and Zhang, Ziyi and Ma, Maggie and Arnaoudova, Venera and Panichella, Annibale and Panichella, Sebastiano and Gonzalez, Danielle and Mirakhorli, Mehdi",
    "abstract": "Automated test case generation tools have been successfully proposed to reduce the amount of human and infrastructure resources required to write and run test cases. However, recent studies demonstrate that the readability of generated tests is very limited due to (i) uninformative identifiers and (ii) lack of proper documentation. Prior studies proposed techniques to improve test readability by either generating natural language summaries or meaningful methods names. While these approaches are shown to improve test readability, they are also affected by two limitations: (1) generated summaries are often perceived as too verbose and redundant by developers, and (2) readable tests require both proper method names but also meaningful identifiers (within-method readability).In this work, we combine template based methods and Deep Learning (DL) approaches to automatically generate test case scenarios (elicited from natural language patterns of test case statements) as well as to train DL models on path-based representations of source code to generate meaningful identifier names. Our approach, called DeepTC-Enhancer, recommends documentation and identifier names with the ultimate goal of enhancing readability of automatically generated test cases.An empirical evaluation with 36 external and internal developers shows that (1) DeepTC-Enhancer outperforms significantly the baseline approach for generating summaries and performs equally with the baseline approach for test case renaming, (2) the transformation proposed by DeepTC-Enhancer results in a significant increase in readability of automatically generated test cases, and (3) there is a significant difference in the feature preferences between external and internal developers.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416622",
    "comment": ""
  },
  {
    "title": "Hybrid deep neural networks to infer state models of black-box systems",
    "author": "Mashhadi, Mohammad Jafar and Hemmati, Hadi",
    "abstract": "Inferring behavior model of a running software system is quite useful for several automated software engineering tasks, such as program comprehension, anomaly detection, and testing. Most existing dynamic model inference techniques are white-box, i.e., they require source code to be instrumented to get run-time traces. However, in many systems, instrumenting the entire source code is not possible (e.g., when using black-box third-party libraries) or might be very costly. Unfortunately, most black-box techniques that detect states over time are either univariate, or make assumptions on the data distribution, or have limited power for learning over a long period of past behavior. To overcome the above issues, in this paper, we propose a hybrid deep neural network that accepts as input a set of time series, one per input/output signal of the system, and applies a set of convolutional and recurrent layers to learn the non-linear correlations between signals and the patterns, over time. We have applied our approach on a real UAV auto-pilot solution from our industry partner with half a million lines of C code. We ran 888 random recent system-level test cases and inferred states, over time. Our comparison with several traditional time series change point detection techniques showed that our approach improves their performance by up to 102\\%, in terms of finding state change points, measured by F1 score. We also showed that our state classification algorithm provides on average 90.45\\% F1 score, which improves traditional classification algorithms by up to 17\\%.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416559",
    "comment": ""
  },
  {
    "title": "Representing and reasoning about dynamic code",
    "author": "Bartels, Jesse and Stephens, Jon and Debray, Saumya",
    "abstract": "Dynamic code, i.e., code that is created or modified at runtime, is ubiquitous in today's world. The behavior of dynamic code can depend on the logic of the dynamic code generator in subtle and non-obvious ways, e.g., JIT compiler bugs can lead to exploitable vulnerabilities in the resulting JIT-compiled code. Existing approaches to program analysis do not provide adequate support for reasoning about such behavioral relationships. This paper takes a first step in addressing this problem by describing a program representation and a new notion of dependency that allows us to reason about dependency and information flow relationships between the dynamic code generator and the generated dynamic code. Experimental results show that analyses based on these concepts are able to capture properties of dynamic code that cannot be identified using traditional program analyses.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416542",
    "comment": ""
  },
  {
    "title": "ER catcher: a static analysis framework for accurate and scalable event-race detection in Android",
    "author": "Salehnamadi, Navid and Alshayban, Abdulaziz and Ahmed, Iftekhar and Malek, Sam",
    "abstract": "Android platform provisions a number of sophisticated concurrency mechanisms for the development of apps. The concurrency mechanisms, while powerful, are quite difficult to properly master by mobile developers. In fact, prior studies have shown concurrency issues, such as event-race defects, to be prevalent among real-world Android apps. In this paper, we propose a flow-, context-, and thread-sensitive static analysis framework, called ER Catcher, for detection of event-race defects in Android apps. ER Catcher introduces a new type of summary function aimed at modeling the concurrent behavior of methods in both Android apps and libraries. In addition, it leverages a novel, statically constructed Vector Clock for rapid analysis of happens-before relations. Altogether, these design choices enable ER Catcher to not only detect event-race defects with a substantially higher degree of accuracy, but also in a fraction of time compared to the existing state-of-the-art technique.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416639",
    "comment": ""
  },
  {
    "title": "A deep multitask learning approach for requirements discovery and annotation from open forum",
    "author": "Li, Mingyang and Shi, Lin and Yang, Ye and Wang, Qing",
    "abstract": "The ability in rapidly learning and adapting to evolving user needs is key to modern business successes. Existing methods are based on text mining and machine learning techniques to analyze user comments and feedback, and often constrained by heavy reliance on manually codified rules or insufficient training data. Multitask learning (MTL) is an effective approach with many successful applications, with the potential to address these limitations associated with requirements analysis tasks. In this paper, we propose a deep MTL-based approach, DEMAR, to address these limitations when discovering requirements from massive issue reports and annotating the sentences in support of automated requirements analysis. DEMAR consists of three main phases: (1) data augmentation phase, for data preparation and allowing data sharing beyond single task learning; (2) model construction phase, for constructing the MTL-based model for requirements discovery and requirements annotation tasks; and (3) model training phase, enabling eavesdropping by shared loss function between the two related tasks. Evaluation results from eight open-source projects show that, the proposed multitask learning approach outperforms two state-of-the-art approaches (CNC and FRA) and six common machine learning algorithms, with the precision of 91\\% and the recall of 83\\% for requirements discovery task, and the overall accuracy of 83\\% for requirements annotation task. The proposed approach provides a novel and effective way to jointly learn two related requirements analysis tasks. We believe that it also sheds light on further directions of exploring multitask learning in solving other software engineering problems.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416627",
    "comment": ""
  },
  {
    "title": "Retrieve and refine: exemplar-based neural comment generation",
    "author": "Wei, Bolin and Li, Yongmin and Li, Ge and Xia, Xin and Jin, Zhi",
    "abstract": "Code comment generation which aims to automatically generate natural language descriptions for source code, is a crucial task in the field of automatic software development. Traditional comment generation methods use manually-crafted templates or information retrieval (IR) techniques to generate summaries for source code. In recent years, neural network-based methods which leveraged acclaimed encoder-decoder deep learning framework to learn comment generation patterns from a large-scale parallel code corpus, have achieved impressive results. However, these emerging methods only take code-related information as input. Software reuse is common in the process of software development, meaning that comments of similar code snippets are helpful for comment generation. Inspired by the IR-based and template-based approaches, in this paper, we propose a neural comment generation approach where we use the existing comments of similar code snippets as exemplars to guide comment generation. Specifically, given a piece of code, we first use an IR technique to retrieve a similar code snippet and treat its comment as an exemplar. Then we design a novel seq2seq neural network that takes the given code, its AST, its similar code, and its exemplar as input, and leverages the information from the exemplar to assist in the target comment generation based on the semantic similarity between the source code and the similar code. We evaluate our approach on a large-scale Java corpus, which contains about 2M samples, and experimental results demonstrate that our model outperforms the state-of-the-art methods by a substantial margin.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416578",
    "comment": ""
  },
  {
    "title": "Where shall we log? studying and suggesting logging locations in code blocks",
    "author": "Li, Zhenhao and Chen, Tse-Hsun (Peter) and Shang, Weiyi",
    "abstract": "Developers write logging statements to generate logs and record system execution behaviors to assist in debugging and software maintenance. However, deciding where to insert logging statements is a crucial yet challenging task. On one hand, logging too little may increase the maintenance difficulty due to missing important system execution information. On the other hand, logging too much may introduce excessive logs that mask the real problems and cause significant performance overhead. Prior studies provide recommendations on logging locations, but such recommendations are only for limited situations (e.g., exception logging) or at a coarse-grained level (e.g., method level). Thus, properly helping developers decide finer-grained logging locations for different situations remains an unsolved challenge. In this paper, we tackle the challenge by first conducting a comprehensive manual study on the characteristics of logging locations in seven open-source systems. We uncover six categories of logging locations and find that developers usually insert logging statements to record execution information in various types of code blocks. Based on the observed patterns, we then propose a deep learning framework to automatically suggest logging locations at the block level. We model the source code at the code block level using the syntactic and semantic information. We find that: 1) our models achieve an average of 80.1\\% balanced accuracy when suggesting logging locations in blocks; 2) our cross-system logging suggestion results reveal that there might be an implicit logging guideline across systems. Our results show that we may accurately provide finer-grained suggestions on logging locations, and such suggestions may be shared across systems.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416636",
    "comment": ""
  },
  {
    "title": "How incidental are the incidents? characterizing and prioritizing incidents for large-scale online service systems",
    "author": "Chen, Junjie and Zhang, Shu and He, Xiaoting and Lin, Qingwei and Zhang, Hongyu and Hao, Dan and Kang, Yu and Gao, Feng and Xu, Zhangwei and Dang, Yingnong and Zhang, Dongmei",
    "abstract": "Although tremendous efforts have been devoted to the quality assurance of online service systems, in reality, these systems still come across many incidents (i.e., unplanned interruptions and outages), which can decrease user satisfaction or cause economic loss. To better understand the characteristics of incidents and improve the incident management process, we perform the first large-scale empirical analysis of incidents collected from 18 real-world online service systems in Microsoft. Surprisingly, we find that although a large number of incidents could occur over a short period of time, many of them actually do not matter, i.e., engineers will not fix them with a high priority after manually identifying their root cause. We call these incidents incidental incidents. Our qualitative and quantitative analyses show that incidental incidents are significant in terms of both number and cost. Therefore, it is important to prioritize incidents by identifying incidental incidents in advance to optimize incident management efforts. In particular, we propose an approach, called DeepIP (Deep learning based Incident Prioritization), to prioritizing incidents based on a large amount of historical incident data. More specifically, we design an attention-based Convolutional Neural Network (CNN) to learn a prediction model to identify incidental incidents. We then prioritize all incidents by ranking the predicted probabilities of incidents being incidental. We evaluate the performance of DeepIP using real-world incident data. The experimental results show that DeepIP effectively prioritizes incidents by identifying incidental incidents and significantly outperforms all the compared approaches. For example, the AUC of DeepIP achieves 0.808, while that of the best compared approach is only 0.624 on average.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416624",
    "comment": ""
  },
  {
    "title": "Stay professional and efficient: automatically generate titles for your bug reports",
    "author": "Chen, Songqiang and Xie, Xiaoyuan and Yin, Bangguo and Ji, Yuanxiang and Chen, Lin and Xu, Baowen",
    "abstract": "Bug reports in a repository are generally organized line by line in a list-view, with their titles and other meta-data displayed. In this list-view, a concise and precise title plays an important role that enables project practitioners to quickly and correctly digest the core idea of the bug, without carefully reading the corresponding details. However, the quality of bug report titles varies in open-source communities, which may be due to the limited time and unprofessionalism of authors. To help report authors efficiently draft good-quality titles, we propose a method, named iTAPE, to automatically generate titles for their bug reports. iTAPE formulates title generation into a one-sentence summarization task. By properly tackling two domain-specific challenges (i.e. lacking off-the-shelf dataset and handling the low-frequency human-named tokens), iTAPE then generates titles using a Seq2Seq-based model. A comprehensive experimental study shows that iTAPE can obtain fairly satisfactory results, in terms of the comparison with three latest one-sentence summarization works, as well as the feedback from human evaluation.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416538",
    "comment": ""
  },
  {
    "title": "Owl eyes: spotting UI display issues via visual understanding",
    "author": "Liu, Zhe and Chen, Chunyang and Wang, Junjie and Huang, Yuekai and Hu, Jun and Wang, Qing",
    "abstract": "Graphical User Interface (GUI) provides a visual bridge between a software application and end users, through which they can interact with each other. With the development of technology and aesthetics, the visual effects of the GUI are more and more attracting. However, such GUI complexity posts a great challenge to the GUI implementation. According to our pilot study of crowdtesting bug reports, display issues such as text overlap, blurred screen, missing image always occur during GUI rendering on different devices due to the software or hardware compatibility. They negatively influence the app usability, resulting in poor user experience. To detect these issues, we propose a novel approach, OwlEye, based on deep learning for modelling visual information of the GUI screenshot. Therefore, OwlEye can detect GUIs with display issues and also locate the detailed region of the issue in the given GUI for guiding developers to fix the bug. We manually construct a large-scale labelled dataset with 4,470 GUI screenshots with UI display issues and develop a heuristics-based data augmentation method for boosting the performance of our OwlEye. The evaluation demonstrates that our OwlEye can achieve 85\\% precision and 84\\% recall in detecting UI display issues, and 90\\% accuracy in localizing these issues. We also evaluate OwlEye with popular Android apps on Google Play and F-droid, and successfully uncover 57 previously-undetected UI display issues with 26 of them being confirmed or fixed so far.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416547",
    "comment": ""
  },
  {
    "title": "Multiple-boundary clustering and prioritization to promote neural network retraining",
    "author": "Shen, Weijun and Li, Yanhui and Chen, Lin and Han, Yuanlei and Zhou, Yuming and Xu, Baowen",
    "abstract": "With the increasing application of deep learning (DL) models in many safety-critical scenarios, effective and efficient DL testing techniques are much in demand to improve the quality of DL models. One of the major challenges is the data gap between the training data to construct the models and the testing data to evaluate them. To bridge the gap, testers aim to collect an effective subset of inputs from the testing contexts, with limited labeling effort, for retraining DL models.To assist the subset selection, we propose Multiple-Boundary Clustering and Prioritization (MCP), a technique to cluster test samples into the boundary areas of multiple boundaries for DL models and specify the priority to select samples evenly from all boundary areas, to make sure enough useful samples for each boundary reconstruction. To evaluate MCP, we conduct an extensive empirical study with three popular DL models and 33 simulated testing contexts. The experiment results show that, compared with state-of-the-art baseline methods, on effectiveness, our approach MCP has a significantly better performance by evaluating the improved quality of retrained DL models; on efficiency, MCP also has the advantages in time costs.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416621",
    "comment": ""
  },
  {
    "title": "Marble: model-based robustness analysis of stateful deep learning systems",
    "author": "Du, Xiaoning and Li, Yi and Xie, Xiaofei and Ma, Lei and Liu, Yang and Zhao, Jianjun",
    "abstract": "State-of-the-art deep learning (DL) systems are vulnerable to adversarial examples, which hinders their potential adoption in safety-and security-critical scenarios. While some recent progress has been made in analyzing the robustness of feed-forward neural networks, the robustness analysis for stateful DL systems, such as recurrent neural networks (RNNs), still remains largely uncharted. In this paper, we propose Marble, a model-based approach for quantitative robustness analysis of real-world RNN-based DL systems. Marble builds a probabilistic model to compactly characterize the robustness of RNNs through abstraction. Furthermore, we propose an iterative refinement algorithm to derive a precise abstraction, which enables accurate quantification of the robustness measurement. We evaluate the effectiveness of Marble on both LSTM and GRU models trained separately with three popular natural language datasets. The results demonstrate that (1) our refinement algorithm is more efficient in deriving an accurate abstraction than the random strategy, and (2) Marble enables quantitative robustness analysis, in rendering better efficiency, accuracy, and scalability than the state-of-the-art techniques.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416564",
    "comment": ""
  },
  {
    "title": "MockSniffer: characterizing and recommending mocking decisions for unit tests",
    "author": "Zhu, Hengcheng and Wei, Lili and Wen, Ming and Liu, Yepang and Cheung, Shing-Chi and Sheng, Qin and Zhou, Cui",
    "abstract": "In unit testing, mocking is popularly used to ease test effort, reduce test flakiness, and increase test coverage by replacing the actual dependencies with simple implementations. However, there are no clear criteria to determine which dependencies in a unit test should be mocked. Inappropriate mocking can have undesirable consequences: under-mocking could result in the inability to isolate the class under test (CUT) from its dependencies while over-mocking increases the developers' burden on maintaining the mocked objects and may lead to spurious test failures. According to existing work, various factors can determine whether a dependency should be mocked. As a result, mocking decisions are often difficult to make in practice. Studies on the evolution of mocked objects also showed that developers tend to change their mocking decisions: 17\\% of the studied mocked objects were introduced sometime after the test scripts were created and another 13\\% of the originally mocked objects eventually became unmocked. In this work, we are motivated to develop an automated technique to make mocking recommendations to facilitate unit testing. We studied 10,846 test scripts in four actively maintained open-source projects that use mocked objects, aiming to characterize the dependencies that are mocked in unit testing. Based on our observations on mocking practices, we designed and implemented a tool, MockSniffer, to identify and recommend mocks for unit tests. The tool is fully automated and requires only the CUT and its dependencies as input. It leverages machine learning techniques to make mocking recommendations by holistically considering multiple factors that can affect developers' mocking decisions. Our evaluation of MockSniffer on ten open-source projects showed that it outperformed three baseline approaches, and achieved good performance in two potential application scenarios.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416539",
    "comment": ""
  },
  {
    "title": "Defect prediction guided search-based software testing",
    "author": "Perera, Anjana and Aleti, Aldeida and B\\\"{o}hme, Marcel and Turhan, Burak",
    "abstract": "Today, most automated test generators, such as search-based software testing (SBST) techniques focus on achieving high code coverage. However, high code coverage is not sufficient to maximise the number of bugs found, especially when given a limited testing budget. In this paper, we propose an automated test generation technique that is also guided by the estimated degree of defectiveness of the source code. Parts of the code that are likely to be more defective receive more testing budget than the less defective parts. To measure the degree of defectiveness, we leverage Schwa, a notable defect prediction technique.We implement our approach into EvoSuite, a state of the art SBST tool for Java. Our experiments on the Defects4J benchmark demonstrate the improved efficiency of defect prediction guided test generation and confirm our hypothesis that spending more time budget on likely defective parts increases the number of bugs found in the same time budget.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416612",
    "comment": ""
  },
  {
    "title": "API-misuse detection driven by fine-grained API-constraint knowledge graph",
    "author": "Ren, Xiaoxue and Ye, Xinyuan and Xing, Zhenchang and Xia, Xin and Xu, Xiwei and Zhu, Liming and Sun, Jianling",
    "abstract": "API misuses cause significant problem in software development. Existing methods detect API misuses against frequent API usage patterns mined from codebase. They make a naive assumption that API usage that deviates from the most-frequent API usage is a misuse. However, there is a big knowledge gap between API usage patterns and API usage caveats in terms of comprehensiveness, explainability and best practices. In this work, we propose a novel approach that detects API misuses directly against the API caveat knowledge, rather than API usage patterns. We develop open information extraction methods to construct a novel API-constraint knowledge graph from API reference documentation. This knowledge graph explicitly models two types of API-constraint relations (call-order and condition-checking) and enriches return and throw relations with return conditions and exception triggers. It empowers the detection of three types of frequent API misuses - missing calls, missing condition checking and missing exception handling, while existing detectors mostly focus on only missing calls. As a proof-of-concept, we apply our approach to Java SDK API Specification. Our evaluation confirms the high accuracy of the extracted API-constraint relations. Our knowledge-driven API misuse detector achieves 0.60 (68/113) precision and 0.28 (68/239) recall for detecting Java API misuses in the API misuse benchmark MuBench. This performance is significantly higher than that of existing pattern-based API misused detectors. A pilot user study with 12 developers shows that our knowledge-driven API misuse detection is very promising in helping developers avoid API misuses and debug the bugs caused by API misuses.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416551",
    "comment": ""
  },
  {
    "title": "Multi-task learning based pre-trained language model for code completion",
    "author": "Liu, Fang and Li, Ge and Zhao, Yunfei and Jin, Zhi",
    "abstract": "Code completion is one of the most useful features in the Integrated Development Environments (IDEs), which can accelerate software development by suggesting the next probable token based on the contextual code in real-time. Recent studies have shown that statistical language modeling techniques can improve the performance of code completion tools through learning from large-scale software repositories. However, these models suffer from two major drawbacks: a) Existing research uses static embeddings, which map a word to the same vector regardless of its context. The differences in the meaning of a token in varying contexts are lost when each token is associated with a single representation; b) Existing language model based code completion models perform poor on completing identifiers, and the type information of the identifiers is ignored in most of these models. To address these challenges, in this paper, we develop a multi-task learning based pre-trained language model for code understanding and code generation with a Transformer-based neural architecture. We pre-train it with hybrid objective functions that incorporate both code understanding and code generation tasks. Then we fine-tune the pre-trained model on code completion. During the completion, our model does not directly predict the next token. Instead, we adopt multi-task learning to predict the token and its type jointly and utilize the predicted type to assist the token prediction. Experiments results on two real-world datasets demonstrate the effectiveness of our model when compared with state-of-the-art methods.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416591",
    "comment": ""
  },
  {
    "title": "Audee: automated testing for deep learning frameworks",
    "author": "Guo, Qianyu and Xie, Xiaofei and Li, Yi and Zhang, Xiaoyu and Liu, Yang and Li, Xiaohong and Shen, Chao",
    "abstract": "Deep learning (DL) has been applied widely, and the quality of DL system becomes crucial, especially for safety-critical applications. Existing work mainly focuses on the quality analysis of DL models, but lacks attention to the underlying frameworks on which all DL models depend. In this work, we propose Audee, a novel approach for testing DL frameworks and localizing bugs. Audee adopts a search-based approach and implements three different mutation strategies to generate diverse test cases by exploring combinations of model structures, parameters, weights and inputs. Audee is able to detect three types of bugs: logical bugs, crashes and Not-a-Number (NaN) errors. In particular, for logical bugs, Audee adopts a cross-reference check to detect behavioural inconsistencies across multiple frameworks (e.g., TensorFlow and PyTorch), which may indicate potential bugs in their implementations. For NaN errors, Audee adopts a heuristic-based approach to generate DNNs that tend to output outliers (i.e., too large or small values), and these values are likely to produce NaN. Furthermore, Audee leverages a causal-testing based technique to localize layers as well as parameters that cause inconsistencies or bugs. To evaluate the effectiveness of our approach, we applied Audee on testing four DL frameworks, i.e., TensorFlow, PyTorch, CNTK, and Theano. We generate a large number of DNNs which cover 25 widely-used APIs in the four frameworks. The results demonstrate that Audee is effective in detecting inconsistencies, crashes and NaN errors. In total, 26 unique unknown bugs were discovered, and 7 of them have already been confirmed or fixed by the developers.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416571",
    "comment": ""
  },
  {
    "title": "Towards interpreting recurrent neural networks through probabilistic abstraction",
    "author": "Dong, Guoliang and Wang, Jingyi and Sun, Jun and Zhang, Yang and Wang, Xinyu and Dai, Ting and Dong, Jin Song and Wang, Xingen",
    "abstract": "Neural networks are becoming a popular tool for solving many real-world problems such as object recognition and machine translation, thanks to its exceptional performance as an end-to-end solution. However, neural networks are complex black-box models, which hinders humans from interpreting and consequently trusting them in making critical decisions. Towards interpreting neural networks, several approaches have been proposed to extract simple deterministic models from neural networks. The results are not encouraging (e.g., low accuracy and limited scalability), fundamentally due to the limited expressiveness of such simple models.In this work, we propose an approach to extract probabilistic automata for interpreting an important class of neural networks, i.e., recurrent neural networks. Our work distinguishes itself from existing approaches in two important ways. One is that probability is used to compensate for the loss of expressiveness. This is inspired by the observation that human reasoning is often 'probabilistic'. The other is that we adaptively identify the right level of abstraction so that a simple model is extracted in a request-specific way. We conduct experiments on several real-world datasets using state-of-the-art architectures including GRU and LSTM. The result shows that our approach significantly improves existing approaches in terms of accuracy or scalability. Lastly, we demonstrate the usefulness of the extracted models through detecting adversarial texts.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416592",
    "comment": ""
  },
  {
    "title": "Continuous compliance",
    "author": "Kellogg, Martin and Sch\\\"{a}f, Martin and Tasiran, Serdar and Ernst, Michael D.",
    "abstract": "Vendors who wish to provide software or services to large corporations and governments must often obtain numerous certificates of compliance. Each certificate asserts that the software satisfies a compliance regime, like SOC or the PCI DSS, to protect the privacy and security of sensitive data. The industry standard for obtaining a compliance certificate is an auditor manually auditing source code. This approach is expensive, error-prone, partial, and prone to regressions.We propose continuous compliance to guarantee that the codebase stays compliant on each code change using lightweight verification tools. Continuous compliance increases assurance and reduces costs.Continuous compliance is applicable to any source-code compliance requirement. To illustrate our approach, we built verification tools for five common audit controls related to data security: cryptographically unsafe algorithms must not be used, keys must be at least 256 bits long, credentials must not be hard-coded into program text, HTTPS must always be used instead of HTTP, and cloud data stores must not be world-readable.We evaluated our approach in three ways. (1) We applied our tools to over 5 million lines of open-source software. (2) We compared our tools to other publicly-available tools for detecting misuses of encryption on a previously-published benchmark, finding that only ours are suitable for continuous compliance. (3) We deployed a continuous compliance process at AWS, a large cloud-services company: we integrated verification tools into the compliance process (including auditors accepting their output as evidence) and ran them on over 68 million lines of code. Our tools and the data for the former two evaluations are publicly available.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416593",
    "comment": ""
  },
  {
    "title": "SADT: syntax-aware differential testing of certificate validation in SSL/TLS implementations",
    "author": "Quan, Lili and Guo, Qianyu and Chen, Hongxu and Xie, Xiaofei and Li, Xiaohong and Liu, Yang and Hu, Jing",
    "abstract": "The security assurance of SSL/TLS critically depends on the correct validation of X.509 certificates. Therefore, it is important to check whether a certificate is correctly validated by the SSL/TLS implementations. Although differential testing has been proven to be effective in finding semantic bugs, it still suffers from the following limitations: (1) The syntax of test cases cannot be correctly guaranteed. (2) Current test cases are not diverse enough to cover more implementation behaviours. This paper tackles these problems by introducing SADT, a novel syntax-aware differential testing framework for evaluating the certificate validation process in SSL/TLS implementations. We first propose a tree-based mutation strategy to ensure that the generated certificates are syntactically correct, and then diversify the certificates by sharing interesting test cases among all target SSL/TLS implementations. Such generated certificates are more likely to trigger discrepancies among SSL/TLS implementations, which may indicate some potential bugs.To evaluate the effectiveness of our approach, we applied SADT on testing 6 widely used SSL/TLS implementations, compared with the state-of-the-art fuzzing technique (i.e., AFL) and two differential testing techniques (i.e., NEZHA and RFCcert). The results show that SADT outperforms other techniques in generating discrepancies. In total, 64 unique discrepancies were discovered by SADT, and 13 of them have been confirmed as bugs or fixed by the developers.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416552",
    "comment": ""
  },
  {
    "title": "CoFI: consistency-guided fault injection for cloud systems",
    "author": "Chen, Haicheng and Dou, Wensheng and Wang, Dong and Qin, Feng",
    "abstract": "Network partitions are inevitable in large-scale cloud systems. Despite developer's efforts in handling network partitions throughout designing, implementing and testing cloud systems, bugs caused by network partitions, i.e., partition bugs, still exist and cause severe failures in production clusters. It is challenging to expose these partition bugs because they often require network partitions to start and stop at specific timings.In this paper, we propose <u>Co</u>nsistency-Guided <u>F</u>ault <u>I</u>njection (CoFI), a novel technique that systematically injects network partitions to effectively expose partition bugs. We observe that, network partitions can leave cloud systems in inconsistent states, where partition bugs are more likely to occur. Based on this observation, CoFI first infers invariants (i.e., consistent states) among different nodes in a cloud system. Once detecting violations to the inferred invariants (i.e., inconsistent states) while running the cloud system, CoFI injects network partitions to prevent the cloud system from recovering back to consistent states, and thoroughly tests whether the cloud system still proceeds correctly at inconsistent states. We have applied CoFI to three widely-deployed cloud systems, i.e., Cassandra, HDFS, and YARN. CoFI has detected 12 previously-unknown bugs, and four of them have been confirmed by developers.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416548",
    "comment": ""
  },
  {
    "title": "ChemTest: an automated software testing framework for an emerging paradigm",
    "author": "Gerten, Michael C. and Lathrop, James I. and Cohen, Myra B. and Klinge, Titus H.",
    "abstract": "In recent years the use of non-traditional computing mechanisms has grown rapidly. One paradigm uses chemical reaction networks (CRNs) to compute via chemical interactions. CRNs are used to prototype molecular devices at the nanoscale such as intelligent drug therapeutics. In practice, these programs are first written and simulated in environments such as MatLab and later compiled into physical molecules such as DNA strands. However, techniques for testing the correctness of CRNs are lacking. Current methods of validating CRNs include model checking and theorem proving, but these are limited in scalability. In this paper we present the first (to the best of our knowledge) testing framework for CRNs, ChemTest. ChemTest evaluates test oracles on individual simulation traces and supports functional, metamorphic, internal and hyper test cases. It also allows for flakiness and programs that are probabilistic. We performed a large case study demonstrating that ChemTest can find seeded faults and scales beyond model checking. Of our tests, 21\\% are inherently flaky, suggesting that systematic support for this paradigm is needed. On average, functional tests find 66.5\\% of the faults, while metamorphic tests find 80.4\\%, showing the benefit of using metamorphic relationships in our test framework. In addition, we show how the time at evaluation impacts fault detection.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416638",
    "comment": ""
  },
  {
    "title": "Automatic extraction of cause-effect-relations from requirements artifacts",
    "author": "Frattini, Julian and Junker, Maximilian and Unterkalmsteiner, Michael and Mendez, Daniel",
    "abstract": "Background: The detection and extraction of causality from natural language sentences have shown great potential in various fields of application. The field of requirements engineering is eligible for multiple reasons: (1) requirements artifacts are primarily written in natural language, (2) causal sentences convey essential context about the subject of requirements, and (3) extracted and formalized causality relations are usable for a (semi-)automatic translation into further artifacts, such as test cases.Objective: We aim at understanding the value of interactive causality extraction based on syntactic criteria for the context of requirements engineering.Method: We developed a prototype of a system for automatic causality extraction and evaluate it by applying it to a set of publicly available requirements artifacts, determining whether the automatic extraction reduces the manual effort of requirements formalization.Result: During the evaluation we analyzed 4457 natural language sentences from 18 requirements documents, 558 of which were causal (12.52\\%). The best evaluation of a requirements document provided an automatic extraction of 48.57\\% cause-effect graphs on average, which demonstrates the feasibility of the approach.Limitation: The feasibility of the approach has been proven in theory but lacks exploration of being scaled up for practical use. Evaluating the applicability of the automatic causality extraction for a requirements engineer is left for future research.Conclusion: A syntactic approach for causality extraction is viable for the context of requirements engineering and can aid a pipeline towards an automatic generation of further artifacts from requirements artifacts.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416549",
    "comment": ""
  },
  {
    "title": "BiLO-CPDP: bi-level programming for automated model discovery in cross-project defect prediction",
    "author": "Li, Ke and Xiang, Zilin and Chen, Tao and Tan, Kay Chen",
    "abstract": "Cross-Project Defect Prediction (CPDP), which borrows data from similar projects by combining a transfer learner with a classifier, have emerged as a promising way to predict software defects when the available data about the target project is insufficient. However, developing such a model is challenge because it is difficult to determine the right combination of transfer learner and classifier along with their optimal hyper-parameter settings. In this paper, we propose a tool, dubbed BiLO-CPDP, which is the first of its kind to formulate the automated CPDP model discovery from the perspective of bi-level programming. In particular, the bi-level programming proceeds the optimization with two nested levels in a hierarchical manner. Specifically, the upper-level optimization routine is designed to search for the right combination of transfer learner and classifier while the nested lower-level optimization routine aims to optimize the corresponding hyper-parameter settings. To evaluate BiLO-CPDP, we conduct experiments on 20 projects to compare it with a total of 21 existing CPDP techniques, along with its single-level optimization variant and Auto-Sklearn, a state-of-the-art automated machine learning tool. Empirical results show that BiLO-CPDP champions better prediction performance than all other 21 existing CPDP techniques on 70\\% of the projects, while being overwhelmingly superior to Auto-Sklearn and its single-level optimization variant on all cases. Furthermore, the unique bi-level formalization in BiLO-CPDP also permits to allocate more budget to the upper-level, which significantly boosts the performance.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416617",
    "comment": ""
  },
  {
    "title": "Automating just-in-time comment updating",
    "author": "Liu, Zhongxin and Xia, Xin and Yan, Meng and Li, Shanping",
    "abstract": "Code comments are valuable for program comprehension and software maintenance, and also require maintenance with code evolution. However, when changing code, developers sometimes neglect updating the related comments, bringing in inconsistent or obsolete comments (aka., bad comments). Such comments are detrimental since they may mislead developers and lead to future bugs. Therefore, it is necessary to fix and avoid bad comments. In this work, we argue that bad comments can be reduced and even avoided by automatically performing comment updates with code changes. We refer to this task as \"Just-In-Time (JIT) Comment Updating\" and propose an approach named CUP (<u>C</u>omment <u>UP</u>dater) to automate this task. CUP can be used to assist developers in updating comments during code changes and can consequently help avoid the introduction of bad comments. Specifically, CUP leverages a novel neural sequence-to-sequence model to learn comment update patterns from extant code-comment co-changes and can automatically generate a new comment based on its corresponding old comment and code change. Several customized enhancements, such as a special tokenizer and a novel co-attention mechanism, are introduced in CUP by us to handle the characteristics of this task. We build a dataset with over 108K comment-code co-change samples and evaluate CUP on it. The evaluation results show that CUP outperforms an information-retrieval-based and a rule-based baselines by substantial margins, and can reduce developers' edits required for JIT comment updating. In addition, the comments generated by our approach are identical to those updated by developers in 1612 (16.7\\%) test samples, 7 times more than the best-performing baseline.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416581",
    "comment": ""
  },
  {
    "title": "Automated implementation of windows-related security-configuration guides",
    "author": "St\\\"{o}ckle, Patrick and Grobauer, Bernd and Pretschner, Alexander",
    "abstract": "Hardening is the process of configuring IT systems to ensure the security of the systems' components and data they process or store. The complexity of contemporary IT infrastructures, however, renders manual security hardening and maintenance a daunting task.In many organizations, security-configuration guides expressed in the SCAP (Security Content Automation Protocol) are used as a basis for hardening, but these guides by themselves provide no means for automatically implementing the required configurations.In this paper, we propose an approach to automatically extract the relevant information from publicly available security-configuration guides for Windows operating systems using natural language processing. In a second step, the extracted information is verified using the information of available settings stored in the Windows Administrative Template files, in which the majority of Windows configuration settings is defined.We show that our implementation of this approach can extract and implement 83\\% of the rules without any manual effort and 96\\% with minimal manual effort. Furthermore, we conduct a study with 12 state-of-the-art guides consisting of 2014 rules with automatic checks and show that our tooling can implement at least 97\\% of them correctly. We have thus significantly reduced the effort of securing systems based on existing security-configuration guides.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416540",
    "comment": ""
  },
  {
    "title": "Identifying software performance changes across variants and versions",
    "author": "M\\\"{u}hlbauer, Stefan and Apel, Sven and Siegmund, Norbert",
    "abstract": "We address the problem of identifying performance changes in the evolution of configurable software systems. Finding optimal configurations and configuration options that influence performance is already difficult, but in the light of software evolution, configuration-dependent performance changes may lurk in a potentially large number of different versions of the system.In this work, we combine two perspectives---variability and time---into a novel perspective. We propose an approach to identify configuration-dependent performance changes retrospectively across the software variants and versions of a software system. In a nutshell, we iteratively sample pairs of configurations and versions and measure the respective performance, which we use to update a model of likelihoods for performance changes. Pursuing a search strategy with the goal of measuring selectively and incrementally further pairs, we increase the accuracy of identified change points related to configuration options and interactions.We have conducted a number of experiments both on controlled synthetic data sets as well as in real-world scenarios with different software systems. Our evaluation demonstrates that we can pinpoint performance shifts to individual configuration options and interactions as well as commits introducing change points with high accuracy and at scale. Experiments on three real-world systems explore the effectiveness and practicality of our approach.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416573",
    "comment": ""
  },
  {
    "title": "CP-detector: using configuration-related performance properties to expose performance bugs",
    "author": "He, Haochen and Jia, Zhouyang and Li, Shanshan and Xu, Erci and Yu, Tingting and Yu, Yue and Wang, Ji and Liao, Xiangke",
    "abstract": "Performance bugs are often hard to detect due to their non fail-stop symptoms. Existing debugging techniques can only detect performance bugs with known patterns (e.g., inefficient loops). The key reason behind this incapability is the lack of a general test oracle. Here, we argue that the performance (e.g., throughput, latency, execution time) expectation of configuration can serve as a strong oracle candidate for performance bug detection. First, prior work shows that most performance bugs are related to configurations. Second, the configuration change reflects common expectation on performance changes. If the actual performance is contrary to the expectation, the related code snippet is likely to be problematic.In this paper, we first conducted a comprehensive study on 173 real-world configuration-related performance bugs (CPBugs) from 12 representative software systems. We then derived seven configuration-related performance properties, which can serve as the test oracle in performance testing. Guided by the study, we designed and evaluated an automated performance testing framework, CP-Detector, for detecting real-world configuration-related performance bugs. CP-Detector was evaluated on 12 open-source projects. The results showed that it detected 43 out of 61 existing bugs and reported 13 new bugs.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416531",
    "comment": ""
  },
  {
    "title": "Just-in-time reactive synthesis",
    "author": "Maoz, Shahar and Shevrin, Ilia",
    "abstract": "Reactive synthesis is an automated procedure to obtain a correct-by-construction reactive system from its temporal logic specification. GR(1) is an expressive assume-guarantee fragment of LTL that enables efficient synthesis and has been recently used in different contexts and application domains.In this work we present just-in-time synthesis (JITS) for GR(1), a novel means to execute synthesized reactive systems. Rather than constructing a controller at synthesis time, we compute next states during system execution, and only when they are required. We prove that JITS does not compromise the correctness of the synthesized system execution. We further show that the basic algorithm can be extended to enable several variants.We have implemented JITS in the Spectra synthesizer. Our evaluation, comparing JITS to existing tools over known benchmark specifications, shows that JITS reduces (1) total synthesis time, (2) the size of the synthesis output, and (3) the loading time for system execution, all while having little to no effect on system execution performance.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416557",
    "comment": ""
  },
  {
    "title": "JISET: JavaScript IR-based semantics extraction toolchain",
    "author": "Park, Jihyeok and Park, Jihee and An, Seungmin and Ryu, Sukyoung",
    "abstract": "JavaScript was initially designed for client-side programming in web browsers, but its engine is now embedded in various kinds of host software. Despite the popularity, since the JavaScript semantics is complex especially due to its dynamic nature, understanding and reasoning about JavaScript programs are challenging tasks. Thus, researchers have proposed several attempts to define the formal semantics of JavaScript based on ECMAScript, the official JavaScript specification. However, the existing approaches are manual, labor-intensive, and error-prone and all of their formal semantics target ECMAScript 5.1 (ES5.1, 2011) or its former versions. Therefore, they are not suitable for understanding modern JavaScript language features introduced since ECMAScript 6 (ES6, 2015). Moreover, ECMAScript has been annually updated since ES6, which already made five releases after ES5.1.To alleviate the problem, we propose JISET, a JavaScript IR-based Semantics Extraction Toolchain. It is the first tool that automatically synthesizes parsers and AST-IR translators directly from a given language specification, ECMAScript. For syntax, we develop a parser generation technique with lookahead parsing for BNFES, a variant of the extended BNF used in ECMAScript. For semantics, JISET synthesizes AST-IR translators using forward compatible rule-based compilation. Compile rules describe how to convert each step of abstract algorithms written in a structured natural language into IRES, an Intermediate Representation that we designed for ECMAScript. For the four most recent ECMAScript versions, JISET automatically synthesized parsers for all versions, and compiled 95.03\\% of the algorithm steps on average. After we complete the missing parts manually, the extracted core semantics of the latest ECMAScript (ES10, 2019) passed all 18,064 applicable tests. Using this first formal semantics of modern JavaScript, we found nine specification errors in ES10, which were all confirmed by the Ecma Technical Committee 39. Furthermore, we showed that JISET is forward compatible by applying it to nine feature proposals ready for inclusion in the next ECMAScript, which let us find three errors in the BigInt proposal.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416632",
    "comment": ""
  },
  {
    "title": "FlashRegex: deducing anti-ReDoS regexes from examples",
    "author": "Li, Yeting and Xu, Zhiwu and Cao, Jialun and Chen, Haiming and Ge, Tingjian and Cheung, Shing-Chi and Zhao, Haoren",
    "abstract": "Regular expressions (regexes) are widely used in different fields of computer science such as programming languages, string processing and databases. However, existing tools for synthesizing or repairing regexes were not designed to be resilient to Regex Denial of Service (ReDoS) attacks. Specifically, if a regex has super-linear (SL) worst-case complexity, an attacker could provide carefully-crafted inputs to launch ReDoS attacks. Therefore, in this paper, we propose a programming-by-example framework, FlashRegex, for generating anti-ReDoS regexes by either synthesizing or repairing from given examples. It is the first framework that integrates regex synthesis and repair with the awareness of ReDoS-vulnerabilities. We present novel algorithms to deduce anti-ReDoS regexes by reducing the ambiguity of these regexes and by using Boolean Satisfiability (SAT) or Neighborhood Search (NS) techniques. We evaluate FlashRegex with five related state-of-the-art tools. The evaluation results show that our work can effectively and efficiently generate anti-ReDoS regexes from given examples, and also reveal that existing synthesis and repair tools have neglected ReDoS-vulnerabilities of regexes. Specifically, the existing synthesis and repair tools generated up to 394 ReDoS-vulnerable regex within few seconds to more than one hour, while FlashRegex generated no SL regex within around five seconds. Furthermore, the evaluation results on ReDoS-vulnerable regex repair also show that FlashRegex has better capability than existing repair tools and even human experts, achieving 4 more ReDoS-invulnerable regex after repair without trimming and resorting, highlighting the usefulness of FlashRegex in terms of the generality, automation and user-friendliness.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416556",
    "comment": ""
  },
  {
    "title": "Inferring and applying def-use like configuration couplings in deployment descriptors",
    "author": "Wen, Chengyuan and Zhang, Yaxuan and He, Xiao and Meng, Na",
    "abstract": "When building enterprise applications on Java frameworks (e.g., Spring), developers often specify components and configure operations with a special kind of XML files named \"deployment descriptors (DD)\". Maintaining such XML files is challenging and time-consuming; because (1) the correct configuration semantics is domain-specific but usually vaguely documented, and (2) existing compilers and program analysis tools rarely examine XML files. To help developers ensure the quality of DD, this paper presents a novel approach---Xeditor---that extracts configuration couplings (i.e., frequently co-occurring configurations) from DD, and adopts the coupling rules to validate new or updated files.Xeditor has two phases: coupling extraction and bug detection. To identify couplings, Xeditor first mines DD in open-source projects, and extracts XML entity pairs that (i) frequently coexist in the same files and (ii) hold the same data at least once. Xeditor then applies customized association rule mining to the extracted pairs. For bug detection, given a new XML file, Xeditor checks whether the file violates any coupling; if so, Xeditor reports the violation(s). For evaluation, we first created two data sets with the 4,248 DD mined from 1,137 GitHub projects. According to the experiments with these data sets, Xeditor extracted couplings with high precision (73\\%); it detected bugs with 92\\% precision, 96\\% recall, and 94\\% accuracy. Additionally, we applied Xeditor to the version history of another 478 GitHub projects. Xeditor identified 25 very suspicious XML updates, 15 of which were later fixed by developers.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416577",
    "comment": ""
  },
  {
    "title": "Mastering uncertainty in performance estimations of configurable software systems",
    "author": "Dorn, Johannes and Apel, Sven and Siegmund, Norbert",
    "abstract": "Understanding the influence of configuration options on performance is key for finding optimal system configurations, system understanding, and performance debugging. In prior research, a number of performance-influence modeling approaches have been proposed, which model a configuration option's influence and a configuration's performance as a scalar value. However, these point estimates falsely imply a certainty regarding an option's influence that neglects several sources of uncertainty within the assessment process, such as (1) measurement bias, (2) model representation and learning process, and (3) incomplete data. This leads to the situation that different approaches and even different learning runs assign different scalar performance values to options and interactions among them. The true influence is uncertain, though. There is no way to quantify this uncertainty with state-of-the-art performance modeling approaches. We propose a novel approach, P4, based on probabilistic programming that explicitly models uncertainty for option influences and consequently provides a confidence interval for each prediction of a configuration's performance alongside a scalar. This way, we can explain, for the first time, why predictions may cause errors and which option's influences may be unreliable. An evaluation on 12 real-world subject systems shows that P4's accuracy is in line with the state of the art while providing reliable confidence intervals, in addition to scalar predictions.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416620",
    "comment": ""
  },
  {
    "title": "Team discussions and dynamics during DevOps tool adoptions in OSS projects",
    "author": "Yin, Likang and Filkov, Vladimir",
    "abstract": "In Open Source Software (OSS) projects, pre-built tools dominate DevOps-oriented pipelines. In practice, a multitude of configuration management, cloud-based continuous integration, and automated deployment tools exist, and often more than one for each task. Tools are adopted (and given up) by OSS projects regularly. Prior work has shown that some tool adoptions are preceded by discussions, and that tool adoptions can result in benefits to the project. But important questions remain: how do teams decide to adopt a tool? What is discussed before the adoption and for how long? And, what team characteristics are determinant of the adoption?In this paper, we employ a large-scale empirical study in order to characterize the team discussions and to discern the teamlevel determinants of tool adoption into OSS projects' development pipelines. Guided by theories of team and individual motivations and dynamics, we perform exploratory data analyses, do deep-dive case studies, and develop regression models to learn the determinants of adoption and discussion length, and the direction of their effect on the adoption. From data of commit and comment traces of large-scale GitHub projects, our models find that prior exposure to a tool and member involvement are positively associated with the tool adoption, while longer discussions and the number of newer team members associate negatively. These results can provide guidance beyond the technical appropriateness for the timeliness of tool adoptions in diverse programmer teams.Our data and code is available at https://github.com/lkyin/tool_adoptions.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416640",
    "comment": ""
  },
  {
    "title": "TestMC: testing model counters using differential and metamorphic testing",
    "author": "Usman, Muhammad and Wang, Wenxi and Khurshid, Sarfraz",
    "abstract": "Model counting is the problem for finding the number of solutions to a formula over a bounded universe. This is a classic problem in computer science that has seen many recent advances in techniques and tools that tackle it. These advances have led to applications of model counting in many domains, e.g., quantitative program analysis, reliability, and security. Given the sheer complexity of the underlying problem, today's model counters employ sophisticated algorithms and heuristics, which result in complex tools that must be heavily optimized. Therefore, establishing the correctness of implementations of model counters necessitates rigorous testing. This experience paper presents an empirical study on testing industrial strength model counters by applying the principles of differential and metamorphic testing together with bounded exhaustive input generation and input minimization. We embody these principles in the TestMC framework, and apply it to test four model counters, including three state-of-the-art model counters from three different classes. Specifically, we test the exact model counters projMC and dSharp, the probabilistic exact model counter Ganak, and the probabilistic approximate model counter ApproxMC. As subjects, we use three complementary test suites of input formulas. One suite consists of larger formulas that are derived from a wide range of real-world software design problems. The second suite consists of a bounded exhaustive set of small formulas that TestMC generated. The third suite consists of formulas generated using an off-the-shelf CNF fuzzer. TestMC found bugs in three of the four subject model counters. The bugs led to crashes, segmentation faults, incorrect model counts, and resource exhaustion by the solvers. Two of the tools were corrected subsequent to the bug reports we submitted based on our study, whereas the bugs we reported in the third tool were deemed by the tool authors to not require a fix.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416563",
    "comment": ""
  },
  {
    "title": "BigFuzz: efficient fuzz testing for data analytics using framework abstraction",
    "author": "Zhang, Qian and Wang, Jiyuan and Gulzar, Muhammad Ali and Padhye, Rohan and Kim, Miryung",
    "abstract": "As big data analytics become increasingly popular, data-intensive scalable computing (DISC) systems help address the scalability issue of handling large data. However, automated testing for such data-centric applications is challenging, because data is often incomplete, continuously evolving, and hard to know a priori. Fuzz testing has been proven to be highly effective in other domains such as security; however, it is nontrivial to apply such traditional fuzzing to big data analytics directly for three reasons: (1) the long latency of DISC systems prohibits the applicability of fuzzing: na\\\"{\\i}ve fuzzing would spend 98\\% of the time in setting up a test environment; (2) conventional branch coverage is unlikely to scale to DISC applications because most binary code comes from the framework implementation such as Apache Spark; and (3) random bit or byte level mutations can hardly generate meaningful data, which fails to reveal real-world application bugs.We propose a novel coverage-guided fuzz testing tool for big data analytics, called BigFuzz. The key essence of our approach is that: (a) we focus on exercising application logic as opposed to increasing framework code coverage by abstracting the DISC framework using specifications. BigFuzz performs automated source to source transformations to construct an equivalent DISC application suitable for fast test generation, and (b) we design schema-aware data mutation operators based on our in-depth study of DISC application error types. BigFuzz speeds up the fuzzing time by 78 to 1477X compared to random fuzzing, improves application code coverage by 20\\% to 271\\%, and achieves 33\\% to 157\\% improvement in detecting application errors. When compared to the state of the art that uses symbolic execution to test big data analytics, BigFuzz is applicable to twice more programs and can find 81\\% more bugs.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416641",
    "comment": ""
  },
  {
    "title": "Scaling client-specific equivalence checking via impact boundary search",
    "author": "Feng, Nick and Mora, Federico and Hui, Vincent and Chechik, Marsha",
    "abstract": "Client-specific equivalence checking (CSEC) is a technique proposed previously to perform impact analysis of changes to downstream components (libraries) from the perspective of an unchanged system (client). Existing analysis techniques, whether general (regression verification, equivalence checking) or special-purpose, when applied to CSEC, either require users to provide specifications, or do not scale. We propose a novel solution to the CSEC problem, called 2clever, that is based on searching the control-flow of a program for impact boundaries. We evaluate a prototype implementation of 2clever on a comprehensive set of benchmarks and conclude that our prototype performs well compared to the state-of-the-art.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416634",
    "comment": ""
  },
  {
    "title": "Code to comment \"translation\": data, metrics, baselining \\& evaluation",
    "author": "Gros, David and Sezhiyan, Hariharan and Devanbu, Prem and Yu, Zhou",
    "abstract": "The relationship of comments to code, and in particular, the task of generating useful comments given the code, has long been of interest. The earliest approaches have been based on strong syntactic theories of comment-structures, and relied on textual templates. More recently, researchers have applied deep-learning methods to this task---specifically, trainable generative translation models which are known to work very well for Natural Language translation (e.g., from German to English). We carefully examine the underlying assumption here: that the task of generating comments sufficiently resembles the task of translating between natural languages, and so similar models and evaluation metrics could be used. We analyze several recent code-comment datasets for this task: CodeNN, DeepCom, FunCom, and DocString. We compare them with WMT19, a standard dataset frequently used to train state-of-the-art natural language translators. We found some interesting differences between the code-comment data and the WMT19 natural language data. Next, we describe and conduct some studies to calibrate BLEU (which is commonly used as a measure of comment quality). using \"affinity pairs\" of methods, from different projects, in the same project, in the same class, etc; Our study suggests that the current performance on some datasets might need to be improved substantially. We also argue that fairly naive information retrieval (IR) methods do well enough at this task to be considered a reasonable baseline. Finally, we make some suggestions on how our findings might be used in future research in this area.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416546",
    "comment": ""
  },
  {
    "title": "Exploring the architectural impact of possible dependencies in Python software",
    "author": "Jin, Wuxia and Cai, Yuanfang and Kazman, Rick and Zhang, Gang and Zheng, Qinghua and Liu, Ting",
    "abstract": "Dependencies among software entities are the basis for many software analytic research and architecture analysis tools. Dynamically typed languages, such as Python, JavaScript and Ruby, tolerate the lack of explicit type references, making certain syntactic dependencies indiscernible in source code. We call these possible dependencies, in contrast with the explicit dependencies that are directly referenced in source code. Type inference techniques have been widely studied and applied, but existing architecture analytic research and tools have not taken possible dependencies into consideration. The fundamental question is, to what extent will these missing possible dependencies impact the architecture analysis? To answer this question, we conducted an empirical study with 105 Python projects, using type inference techniques to manifest possible dependencies. Our study revealed that the architectural impact of possible dependencies is substantial---higher than that of explicit dependencies: (1) file-level possible dependencies account for at least 27.93\\% of all file-level dependencies, and create different dependency structures than that of explicit dependencies only, with an average difference of 30.71\\%; (2) adding possible dependencies significantly improves the precision (0.52\\%~14.18\\%), recall(31.73\\%~39.12\\%), and F1 scores (22.13\\%~32.09\\%) of capturing co-change relations; (3) on average, a file involved in possible dependencies influences 28\\% more files and 42\\% more dependencies within architectural sub-spaces than a file involved in just explicit dependencies; (4) on average, a file involved in possible dependencies consumes 32\\% more maintenance effort. Consequently, maintainability scores reported by existing tools make a system written in these dynamic languages appear to be better modularized than it actually is. This evidence strongly suggests that possible dependencies have a more significant impact than explicit dependencies on architecture quality, that architecture analysis and tools should assess and even emphasize the architectural impact of possible dependencies due to dynamic typing.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416619",
    "comment": ""
  },
  {
    "title": "Problems and opportunities in training deep learning software systems: an analysis of variance",
    "author": "Pham, Hung Viet and Qian, Shangshu and Wang, Jiannan and Lutellier, Thibaud and Rosenthal, Jonathan and Tan, Lin and Yu, Yaoliang and Nagappan, Nachiappan",
    "abstract": "Deep learning (DL) training algorithms utilize nondeterminism to improve models' accuracy and training efficiency. Hence, multiple identical training runs (e.g., identical training data, algorithm, and network) produce different models with different accuracies and training times. In addition to these algorithmic factors, DL libraries (e.g., TensorFlow and cuDNN) introduce additional variance (referred to as implementation-level variance) due to parallelism, optimization, and floating-point computation.This work is the first to study the variance of DL systems and the awareness of this variance among researchers and practitioners. Our experiments on three datasets with six popular networks show large overall accuracy differences among identical training runs. Even after excluding weak models, the accuracy difference is 10.8\\%. In addition, implementation-level factors alone cause the accuracy difference across identical training runs to be up to 2.9\\%, the per-class accuracy difference to be up to 52.4\\%, and the training time difference to be up to 145.3\\%. All core libraries (TensorFlow, CNTK, and Theano) and low-level libraries (e.g., cuDNN) exhibit implementation-level variance across all evaluated versions.Our researcher and practitioner survey shows that 83.8\\% of the 901 participants are unaware of or unsure about any implementation-level variance. In addition, our literature survey shows that only 19.53\\% of papers in recent top software engineering (SE), artificial intelligence (AI), and systems conferences use multiple identical training runs to quantify the variance of their DL approaches. This paper raises awareness of DL variance and directs SE researchers to challenging tasks such as creating deterministic DL implementations to facilitate debugging and improving the reproducibility of DL software and results.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416545",
    "comment": ""
  },
  {
    "title": "NeuroDiff: scalable differential verification of neural networks using fine-grained approximation",
    "author": "Paulsen, Brandon and Wang, Jingbo and Wang, Jiawei and Wang, Chao",
    "abstract": "As neural networks make their way into safety-critical systems, where misbehavior can lead to catastrophes, there is a growing interest in certifying the equivalence of two structurally similar neural networks - a problem known as differential verification. For example, compression techniques are often used in practice for deploying trained neural networks on computationally- and energy-constrained devices, which raises the question of how faithfully the compressed network mimics the original network. Unfortunately, existing methods either focus on verifying a single network or rely on loose approximations to prove the equivalence of two networks. Due to overly conservative approximation, differential verification lacks scalability in terms of both accuracy and computational cost. To overcome these problems, we propose NeuroDiff, a symbolic and fine-grained approximation technique that drastically increases the accuracy of differential verification on feed-forward ReLU networks while achieving many orders-of-magnitude speedup. NeuroDiff has two key contributions. The first one is new convex approximations that more accurately bound the difference of two networks under all possible inputs. The second one is judicious use of symbolic variables to represent neurons whose difference bounds have accumulated significant error. We find that these two techniques are complementary, i.e., when combined, the benefit is greater than the sum of their individual benefits. We have evaluated NeuroDiff on a variety of differential verification tasks. Our results show that NeuroDiff is up to 1000X faster and 5X more accurate than the state-of-the-art tool.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416560",
    "comment": ""
  },
  {
    "title": "Identifying and describing information seeking tasks",
    "author": "Satterfield, Chris and Fritz, Thomas and Murphy, Gail C.",
    "abstract": "A software developer works on many tasks per day, frequently switching between these tasks back and forth. This constant churn of tasks makes it difficult for a developer to know the specifics of when they worked on what task, complicating task resumption, planning, retrospection, and reporting activities. In a first step towards an automated aid to this issue, we introduce a new approach to help identify the topic of work during an information seeking task --- one of the most common types of tasks that software developers face --- that is based on capturing the contents of the developer's active window at regular intervals and creating a vector representation of key information the developer viewed. To evaluate our approach, we created a data set with multiple developers working on the same set of six information seeking tasks that we also make available for other researchers to investigate similar approaches. Our analysis shows that our approach enables: 1) segments of a developer's work to be automatically associated with a task from a known set of tasks with average accuracy of 70.6\\%, and 2) a word cloud describing a segment of work that a developer can use to recognize a task with average accuracy of 67.9\\%.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416537",
    "comment": ""
  },
  {
    "title": "Predicting code context models for software development tasks",
    "author": "Wan, Zhiyuan and Murphy, Gail C. and Xia, Xin",
    "abstract": "Code context models consist of source code elements and their relations relevant to a development task. Prior research showed that making code context models explicit in software tools can benefit software development practices, e.g., code navigation and searching. However, little focus has been put on how to proactively form code context models. In this paper, we explore the proactive formation of code context models based on the topological patterns of code elements from interaction histories for a project. Specifically, we first learn abstract topological patterns based on the stereotype roles of code elements, rather than on specific code elements; we then leverage the learned patterns to predict the code context models for a given task by graph pattern matching. To determine the effectiveness of this approach, we applied the approach to interaction histories stored for the Eclipse Mylyn open source project. We found that our approach achieves maximum F-measures of 0.67, 0.33 and 0.21 for 1-step, 2-step and 3-step predictions, respectively. The most similar approach to ours is Suade, which supports 1-step prediction only. In comparison to this existing work, our approach predicts code context models with significantly higher F-measure (0.57 over 0.23 on average). The results demonstrate the value of integrating historical and structural approaches to form more accurate code context models.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416544",
    "comment": ""
  },
  {
    "title": "SCDetector: software functional clone detection based on semantic tokens analysis",
    "author": "Wu, Yueming and Zou, Deqing and Dou, Shihan and Yang, Siru and Yang, Wei and Cheng, Feng and Liang, Hong and Jin, Hai",
    "abstract": "Code clone detection is to find out code fragments with similar functionalities, which has been more and more important in software engineering. Many approaches have been proposed to detect code clones, in which token-based methods are the most scalable but cannot handle semantic clones because of the lack of consideration of program semantics. To address the issue, researchers conduct program analysis to distill the program semantics into a graph representation and detect clones by matching the graphs. However, such approaches suffer from low scalability since graph matching is typically time-consuming.In this paper, we propose SCDetector to combine the scalability of token-based methods with the accuracy of graph-based methods for software functional clone detection. Given a function source code, we first extract the control flow graph by static analysis. Instead of using traditional heavyweight graph matching, we treat the graph as a social network and apply social-network-centrality analysis to dig out the centrality of each basic block. Then we assign the centrality to each token in a basic block and sum the centrality of the same token in different basic blocks. By this, a graph is turned into certain tokens with graph details (i.e., centrality), called semantic tokens. Finally, these semantic tokens are fed into a Siamese architecture neural network to train a code clone detector. We evaluate SCDetector on two large datasets of functionally similar code. Experimental results indicate that our system is superior to four state-of-the-art methods (i.e., SourcererCC, Deckard, RtvNN, and ASTNN) and the time cost of SCDetector is 14 times less than a traditional graph-based method (i.e., CCSharp) on detecting semantic clones.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416562",
    "comment": ""
  },
  {
    "title": "Generating concept based API element comparison using a knowledge graph",
    "author": "Liu, Yang and Liu, Mingwei and Peng, Xin and Treude, Christoph and Xing, Zhenchang and Zhang, Xiaoxin",
    "abstract": "Developers are concerned with the comparison of similar APIs in terms of their commonalities and (often subtle) differences. Our empirical study of Stack Overflow questions and API documentation confirms that API comparison questions are common and can often be answered by knowledge contained in API reference documentation. Our study also identifies eight types of API statements that are useful for API comparison. Based on these findings, we propose a knowledge graph based approach APIComp that automatically extracts API knowledge from API reference documentation to support the comparison of a pair of API classes or methods from different aspects. Our approach includes an offline phase for constructing an API knowledge graph, and an online phase for generating an API comparison result for a given pair of API elements. Our evaluation shows that the quality of different kinds of extracted knowledge in the API knowledge graph is generally high. Furthermore, the comparison results generated by APIComp are significantly better than those generated by a baseline approach based on heuristic rules and text similarity, and our generated API comparison results are useful for helping developers in API selection tasks.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416628",
    "comment": ""
  },
  {
    "title": "Multiplex symbolic execution: exploring multiple paths by solving once",
    "author": "Zhang, Yufeng and Chen, Zhenbang and Shuai, Ziqi and Zhang, Tianqi and Li, Kenli and Wang, Ji",
    "abstract": "Path explosion and constraint solving are two challenges to symbolic execution's scalability. Symbolic execution explores the program's path space with a searching strategy and invokes the underlying constraint solver in a black-box manner to check the feasibility of a path. Inside the constraint solver, another searching procedure is employed to prove or disprove the feasibility. Hence, there exists the problem of double searchings in symbolic execution. In this paper, we propose to unify the double searching procedures to improve the scalability of symbolic execution. We propose Multiplex Symbolic Execution (MuSE) that utilizes the intermediate assignments during the constraint solving procedure to generate new program inputs. MuSE maps the constraint solving procedure to the path exploration in symbolic execution and explores multiple paths in one time of solving. We have implemented MuSE on two symbolic execution tools (based on KLEE and JPF) and three commonly used constraint solving algorithms. The results of the extensive experiments on real-world benchmarks indicate that MuSE has orders of magnitude speedup to achieve the same coverage.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416645",
    "comment": ""
  },
  {
    "title": "Zeror: speed up fuzzing with coverage-sensitive tracing and scheduling",
    "author": "Zhou, Chijin and Wang, Mingzhe and Liang, Jie and Liu, Zhe and Jiang, Yu",
    "abstract": "Coverage-guided fuzzing is one of the most popular software testing techniques for vulnerability detection. While effective, current fuzzing methods suffer from significant performance penalty due to instrumentation overhead, which limits its practical use. Existing solutions improve the fuzzing speed by decreasing instrumentation overheads but sacrificing coverage accuracy, which results in unstable performance of vulnerability detection.In this paper, we propose a coverage-sensitive tracing and scheduling framework Zeror that can improve the performance of existing fuzzers, especially in their speed and vulnerability detection. The Zeror is mainly made up of two parts: (1) a self-modifying tracing mechanism to provide a zero-overhead instrumentation for more effective coverage collection, and (2) a real-time scheduling mechanism to support adaptive switch between the zero-overhead instrumented binary and the fully instrumented binary for better vulnerability detection. In this way, Zeror is able to decrease collection overhead and preserve fine-grained coverage for guidance.For evaluation, we implement a prototype of Zeror and evaluate it on Google fuzzer-test-suite, which consists of 24 widely-used applications. The results show that Zeror performs better than existing fuzzing speed-up frameworks such as Untracer and INSTRIM, improves the execution speed of the state-of-the-art fuzzers such as AFL and MOPT by 159.80\\%, helps them achieve better coverage (averagely 10.14\\% for AFL, 6.91\\% for MOPT) and detect vulnerabilities faster (averagely 29.00\\% for AFL, 46.99\\% for MOPT).",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416572",
    "comment": ""
  },
  {
    "title": "Detecting and explaining self-admitted technical debts with attention-based neural networks",
    "author": "Wang, Xin and Liu, Jin and Li, Li and Chen, Xiao and Liu, Xiao and Wu, Hao",
    "abstract": "Self-Admitted Technical Debt (SATD) is a sub-type of technical debt. It is introduced to represent such technical debts that are intentionally introduced by developers in the process of software development. While being able to gain short-term benefits, the introduction of SATDs often requires to be paid back later with a higher cost, e.g., introducing bugs to the software or increasing the complexity of the software.To cope with these issues, our community has proposed various machine learning-based approaches to detect SATDs. These approaches, however, are either not generic that usually require manual feature engineering efforts or do not provide promising means to explain the predicted outcomes. To that end, we propose to the community a novel approach, namely HATD (Hybrid Attention-based method for self-admitted Technical Debt detection), to detect and explain SATDs using attention-based neural networks. Through extensive experiments on 445,365 comments in 20 projects, we show that HATD is effective in detecting SATDs on both in-the-lab and in-the-wild datasets under both within-project and cross-project settings. HATD also outperforms the state-of-the-art approaches in detecting and explaining SATDs.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416583",
    "comment": ""
  },
  {
    "title": "OCoR: an overlapping-aware code retriever",
    "author": "Zhu, Qihao and Sun, Zeyu and Liang, Xiran and Xiong, Yingfei and Zhang, Lu",
    "abstract": "Code retrieval helps developers reuse code snippets in the open-source projects. Given a natural language description, code retrieval aims to search for the most relevant code relevant among a set of code snippets. Existing state-of-the-art approaches apply neural networks to code retrieval. However, these approaches still fail to capture an important feature: overlaps. The overlaps between different names used by different people indicate that two different names may be potentially related (e.g., \"message\" and \"msg\"), and the overlaps between identifiers in code and words in natural language descriptions indicate that the code snippet and the description may potentially be related.To address this problem, we propose a novel neural architecture named OCoR1, where we introduce two specifically-designed components to capture overlaps: the first embeds names by characters to capture the overlaps between names, and the second introduces a novel overlap matrix to represent the degrees of overlaps between each natural language word and each identifier.The evaluation was conducted on two established datasets. The experimental results show that OCoR significantly outperforms the existing state-of-the-art approaches and achieves 13.1\\% to 22.3\\% improvements. Moreover, we also conducted several in-depth experiments to help understand the performance of the different components in OCoR.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416530",
    "comment": ""
  },
  {
    "title": "Understanding performance concerns in the API documentation of data science libraries",
    "author": "Tao, Yida and Jiang, Jiefang and Liu, Yepang and Xu, Zhiwu and Qin, Shengchao",
    "abstract": "The development of efficient data science applications is often impeded by unbearably long execution time and rapid RAM exhaustion. Since API documentation is the primary information source for troubleshooting, we investigate how performance concerns are documented in popular data science libraries. Our quantitative results reveal the prevalence of data science APIs that are documented in performance-related context and the infrequent maintenance activities on such documentation. Our qualitative analyses further reveal that crowd documentation like Stack Overflow and GitHub are highly complementary to official documentation in terms of the API coverage, the knowledge distribution, as well as the specific information conveyed through performance-related content. Data science practitioners could benefit from our findings by learning a more targeted search strategy for resolving performance issues. Researchers can be more assured of the advantages of integrating both the official and the crowd documentation to achieve a holistic view on the performance concerns in data science development.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416543",
    "comment": ""
  },
  {
    "title": "On the effectiveness of unified debugging: an extensive study on 16 program repair systems",
    "author": "Benton, Samuel and Li, Xia and Lou, Yiling and Zhang, Lingming",
    "abstract": "Automated debugging techniques, including fault localization and program repair, have been studied for over a decade. However, the only existing connection between fault localization and program repair is that fault localization computes the potential buggy elements for program repair to patch. Recently, a pioneering work, ProFL, explored the idea of unified debugging to unify fault localization and program repair in the other direction for the first time to boost both areas. More specifically, ProFL utilizes the patch execution results from one state-of-the-art repair system, PraPR, to help improve state-of-the-art fault localization. In this way, ProFL not only improves fault localization for manual repair, but also extends the application scope of automated repair to all possible bugs (not only the small ratio of bugs that can be automaticallyfi xed). However, ProFL only considers one APR system (i.e., PraPR), and it is not clear how other existing APR systems based on different designs contribute to unified debugging. In this work, we perform an extensive study of the unified-debugging approach on 16 state-of-the-art program repair systems for the first time. Our experimental results on the widely studied Defects4J benchmark suite reveal various practical guidelines for unified debugging, such as (1) nearly all the studied 16 repair systems can positively contribute to unified debugging despite their varying repairing capabilities, (2) repair systems targeting multi-edit patches can bring extraneous noise into unified debugging, (3) repair systems with more executed/plausible patches tend to perform better for unified debugging, and (4) unified debugging effectiveness does not rely on the availability of correct patches in automated repair. Based on our results, we further propose an advanced unified debugging technique, UniDebug++, which can localize over 20\\% more bugs within Top-1 positions than state-of-the-art unified debugging technique, ProFL.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416566",
    "comment": ""
  },
  {
    "title": "Automated third-party library detection for Android applications: are we there yet?",
    "author": "Zhan, Xian and Fan, Lingling and Liu, Tianming and Chen, Sen and Li, Li and Wang, Haoyu and Xu, Yifei and Luo, Xiapu and Liu, Yang",
    "abstract": "Third-party libraries (TPLs) have become a significant part of the Android ecosystem. Developers can employ various TPLs with different functionalities to facilitate their app development. Unfortunately, the popularity of TPLs also brings new challenges and even threats. TPLs may carry malicious or vulnerable code, which can infect popular apps to pose threats to mobile users. Besides, the code of third-party libraries could constitute noises in some downstream tasks (e.g., malware and repackaged app detection). Thus, researchers have developed various tools to identify TPLs. However, no existing work has studied these TPL detection tools in detail; different tools focus on different applications with performance differences, but little is known about them.To better understand existing TPL detection tools and dissect TPL detection techniques, we conduct a comprehensive empirical study to fill the gap by evaluating and comparing all publicly available TPL detection tools based on four criteria: effectiveness, efficiency, code obfuscation-resilience capability, and ease of use. We reveal their advantages and disadvantages based on a systematic and thorough empirical study. Furthermore, we also conduct a user study to evaluate the usability of each tool. The results show that LibScout outperforms others regarding effectiveness, LibRadar takes less time than others and is also regarded as the most easy-to-use one, and LibPecker performs the best in defending against code obfuscation techniques. We further summarize the lessons learned from different perspectives, including users, tool implementation, and researchers. Besides, we enhance these open-sourced tools by fixing their limitations to improve their detection ability. We also build an extensible framework that integrates all existing available TPL detection tools, providing online service for the research community. We make publicly available the evaluation dataset and enhanced tools. We believe our work provides a clear picture of existing TPL detection techniques and also give a road-map for future directions.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416582",
    "comment": ""
  },
  {
    "title": "CCGraph: a PDG-based code clone detector with approximate graph matching",
    "author": "Zou, Yue and Ban, Bihuan and Xue, Yinxing and Xu, Yun",
    "abstract": "Software clone detection is an active research area, which is very important for software maintenance, bug detection, etc. The two pieces of cloned code reflect some similarities or equivalents in the syntax or structure of the code representations. There are many representations of code like AST, token, PDG, etc. The PDG (Program Dependency Graph) of source code can contain both syntactic and structural information. However, most existing PDG-based tools are quite time-consuming and miss many clones because they detect code clones with exact graph matching by using subgraph isomorphism. In this paper, we propose a novel PDG-based code clone detector, CCGraph, that uses graph kernels. Firstly, we normalize the structure of PDGs and design a two-stage filtering strategy by measuring the characteristic vectors of codes. Then we detect the code clones by using an approximate graph matching algorithm based on the reforming WL (Weisfeiler-Lehman) graph kernel. Experiment results show that CCGraph retains a high accuracy, has both better recall and F1-score values, and detects more semantic clones than other two related state-of-the-art tools. Besides, CCGraph is much more efficient than the existing PDG-based tools.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416541",
    "comment": ""
  },
  {
    "title": "Towards generating thread-safe classes automatically",
    "author": "Wang, Haichi and Wang, Zan and Sun, Jun and Liu, Shuang and Sadiq, Ayesha and Li, Yuan-Fang",
    "abstract": "The existing concurrency model for Java (or C) requires programmers to design and implement thread-safe classes by explicitly acquiring locks and releasing locks. Such a model is error-prone and is the reason for many concurrency bugs. While there are alternative models like transactional memory, manually writing locks remains prevalent in practice. In this work, we propose AutoLock, which aims to solve the problem by fully automatically generating thread-safe classes. Given a class which is assumed to be correct with sequential clients, AutoLock automatically generates a thread-safe class which is linearizable, and does it in a way without requiring a specification of the class. AutoLock takes three steps: (1) infer access annotations (i.e., abstract information on how variables are accessed and aliased), (2) synthesize a locking policy based on the access annotations, and (3) consistently implement the locking policy. AutoLock has been evaluated on a set of benchmark programs and the results show that AutoLock generates thread-safe classes effectively and could have prevented existing concurrency bugs.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416625",
    "comment": ""
  },
  {
    "title": "No strings attached: an empirical study of string-related software bugs",
    "author": "Eghbali, Aryaz and Pradel, Michael",
    "abstract": "Strings play many roles in programming because they often contain complex and semantically rich information. For example, programmers use strings to filter inputs via regular expression matching, to express the names of program elements accessed through some form of reflection, to embed code written in another formal language, and to assemble textual output produced by a program. The omnipresence of strings leads to a wide range of mistakes that developers may make, yet little is currently known about these mistakes. The lack of knowledge about string-related bugs leads to developers repeating the same mistakes again and again, and to poor support for finding and fixing such bugs. This paper presents the first empirical study of the root causes, consequences, and other properties of string-related bugs. We systematically study 204 string-related bugs in a diverse set of projects written in JavaScript, a language where strings play a particularly important role. Our findings include (i) that many string-related mistakes are caused by a recurring set of root cause patterns, such as incorrect string literals and regular expressions, (ii) that string-related bugs have a diverse set of consequences, including incorrect output or silent omission of expected behavior, (iii) that fixing string-related bugs often requires changing just a single line, with many of the required repair ingredients available in the surrounding code, (iv) that string-related bugs occur across all parts of applications, including the core components, and (v) that almost none of these bugs are detected by existing static analyzers. Our findings not only show the importance and prevalence of string-related bugs, but they help developers to avoid common mistakes and tool builders to tackle the challenge of finding and fixing string-related bugs.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416576",
    "comment": ""
  },
  {
    "title": "Automated patch correctness assessment: how far are we?",
    "author": "Wang, Shangwen and Wen, Ming and Lin, Bo and Wu, Hongjun and Qin, Yihao and Zou, Deqing and Mao, Xiaoguang and Jin, Hai",
    "abstract": "Test-based automated program repair (APR) has attracted huge attention from both industry and academia. Despite the significant progress made in recent studies, the overfitting problem (i.e., the generated patch is plausible but overfitting) is still a major and long-standing challenge. Therefore, plenty of techniques have been proposed to assess the correctness of patches either in the patch generation phase or in the evaluation of APR techniques. However, the effectiveness of existing techniques has not been systematically compared and little is known to their advantages and disadvantages. To fill this gap, we performed a large-scale empirical study in this paper. Specifically, we systematically investigated the effectiveness of existing automated patch correctness assessment techniques, including both static and dynamic ones, based on 902 patches automatically generated by 21 APR tools from 4 different categories. Our empirical study revealed the following major findings: (1) static code features with respect to patch syntax and semantics are generally effective in differentiating overfitting patches over correct ones; (2) dynamic techniques can generally achieve high precision while heuristics based on static code features are more effective towards recall; (3) existing techniques are more effective towards certain projects and types of APR techniques while less effective to the others; (4) existing techniques are highly complementary to each other. For instance, a single technique can only detect at most 53.5\\% of the overfitting patches while 93.3\\% of them can be detected by at least one technique when the oracle information is available. Based on our findings, we designed an integration strategy to first integrate static code features via learning, and then combine with others by the majority voting strategy. Our experiments show that the strategy can enhance the performance of existing patch correctness assessment techniques significantly.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416590",
    "comment": ""
  },
  {
    "title": "Evaluating representation learning of code changes for predicting patch correctness in program repair",
    "author": "Tian, Haoye and Liu, Kui and Kabor\\'{e}, Abdoul Kader and Koyuncu, Anil and Li, Li and Klein, Jacques and Bissyand\\'{e}, Tegawend\\'{e} F.",
    "abstract": "A large body of the literature of automated program repair develops approaches where patches are generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state of the art explore research directions that require dynamic information or that rely on manually-crafted heuristics, we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness. Our empirical work mainly investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations. We report on findings based on embeddings produced by pre-trained and re-trained neural networks. Experimental results demonstrate the potential of embeddings to empower learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based embeddings associated with logistic regression yielded an AUC value of about 0.8 in the prediction of patch correctness on a deduplicated dataset of 1000 labeled patches. Our investigations show that learned representations can lead to reasonable performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. These representations may further be complementary to features that were carefully (manually) engineered in the literature.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416532",
    "comment": ""
  },
  {
    "title": "Scalable multiple-view analysis of reactive systems via bidirectional model transformations",
    "author": "Tsigkanos, Christos and Li, Nianyu and Jin, Zhi and Hu, Zhenjiang and Ghezzi, Carlo",
    "abstract": "Systematic model-driven design and early validation enable engineers to verify that a reactive system does not violate its requirements before actually implementing it. Requirements may come from multiple stakeholders, who are often concerned with different facets - design typically involves different experts having different concerns and views of the system. Engineers start from a specification which may be sourced from some domain model, while validation is often done on state-transition structures that support model checking. Two computationally expensive steps may work against scalability: transformation from specification to state-transition structures, and model checking. We propose a technique that makes the former efficient and also makes the resulting transition systems small enough to be efficiently verified. The technique automatically projects the specification into submodels depending on a property sought to be evaluated, which captures some stakeholder's viewpoint. The resulting reactive system submodel is then transformed into a state-transition structure and verified. The technique achieves cone-of-influence reduction, by slicing at the specification model level. Submodels are analysis-equivalent to the corresponding full model. If stakeholders propose a change to a submodel based on their own view, changes are automatically propagated to the specification model and other views affected. Automated reflection is achieved thanks to bidirectional model transformations, ensuring correctness. We cast our proposal in the context of graph-based reactive systems whose dynamics is described by rewriting rules. We demonstrate our view-based framework in practice on a case study within cyber-physical systems.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416579",
    "comment": ""
  },
  {
    "title": "Trace-checking signal-based temporal properties: a model-driven approach",
    "author": "Boufaied, Chaima and Menghi, Claudio and Bianculli, Domenico and Briand, Lionel and Parache, Yago Isasi",
    "abstract": "Signal-based temporal properties (SBTPs) characterize the behavior of a system when its inputs and outputs are signals over time; they are very common for the requirements specification of cyber-physical systems. Although there exist several specification languages for expressing SBTPs, such languages either do not easily allow the specification of important types of properties (such as spike or oscillatory behaviors), or are not supported by (efficient) trace-checking procedures.In this paper, we propose SB-TemPsy, a novel model-driven trace-checking approach for SBTPs. SB-TemPsy provides (i) SB-TemPsy-DSL, a domain-specific language that allows the specification of SBTPs covering the most frequent requirement types in cyber-physical systems, and (ii) SB-TemPsy-Check, an efficient, model-driven trace-checking procedure. This procedure reduces the problem of checking an SB-TemPsy-DSL property over an execution trace to the problem of evaluating an Object Constraint Language constraint on a model of the execution trace.We evaluated our contributions by assessing the expressiveness of SB-TemPsy-DSL and the applicability of SB-TemPsy-Check using a representative industrial case study in the satellite domain. SB-TemPsy-DSL could express 97\\% of the requirements of our case study and SB-TemPsy-Check yielded a trace-checking verdict in 87\\% of the cases, with an average checking time of 48.7 s. From a practical standpoint and compared to state-of-the-art alternatives, our approach strikes a better trade-off between expressiveness and performance as it supports a large set of property types that can be checked, in most cases, within practical time limits.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416631",
    "comment": ""
  },
  {
    "title": "Attend and represent: a novel view on algorithm selection for software verification",
    "author": "Richter, Cedric and Wehrheim, Heike",
    "abstract": "Today, a plethora of different software verification tools exist. When having a concrete verification task at hand, software developers thus face the problem of algorithm selection. Existing algorithm selectors for software verification typically use handpicked program features together with (1) either manually designed selection heuristics or (2) machine learned strategies. While the first approach suffers from not being transferable to other selection problems, the second approach lacks interpretability, i.e., insights into reasons for choosing particular tools.In this paper, we propose a novel approach to algorithm selection for software verification. Our approach employs representation learning together with an attention mechanism. Representation learning circumvents feature engineering, i.e., avoids the handpicking of program features. Attention permits a form of interpretability of the learned selectors. We have implemented our approach and have experimentally evaluated and compared it with existing approaches. The evaluation shows that representation learning does not only outperform manual feature engineering, but also enables transferability of the learning model to other selection tasks.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416633",
    "comment": ""
  },
  {
    "title": "Cross-contract static analysis for detecting practical reentrancy vulnerabilities in smart contracts",
    "author": "Xue, Yinxing and Ma, Mingliang and Lin, Yun and Sui, Yulei and Ye, Jiaming and Peng, Tianyong",
    "abstract": "Reentrancy bugs, one of the most severe vulnerabilities in smart contracts, have caused huge financial loss in recent years. Researchers have proposed many approaches to detecting them. However, empirical studies have shown that these approaches suffer from undesirable false positives and false negatives, when the code under detection involves the interaction between multiple smart contracts.In this paper, we propose an accurate and efficient cross-contract reentrancy detection approach in practice. Rather than design rule-of-thumb heuristics, we conduct a large empirical study of 11714 real-world contracts from Etherscan against three well-known general-purpose security tools for reentrancy detection. We manually summarized the reentrancy scenarios where the state-of-the-art approaches cannot address. Based on the empirical evidence, we present Clairvoyance, a cross-function and cross-contract static analysis to detect reentrancy vulnerabilities in real world with significantly higher accuracy. To reduce false negatives, we enable, for the first time, a cross-contract call chain analysis by tracking possibly tainted paths. To reduce false positives, we systematically summarized five major path protective techniques (PPTs) to support fast yet precise path feasibility checking. We implemented our approach and compared Clairvoyance with five state-of-the-art tools on 17770 real-worlds contracts. The results show that Clairvoyance yields the best detection accuracy among all the five tools and also finds 101 unknown reentrancy vulnerabilities.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416553",
    "comment": ""
  },
  {
    "title": "Cats are not fish: deep learning testing calls for out-of-distribution awareness",
    "author": "Berend, David and Xie, Xiaofei and Ma, Lei and Zhou, Lingjun and Liu, Yang and Xu, Chi and Zhao, Jianjun",
    "abstract": "As Deep Learning (DL) is continuously adopted in many industrial applications, its quality and reliability start to raise concerns. Similar to the traditional software development process, testing the DL software to uncover its defects at an early stage is an effective way to reduce risks after deployment. According to the fundamental assumption of deep learning, the DL software does not provide statistical guarantee and has limited capability in handling data that falls outside of its learned distribution, i.e., out-of-distribution (OOD) data. Although recent progress has been made in designing novel testing techniques for DL software, which can detect thousands of errors, the current state-of-the-art DL testing techniques usually do not take the distribution of generated test data into consideration. It is therefore hard to judge whether the \"identified errors\" are indeed meaningful errors to the DL application (i.e., due to quality issues of the model) or outliers that cannot be handled by the current model (i.e., due to the lack of training data). Tofill this gap, we take the first step and conduct a large scale empirical study, with a total of 451 experiment configurations, 42 deep neural networks (DNNs) and 1.2 million test data instances, to investigate and characterize the impact of OOD-awareness on DL testing. We further analyze the consequences when DL systems go into production by evaluating the effectiveness of adversarial retraining with distribution-aware errors. The results confirm that introducing data distribution awareness in both testing and enhancement phases outperforms distribution unaware retraining by up to 21.5\\%.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416609",
    "comment": ""
  },
  {
    "title": "Metamorphic object insertion for testing object detection systems",
    "author": "Wang, Shuai and Su, Zhendong",
    "abstract": "Recent advances in deep neural networks (DNNs) have led to object detectors (ODs) that can rapidly process pictures or videos, and recognize the objects that they contain. Despite the promising progress by industrial manufacturers such as Amazon and Google in commercializing deep learning-based ODs as a standard computer vision service, ODs --- similar to traditional software --- may still produce incorrect results. These errors, in turn, can lead to severe negative outcomes for the users. For instance, an autonomous driving system that fails to detect pedestrians can cause accidents or even fatalities. However, despite their importance, principled, systematic methods for testing ODs do not yet exist.To fill this critical gap, we introduce the design and realization of MetaOD, a metamorphic testing system specifically designed for ODs to effectively uncover erroneous detection results. To this end, we (1) synthesize natural-looking images by inserting extra object instances into background images, and (2) design metamorphic conditions asserting the equivalence of OD results between the original and synthetic images after excluding the prediction results on the inserted objects. MetaOD is designed as a streamlined workflow that performs object extraction, selection, and insertion. We develop a set of practical techniques to realize an effective workflow, and generate diverse, natural-looking images for testing. Evaluated on four commercial OD services and four pretrained models provided by the TensorFlow API, MetaOD found tens of thousands of detection failures. To further demonstrate the practical usage of MetaOD, we use the synthetic images that cause erroneous detection results to retrain the model. Our results show that the model performance is significantly increased, from an mAP score of 9.3 to an mAP score of 10.5.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416584",
    "comment": ""
  },
  {
    "title": "Seven reasons why: an in-depth study of the limitations of random test input generation for Android",
    "author": "Behrang, Farnaz and Orso, Alessandro",
    "abstract": "Experience paper: Testing of mobile apps is time-consuming and requires a great deal of manual effort. For this reason, industry and academic researchers have proposed a number of test input generation techniques for automating app testing. Although useful, these techniques have weaknesses and limitations that often prevent them from achieving high coverage. We believe that one of the reasons for these limitations is that tool developers tend to focus mainly on improving the strategy the techniques employ to explore app behavior, whereas limited effort has been put into investigating other ways to improve the performance of these techniques. To address this problem, and get a better understanding of the limitations of input-generation techniques for mobile apps, we conducted an in-depth study of the limitations of Monkey-arguably the most widely used tool for automated testing of Android apps. Specifically, in our study, we manually analyzed Monkey's performance on a benchmark of 64 apps to identify the common limitations that prevent the tool from achieving better coverage results. We then assessed the coverage improvement that Monkey could achieve if these limitations were eliminated. In our analysis of the results, we also discuss whether other existing test input generation tools suffer from these common limitations and provide insights on how they could address them.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416567",
    "comment": ""
  },
  {
    "title": "Test automation in open-source Android apps: a large-scale empirical study",
    "author": "Lin, Jun-Wei and Salehnamadi, Navid and Malek, Sam",
    "abstract": "Automated testing of mobile apps has received significant attention in recent years from researchers and practitioners alike. In this paper, we report on the largest empirical study to date, aimed at understanding the test automation culture prevalent among mobile app developers. We systematically examined more than 3.5 million repositories on GitHub and identified more than 12, 000 non-trivial and real-world Android apps. We then analyzed these non-trivial apps to investigate (1) the prevalence of adoption of test automation; (2) working habits of mobile app developers in regards to automated testing; and (3) the correlation between the adoption of test automation and the popularity of projects. Among others, we found that (1) only 8\\% of the mobile app development projects leverage automated testing practices; (2) developers tend to follow the same test automation practices across projects; and (3) popular projects, measured in terms of the number of contributors, stars, and forks on GitHub, are more likely to adopt test automation practices. To understand the rationale behind our observations, we further conducted a survey with 148 professional and experienced developers contributing to the subject apps. Our findings shed light on the current practices and future research directions pertaining to test automation for mobile app development.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416623",
    "comment": ""
  },
  {
    "title": "Synthesis-based resolution of feature interactions in cyber-physical systems",
    "author": "Gafford, Benjamin and D\\\"{u}rschmid, Tobias and Moreno, Gabriel A. and Kang, Eunsuk",
    "abstract": "The feature interaction problem arises when two or more independent features interact with each other in an undesirable manner. Feature interactions remain a challenging and important problem in emerging domains of cyber-physical systems (CPS), such as intelligent vehicles, unmanned aerial vehicles (UAVs) and the Internet of Things (IoT), where the outcome of an unexpected interaction may result in a safety failure. Existing approaches to resolving feature interactions rely on priority lists or fixed strategies, but may not be effective in scenarios where none of the competing feature actions are satisfactory with respect to system requirements. This paper proposes a novel synthesis-based approach to resolution, where a conflict among features is resolved by synthesizing an action that best satisfies the specification of desirable system behaviors in the given environmental context. Unlike existing resolution methods, our approach is capable of producing a desirable system outcome even when none of the conflicting actions are satisfactory. The effectiveness of the proposed approach is demonstrated using a case study involving interactions among safety-critical features in an autonomous drone.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416630",
    "comment": ""
  },
  {
    "title": "MoFuzz: a fuzzer suite for testing model-driven software engineering tools",
    "author": "Nguyen, Hoang Lam and Nassar, Nebras and Kehrer, Timo and Grunske, Lars",
    "abstract": "Fuzzing or fuzz testing is an established technique that aims to discover unexpected program behavior (e.g., bugs, security vulnerabilities, or crashes) by feeding automatically generated data into a program under test. However, the application of fuzzing to test Model-Driven Software Engineering (MDSE) tools is still limited because of the difficulty of existing fuzzers to provide structured, well-typed inputs, namely models that conform to typing and consistency constraints induced by a given meta-model and underlying modeling framework. By drawing from recent advances on both fuzz testing and automated model generation, we present three different approaches for fuzzing MDSE tools: A graph grammar-based fuzzer and two variants of a coverage-guided mutation-based fuzzer working with different sets of model mutation operators. Our evaluation on a set of real-world MDSE tools shows that our approaches can outperform both standard fuzzers and model generators w.r.t. their fuzzing capabilities. Moreover, we found that each of our approaches comes with its own strengths and weaknesses in terms of fault finding capabilities and the ability to cover different aspects of the system under test. Thus the approaches complement each other, forming a fuzzer suite for testing MDSE tools.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416668",
    "comment": ""
  },
  {
    "title": "Prober: practically defending overflows with page protection",
    "author": "Liu, Hongyu and Tian, Ruiqin and Ren, Bin and Liu, Tongping",
    "abstract": "Heap-based overflows are still not completely solved even after decades of research. This paper proposes Prober, a novel system aiming to detect and prevent heap overflows in the production environment. Prober leverages a key observation based on the analysis of dozens of real bugs: all heap overflows are related to arrays. Based on this observation, Prober only focuses on array-related heap objects, instead of all heap objects. Prober utilizes static analysis to label all susceptible call-stacks during the compilation, and then employs the page protection to detect any invalid accesses during the runtime. In addition to this, Prober integrates multiple existing methods together to ensure the efficiency of its detection. Overall, Prober introduces almost negligible performance overhead, with 1.5\\% on average. Prober not only stops possible attacks on time, but also reports the faulty instructions that could guide bug fixes. Prober is ready for deployment due to its effectiveness and low overhead.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416533",
    "comment": ""
  },
  {
    "title": "MinerRay: semantics-aware analysis for ever-evolving cryptojacking detection",
    "author": "Romano, Alan and Zheng, Yunhui and Wang, Weihang",
    "abstract": "Recent advances in web technology have made in-browser cryptomining a viable funding model. However, these services have been abused to launch large-scale cryptojacking attacks to secretly mine cryptocurrency in browsers. To detect them, various signature-based or runtime feature-based methods have been proposed. However, they can be imprecise or easily circumvented. To this end, we propose MinerRay, a generic scheme to detect malicious in-browser cryptominers. Instead of leveraging unreliable external patterns, MinerRay infers the essence of cryptomining behaviors that differentiate mining from common browser activities in both WebAssembly and JavaScript contexts. Additionally, to detect stealthy mining activities without user consents, MinerRay checks if the miner can only be instantiated from user actions.MinerRay was evaluated on over 1 million websites. It detected cryptominers on 901 websites, where 885 secretly start mining without user consent. Besides, we compared MinerRay with five state-of-the-art signature-based or behavior-based cryptominer detectors (MineSweeper, CMTracker, Outguard, No Coin, and minerBlock). We observed that emerging miners with new signatures or new services were detected by MinerRay but missed by others. The results show that our proposed technique is effective and robust in detecting evolving cryptominers, yielding more true positives, and fewer errors.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416580",
    "comment": ""
  },
  {
    "title": "Summary-based symbolic evaluation for smart contracts",
    "author": "Feng, Yu and Torlak, Emina and Bodik, Rastislav",
    "abstract": "This paper presents Solar, a system for automatic synthesis of adversarial contracts that exploit vulnerabilities in a victim smart contract. To make the synthesis tractable, we introduce a query language as well as summary-based symbolic evaluation, which significantly reduces the number of instructions that our synthesizer needs to evaluate symbolically, without compromising the precision of the vulnerability query. We encoded common vulnerabilities of smart contracts and evaluated Solar on the entire data set from Etherscan. Our experiments demonstrate the benefits of summary-based symbolic evaluation and show that Solar outperforms state-of-the-art smart contracts analyzers, teether, Mythril, and ContractFuzzer, in terms of running time and precision.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3416646",
    "comment": ""
  },
  {
    "title": "Applying learning techniques to oracle synthesis",
    "author": "Molina, Facundo",
    "abstract": "Software reliability is a primary concern in the construction of software, and thus a fundamental component in the definition of software quality. Analyzing software reliability requires a specification of the intended behavior of the software under analysis. Unfortunately, software many times lacks such specifications. This issue seriously diminishes the analyzability of software with respect to its reliability. Thus, finding novel techniques to capture the intended software behavior in the form of specifications would allow us to exploit them for automated reliability analysis.Our research focuses on the application of learning techniques to automatically distinguish correct from incorrect software behavior. The aim here is to decrease the developer's effort in specifying oracles, and instead generating them from actual software behaviors.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415287",
    "comment": ""
  },
  {
    "title": "Automated generation of client-specific backends utilizing existing microservices and architectural knowledge",
    "author": "Wieber, Nils",
    "abstract": "The design and development of production-grade microservice backends is a tedious and error-prone task. In particular, they must be capable of handling all Functional Requirements (FRs) and all Non-Functional Requirements (NFRs) (like security) including all operational requirements (like monitoring). This becomes even more difficult if there are many clients with different roles, linked to diverse (non-)functional requirements and many existing services are involved, which have to consider these in a consistent way. In this paper we present a model-driven approach that automatically generates client-specific production-grade backends by incorporating previously expressed architectural knowledge out of an interpretable specification of the targeted APIs and the NFRs.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415283",
    "comment": ""
  },
  {
    "title": "SAT-based arithmetic support for alloy",
    "author": "Cornejo, C\\'{e}sar",
    "abstract": "Formal specifications in Alloy are organized around user-defined data domains, associated with signatures, with almost no support for built-in datatypes. This minimality in the built-in datatypes provided by the language is one of its main features, as it contributes to the automated analyzability of models. One of the few built-in datatypes available in Alloy specifications are integers, whose SAT-based treatment allows only for small bit-widths. In many contexts, where relational datatypes dominate, the use of integers may be auxiliary, e.g., in the use of cardinality constraints and other features. However, as the applications of Alloy are increased, e.g., with the use of the language and its tool support as backend engine for different analysis tasks, the provision of efficient support for numerical datatypes becomes a need. In this work, we present our current preliminary approach to providing an efficient, scalable and user-friendly extension to Alloy, with arithmetic support for numerical datatypes. Our implementation allows for arithmetic with varying precisions, and is implemented via standard Alloy constructions, thus resorting to SAT solving for resolving arithmetic constraints in models.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415285",
    "comment": ""
  },
  {
    "title": "Towards robust production machine learning systems: managing dataset shift",
    "author": "Abdelkader, Hala",
    "abstract": "The advances in machine learning (ML) have stimulated the integration of their capabilities into software systems. However, there is a tangible gap between software engineering and machine learning practices, that is delaying the progress of intelligent services development. Software organisations are devoting effort to adjust the software engineering processes and practices to facilitate the integration of machine learning models. Machine learning researchers as well are focusing on improving the interpretability of machine learning models to support overall system robustness. Our research focuses on bridging this gap through a methodology that evaluates the robustness of machine learning-enabled software engineering systems. In particular, this methodology will automate the evaluation of the robustness properties of software systems against dataset shift problems in ML. It will also feature a notification mechanism that facilitates the debugging of ML components.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415281",
    "comment": ""
  },
  {
    "title": "Towards transparency-encouraging partial software disclosure to enable trust in data usage",
    "author": "Schindler, Christian",
    "abstract": "Whenever software components process personal or private data, appropriate data protection mechanisms are mandatory. An essential factor in achieving trust and transparency is not to give preference to a single party but to make it possible to audit the data usage in an unbiased way. The scenario in mind for this contribution contains (i) users bringing in sensitive data they want to be safe, (ii) developers building software-based services whose Intellectual Properties (IPs) they desire to protect, and (iii) platform providers wanting to be trusted and to be able to rely on the developers integrity. The authors see these interests as an insufficiently solved field of tension that can be relaxed by a suitable level of transparently represented software components to give insights without exposing every detail.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415282",
    "comment": ""
  },
  {
    "title": "Using defect prediction to improve the bug detection capability of search-based software testing",
    "author": "Perera, Anjana",
    "abstract": "Automated test generators, such as search based software testing (SBST) techniques, replace the tedious and expensive task of manually writing test cases. SBST techniques are effective at generating tests with high code coverage. However, is high code coverage sufficient to maximise the number of bugs found? We argue that SBST needs to be focused to search for test cases in defective areas rather in non-defective areas of the code in order to maximise the likelihood of discovering the bugs. Defect prediction algorithms give useful information about the bug-prone areas in software. Therefore, we formulate the objective of this thesis: Improve the bug detection capability of SBST by incorporating defect prediction information. To achieve this, we devise two research objectives, i.e., 1) Develop a novel approach (SBSTCL) that allocates time budget to classes based on the likelihood of classes being defective, and 2) Develop a novel strategy (SBSTML) to guide the underlying search algorithm (i.e., genetic algorithm) towards the defective areas in a class. Through empirical evaluation on 434 real reported bugs in the Defects4J dataset, we demonstrate that our novel approach, SBSTCL, is significantly more efficient than the state of the art SBST when they are given a tight time budget in a resource constrained environment.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415286",
    "comment": ""
  },
  {
    "title": "The new approach to IT testing: real transaction-based automated validation solution",
    "author": "Kim, YongSik and Min, SoAh and Kim, YouKyung",
    "abstract": "Traditional IT projects have rolled out newly developed software or systems after iterating manual tests based on the scenarios and cases that are considered sufficient. However, due to the time and budget limitation of IT projects, these traditional tests almost always fail to include all the possible scenarios and cases of the real world. Thus, we cannot eliminate all potential defects before go-live and unexpected failures might occur as a result, which can lead to severe damage to both customers and IT project contractors.This paper demonstrates a real transaction-based automated testing approach named 'PerfecTwin' with several real-world examples. PerfecTwin overcomes the above limitations of the traditional testing by running the new and old systems side-by-side, automatically validating the new system against the old system's actual transactions, in real time, which can eliminate almost all potential defects before go-live.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3421839",
    "comment": ""
  },
  {
    "title": "Automatic generation of IFTTT mashup infrastructures",
    "author": "Liu, Lei and Bahrami, Mehdi and Chen, Wei-Peng",
    "abstract": "In recent years, IF-This-Then-That (IFTTT) services are becoming more and more popular. Many platforms such as Zapier, IFTTT.com, and Workato provide such services, which allow users to create workflows with \"triggers\" and \"actions\" by using Web Application Programming Interfaces (APIs). However, the number of IFTTT recipes in the above platforms increases much slower than the growth of Web APIs. This is because human efforts are still largely required to build and deploy IFTTT recipes in the above platforms. To address this problem, in this paper, we present an automation tool to automatically generate the IFTTT mashup infrastructure. The proposed tool provides 5 REST APIs, which can automatically generate triggers, rules, and actions in AWS, and create a workflow XML to describe an IFTTT mashup by connecting the triggers, rules, and actions. This workflow XML is automatically sent to Fujitsu RunMyProcess (RMP) to set up and execute IFTTT mashup. The proposed tool, together with its associated method and procedure, enables an end-to-end solution for automatically creating, deploying, and executing IFTTT mashups in a few seconds, which can greatly reduce the development cycle and cost for new IFTTT mashups.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3421837",
    "comment": ""
  },
  {
    "title": "Towards building robust DNN applications: an industrial case study of evolutionary data augmentation",
    "author": "Yokoyama, Haruki and Onoue, Satoshi and Kikuchi, Shinji",
    "abstract": "Data augmentation techniques that increase the amount of training data by adding realistic transformations are used in machine learning to improve the level of accuracy. Recent studies have demonstrated that data augmentation techniques improve the robustness of image classification models with open datasets; however, it has yet to be investigated whether these techniques are effective for industrial datasets. In this study, we investigate the feasibility of data augmentation techniques for industrial use. We evaluate data augmentation techniques in image classification and object detection tasks using an industrial in-house graphical user interface dataset. As the results indicate, the genetic algorithm-based data augmentation technique outperforms two random-based methods in terms of the robustness of the image classification model. In addition, through this evaluation and interviews with the developers, we learned following two lessons: data augmentation techniques should (1) maintain the training speed to avoid slowing the development and (2) include extensibility for a variety of tasks.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3421841",
    "comment": ""
  },
  {
    "title": "Industry practice of Javascript dynamic analysis on WeChat mini-programs",
    "author": "Liu, Yi and Xie, Jinhui and Yang, Jianbo and Guo, Shiyu and Deng, Yuetang and Li, Shuqing and Wu, Yechang and Liu, Yepang",
    "abstract": "JavaScript is one of the most popular programming languages. WeChat Mini-Program is a large ecosystem of JavaScript applications that runs on the WeChat platform. Millions of Mini-Programs are accessed by WeChat users every week. Consequently, the performance and robustness of Mini-Programs are particularly important. Unfortunately, many Mini-Programs suffer from various defects and performance problems. Dynamic analysis is a useful technique to pinpoint application defects. However, due to the dynamic features of the JavaScript language and the complexity of the runtime environment, dynamic analysis techniques were rarely used to improve the quality of JavaScript applications running on industrial platforms such as WeChat Mini-Program previously. In this work, we report our experience of extending Jalangi, a dynamic analysis framework for JavaScript applications developed by academia, and applying the extended version, named WeJalangi, to diagnose defects in WeChat Mini-Programs. WeJalangi is compatible with existing dynamic analysis tools such as DLint, Smemory, and JITProf. We implemented a null pointer checker on WeJalangi and tested the tool's usability on 152 open-source Mini-Programs. We also conducted a case study in Tencent by applying WeJalangi on six popular commercial Mini-Programs. In the case study, WeJalangi accurately located six null pointer issues and three of them haven't been discovered previously. All of the reported defects have been confirmed by developers and testers.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3421842",
    "comment": ""
  },
  {
    "title": "Lightweight MBT testing for national e-health portal in Norway",
    "author": "Gafurov, Davrondzhon and Grovan, Margrete Sunde and Hurum, Arne Erik",
    "abstract": "We present lightweight model-based testing (MBT) of privacy and authorization concepts of national portal for electronic health services in Norway (which has over a million of visits per month). We have developed test models for creating and updating privacy levels and authorization categories using finite state machine. Our models emphasize not only positive but also negative behavioral aspects of the system. Using edge and edge-pair coverage as an acceptance criteria we identify and systematically derive abstract tests (high level user scenario) from models. Abstract tests are further refined and transformed into concrete tests with detailed steps and data. Although derivation of abstract tests and their transformation into concrete ones are manual, execution of concrete tests and generation of test report are automated. In total, we extracted 85 abstract test cases which resulted in 80 concrete test cases with over 550 iterations. Automated execution of all tests takes about 1 hour, while manual execution of one takes about 5 minutes (over 40 times speedup). MBT contributed to shift the focus of our intellectual work effort into model design rather than test case design, thus making derivation of test scenarios systematic and straight forward. In addition, applying MBT augmented and extended our traditional quality assurance techniques by facilitating better comprehension of new privacy and authorization concepts. Graphical models served as a useful aid in learning these concepts for newcomers.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3421843",
    "comment": ""
  },
  {
    "title": "Code-based vulnerability detection in Node.js applications: how far are we?",
    "author": "Chinthanet, Bodin and Ponta, Serena Elisa and Plate, Henrik and Sabetta, Antonino and Kula, Raula Gaikovina and Ishio, Takashi and Matsumoto, Kenichi",
    "abstract": "With one of the largest available collection of reusable packages, the JavaScript runtime environment Node.js is one of the most popular programming application. With recent work showing evidence that known vulnerabilities are prevalent in both open source and industrial software, we propose and implement a viable code-based vulnerability detection tool for Node.js applications. Our case study lists the challenges encountered while implementing our Node.js vulnerable code detector.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3421838",
    "comment": ""
  },
  {
    "title": "A framework for automated test mocking of mobile apps",
    "author": "Fazzini, Mattia and Gorla, Alessandra and Orso, Alessandro",
    "abstract": "Mobile apps interact with their environment extensively, and these interactions can complicate testing activities because test cases may need a complete environment to be executed. Interactions with the environment can also introduce test flakiness, for instance when the environment behaves in non-deterministic ways. For these reasons, it is common to create test mocks that can eliminate the need for (part of) the environment to be present during testing. Manual mock creation, however, can be extremely time consuming and error-prone. Moreover, the generated mocks can typically only be used in the context of the specific tests for which they were created. To address these issues, we propose MOKA, a general framework for collecting and generating reusable test mocks in an automated way. MOKA leverages the ability to observe a large number of interactions between an application and its environment and uses an iterative approach to generate two possible, alternative types of mocks with different reusability characteristics: advanced mocks generated through program synthesis (ideally) and basic record-replay-based mocks (as a fallback solution). In this paper, we describe the new ideas behind MOKA, its main characteristics, a preliminary empirical study, and a set of possible applications that could benefit from our framework.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418927",
    "comment": ""
  },
  {
    "title": "A hybrid analysis to detect Java serialisation vulnerabilities",
    "author": "Rasheed, Shawn and Dietrich, Jens",
    "abstract": "Serialisation related security vulnerabilities have recently been reported for numerous Java applications. Since serialisation presents both soundness and precision challenges for static analysis, it can be difficult for analyses to precisely pinpoint serialisation vulnerabilities in a Java library. In this paper, we propose a hybrid approach that extends a static analysis with fuzzing to detect serialisation vulnerabilities. The novelty of our approach is in its use of a heap abstraction to direct fuzzing for vulnerabilities in Java libraries. This guides fuzzing to produce results quickly and effectively, and it validates static analysis reports automatically. Our approach shows potential as it can detect known serialisation vulnerabilities in the Apache Commons Collections library.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418931",
    "comment": ""
  },
  {
    "title": "BugPecker: locating faulty methods with deep learning on revision graphs",
    "author": "Cao, Junming and Yang, Shouliang and Jiang, Wenhui and Zeng, Hushuang and Shen, Beijun and Zhong, Hao",
    "abstract": "Given a bug report of a project, the task of locating the faults of the bug report is called fault localization. To help programmers in the fault localization process, many approaches have been proposed, and have achieved promising results to locate faulty files. However, it is still challenging to locate faulty methods, because many methods are short and do not have sufficient details to determine whether they are faulty. In this paper, we present BugPecker, a novel approach to locate faulty methods based on its deep learning on revision graphs. Its key idea includes (1) building revision graphs and capturing the details of past fixes as much as possible, and (2) discovering relations inside our revision graphs to expand the details for methods and calculating various features to assist our ranking. We have implemented BugPecker, and evaluated it on three open source projects. The early results show that BugPecker achieves a mean average precision (MAP) of 0.263 and mean reciprocal rank (MRR) of 0.291, which improve the prior approaches significantly. For example, BugPecker improves the MAP values of all three projects by five times, compared with two recent approaches such as DNNLoc-m and BLIA 1.5.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418934",
    "comment": ""
  },
  {
    "title": "Closer to the edge: testing compilers more thoroughly by being less conservative about undefined behaviour",
    "author": "Even-Mendoza, Karine and Cadar, Cristian and Donaldson, Alastair F.",
    "abstract": "Randomised compiler testing techniques require a means of generating programs that are free from undefined behaviour (UB) in order to reliably reveal miscompilation bugs. Existing program generators such as Csmith heavily restrict the form of generated programs in order to achieve UB-freedom. We hypothesise that the idiomatic nature of such programs limits the test coverage they can offer. Our idea is to generate less restricted programs that are still UB-free---programs that get closer to the edge of UB, but that do not quite cross the edge. We present preliminary support for our idea via a prototype tool, CsmithEdge, which uses simple dynamic analysis to determine where Csmith has been too conservative in its use of safe math wrappers that guarantee UB-freedom for arithmetic operations. By eliminating redundant wrappers, CsmithEdge was able to discover two new miscompilation bugs in GCC that could not be found via intensive testing using regular Csmith, and to achieve substantial differences in code coverage on GCC compared with regular Csmith.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418933",
    "comment": ""
  },
  {
    "title": "Generating highly-structured input data by combining search-based testing and grammar-based fuzzing",
    "author": "Olsthoorn, Mitchell and van Deursen, Arie and Panichella, Annibale",
    "abstract": "Software testing is an important and time-consuming task that is often done manually. In the last decades, researchers have come up with techniques to generate input data (e.g., fuzzing) and automate the process of generating test cases (e.g., search-based testing). However, these techniques are known to have their own limitations: search-based testing does not generate highly-structured data; grammar-based fuzzing does not generate test case structures. To address these limitations, we combine these two techniques. By applying grammar-based mutations to the input data gathered by the search-based testing algorithm, it allows us to co-evolve both aspects of test case generation. We evaluate our approach, called G-EvoSuite, by performing an empirical study on 20 Java classes from the three most popular JSON parsers across multiple search budgets. Our results show that the proposed approach on average improves branch coverage for JSON related classes by 15 \\% (with a maximum increase of 50 \\%) without negatively impacting other classes.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418930",
    "comment": ""
  },
  {
    "title": "Making fair ML software using trustworthy explanation",
    "author": "Chakraborty, Joymallya and Peng, Kewen and Menzies, Tim",
    "abstract": "Machine learning software is being used in many applications (finance, hiring, admissions, criminal justice) having huge social impact. But sometimes the behavior of this software is biased and it shows discrimination based on some sensitive attributes such as sex, race etc. Prior works concentrated on finding and mitigating bias in ML models. A recent trend is using instance-based model-agnostic explanation methods such as LIME[36] to find out bias in the model prediction. Our work concentrates on finding shortcomings of current bias measures and explanation methods. We show how our proposed method based on K nearest neighbors can overcome those shortcomings and find the underlying bias of black box models. Our results are more trustworthy and helpful for the practitioners. Finally, We describe our future framework combining explanation and planning to build fair software.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418932",
    "comment": ""
  },
  {
    "title": "On benign features in malware detection",
    "author": "Cao, Michael and Badihi, Sahar and Ahmed, Khaled and Xiong, Peiyu and Rubin, Julia",
    "abstract": "This paper investigates the problem of classifying Android applications into malicious and benign. We analyze the performance of a popular malware detection tool, Drebin, and show that its correct classification decisions often stem from using benign rather than malicious features for making predictions. That, effectively, turns the classifier into a benign app detector rather than a malware detector. While such behavior allows the classifier to achieve a high detection accuracy, it also makes it vulnerable to attacks, e.g., by a malicious app pretending to be benign by using features similar to those of benign apps. In this paper, we propose an approach for deprioritizing benign features in malware detection, focusing the detection on truly malicious portions of the apps. We show that our proposed approach makes a classifier more resilient to attacks while still allowing it to maintain a high detection accuracy.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418926",
    "comment": ""
  },
  {
    "title": "Proving termination by k-induction",
    "author": "Chen, Jianhui and He, Fei",
    "abstract": "We propose a novel approach to proving the termination of imperative programs by k-induction. By our approach, the termination proving problem can be formalized as a k-inductive invariant synthesis task. On the one hand, k-induction uses weaker invariants than that required by the standard inductive approach. On the other hand, the base case of k-induction, which unrolls the program, can provide stronger pre-condition for invariant synthesis. As a result, the termination arguments of our approach can be synthesized more efficiently than the standard method. We implement a prototype of our k-inductive approach. The experimental results show the significant effectiveness and efficiency of our approach.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418929",
    "comment": ""
  },
  {
    "title": "SRRTA: regression testing acceleration via state reuse",
    "author": "Dong, Jinhao and Lou, Yiling and Hao, Dan",
    "abstract": "Regression testing is widely recognized as an important but time-consuming process. To alleviate this cost issue, test selection, reduction, and prioritization have been widely studied, and they share the commonality that they improve regression testing by optimizing the execution of the whole test suite. In this paper, we attempt to accelerate regression testing from a totally new perspective, i.e., skipping some execution of a new program by reusing program states of an old program. Following this intuition, we propose a state-reuse based acceleration approach SRRTA, consisting of two components: state storage and loading. With the former, SRRTA collects some program states during the execution of an old version through three heuristic-based storage strategies; with the latter, SRRTA loads the stored program states with efficiency optimization strategies. Through the preliminary study on commons-math, SRRTA reduces 82.7\\% of the regression testing time.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418928",
    "comment": ""
  },
  {
    "title": "An automated assessment of Android clipboards",
    "author": "Wang, Wei and Sun, Ruoxi and Xue, Minhui and Ranasinghe, Damith C.",
    "abstract": "Since the new privacy feature in iOS enabling users to acknowledge which app is reading or writing to his or her clipboard through prompting notifications was updated, a plethora of top apps have been reported to frequently access the clipboard without user consent. However, the lack of monitoring and control of Android application's access to the clipboard data leave Android users blind to their potential to leak private information from Android clipboards, raising severe security and privacy concerns. In this preliminary work, we envisage and investigate an approach to (i) dynamically detect clipboard access behaviour, and (ii) determine privacy leaks via static data flow analysis, in which we enhance the results of taint analysis with call graph concatenation to enable leakage source backtracking. Our preliminary results indicate that the proposed method can expose clipboard data leakage as substantiated by our discovery of a popular app, i.e., Sogou Input, directly monitoring and transferring user data in a clipboard to backend servers.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418905",
    "comment": ""
  },
  {
    "title": "Edge4Sys: a device-edge collaborative framework for MEC based smart systems",
    "author": "Gao, Han and Xu, Yi and Liu, Xiao and Xu, Jia and Chen, Tianxiang and Zhou, Bowen and Li, Rui and Li, Xuejun",
    "abstract": "At present, most of the smart systems are based on cloud computing, and massive data generated at the smart end device will need to be transferred to the cloud where AI models are deployed. Therefore, a big challenge for smart system engineers is that cloud based smart systems often face issues such as network congestion and high latency. In recent years, mobile edge computing (MEC) is becoming a promising solution which supports computation-intensive tasks such as deep learning through computation offloading to the servers located at the local network edge. To take full advantage of MEC, an effective collaboration between the end device and the edge server is essential. In this paper, as an initial investigation, we propose Edge4Sys, a Device-Edge Collaborative Framework for MEC based Smart System. Specifically, we employ the deep learning based user identification process in a MEC-based UAV (Unmanned Aerial Vehicle) delivery system as a case study to demonstrate the effectiveness of the proposed framework which can significantly reduce the network traffic and the response time.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418908",
    "comment": ""
  },
  {
    "title": "Efficient multiplex symbolic execution with adaptive search strategy",
    "author": "Zhang, Tianqi and Zhang, Yufeng and Chen, Zhenbang and Shuai, Ziqi and Wang, Ji",
    "abstract": "Symbolic execution is still facing the scalability problem caused by path explosion and constraint solving overhead. The recently proposed MuSE framework supports exploring multiple paths by generating partial solutions in one time of solving. In this work, we improve MuSE from two aspects. Firstly, we use a light-weight check to reduce redundant partial solutions for avoiding the redundant executions having the same results. Secondly, we introduce online learning to devise an adaptive search strategy for the target programs. The preliminary experimental results indicate the promising of the proposed methods.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418902",
    "comment": ""
  },
  {
    "title": "Managing app testing device clouds: issues and opportunities",
    "author": "Fazzini, Mattia and Orso, Alessandro",
    "abstract": "Because creating and maintaining an in-house test lab is expensive and time-consuming, companies and app developers often use device clouds to test their apps. Because quality-assurance activities depend on such device clouds, it is important to understand possible issues related to their use. To this end, in this paper we present a preliminary study that investigates issues and highlights research opportunities in the context of managing and maintaining device clouds. In the study, we analyzed over 12 million test executions on 110 devices. We found that the management software of the cloud infrastructure we considered affected some test executions, and almost all the cloud devices had at least one security-related issue.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418909",
    "comment": ""
  },
  {
    "title": "Styx: a data-oriented mutation framework to improve the robustness of DNN",
    "author": "Liu, Meixi and Hong, Weijiang and Pan, Weiyu and Feng, Chendong and Chen, Zhenbang and Wang, Ji",
    "abstract": "The robustness of deep neural network (DNN) is critical and challenging to ensure. In this paper, we propose a general data-oriented mutation framework, called Styx, to improve the robustness of DNN. Styx generates new training data by slightly mutating the training data. In this way, Styx ensures the DNN's accuracy on the test dataset while improving the adaptability to small perturbations, i.e., improving the robustness. We have instantiated Styx for image classification and proposed pixel-level mutation rules that are applicable to any image classification DNNs. We have applied Styx on several commonly used benchmarks and compared Styx with the representative adversarial training methods. The preliminary experimental results indicate the effectiveness of Styx.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418903",
    "comment": ""
  },
  {
    "title": "Synthesizing smart solving strategy for symbolic execution",
    "author": "Chen, Zehua and Chen, Zhenbang and Shuai, Ziqi and Zhang, Yufeng and Pan, Weiyu",
    "abstract": "Constraint solving is one of the challenges for symbolic execution. Modern SMT solvers allow users to customize the internal solving procedure by solving strategies. In this extended abstract, we report our recent progress in synthesizing a program-specific solving strategy for the symbolic execution of a program. We propose a two-stage procedure for symbolic execution. At the first stage, we synthesize a solving strategy by utilizing deep learning techniques. Then, the strategy will be used in the second stage to improve the performance of constraint solving. The preliminary experimental results indicate the promising of our method.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418904",
    "comment": ""
  },
  {
    "title": "The symptom, cause and repair of workaround",
    "author": "Song, Daohan and Zhong, Hao and Jia, Li",
    "abstract": "In software development, issue tracker systems are widely used to manage bug reports. In such a system, a bug report can be filed, diagnosed, assigned, and fixed. In the standard process, a bug can be resolved as fixed, invalid, duplicated or won't fix. Although the above resolutions are well-defined and easy to understand, a bug report can end with a less known resolution, i.e., workaround. Compared with other resolutions, the definition of workarounds is more ambiguous. Besides the problem that is reported in a bug report, the resolution of a workaround raises more questions. Some questions are important for users, especially those programmers who build their projects upon others (e.g., libraries). Although some early studies have been conducted to analyze API workarounds, many research questions on workarounds are still open. For example, which bugs are resolved as workarounds? Why is a bug report resolved as workarounds? What are the repairs of workarounds? In this experience paper, we conduct the first empirical study to explore the above research questions. In particular, we analyzed 221 real workarounds that were collected from Apache projects. Our results lead to some interesting and useful answers to all the above questions. For example, we find that most bug reports are resolved as workarounds, because their problems reside in libraries (24.43\\%), settings (18.55\\%), and clients (10.41\\%). Among them, many bugs are difficult to be fixed fully and perfectly. As a late breaking result, we can only briefly introduce our study, but we present a detailed plan to extend it to a full paper.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418910",
    "comment": ""
  },
  {
    "title": "Towards immersive comprehension of software systems using augmented reality: an empirical evaluation",
    "author": "Mehra, Rohit and Sharma, Vibhu Saujanya and Kaulgud, Vikrant and Podder, Sanjay and Burden, Adam P.",
    "abstract": "While traditionally, software comprehension relies on approaches like reading through the code or looking at charts on screens, which are 2D mediums, there have been some recent approaches that advocate exploring 3D approaches like Augmented or Virtual Reality (AR/VR) to have a richer experience towards understanding software and its internal relationships. However, there is a dearth of objective studies that compare such 3D representations with their traditional 2D counterparts in the context of software comprehension. In this paper, we present an evaluation study to quantitatively and qualitatively compare 2D and 3D software representations with respect to typical comprehension tasks. For the 3D medium, we utilize an AR-based approach for 3D visualizations of a software system (XRaSE), while the 2D medium comprises of textual IDEs and 2D graph representations. The study, which has been conducted using 20 professional developers, shows that for most comprehension tasks, the developers perform much better using the 3D representation, especially in terms of velocity and recollection, while also displaying reduced cognitive load and better engagement.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418907",
    "comment": ""
  },
  {
    "title": "Towards programming and verification for activity-oriented smart home systems",
    "author": "Li, Xuansong and Song, Wei and Zhang, Xiangyu",
    "abstract": "Smart home systems are becoming increasingly popular. Software engineering of such systems hence becomes a prominent challenge. In this engineering paradigm, users are often interested in considering sensor states while they are performing various activities. Existing works have proposed initial efforts on incremental development method for activity-oriented requirements. However, there is no systematic way of ensuring reliability and security of such systems which may be developed by various developers and may execute in a complex environment. Some properties, especially those including timing constraints, need to be satisfied. In this paper, we introduce Actom, a framework for identification of activity-oriented requirements and runtime verification. Actom supports the development of the mapping between activities and required sensor readings (activity-sensor mapping). At runtime, Actom receives results of activity recognition and is able to trigger actuators to provide the required physical conditions for the activities, as determined by the activity-sensor mapping. Moreover, Actom continuously monitors whether activity-sensor mapping holds over a time period during the activity. We also discuss the evaluation plan to demonstrate the effectiveness and efficiency of Actom. The end product will be a systematic framework to facilitate the development of activity-oriented requirements and monitor properties with timing constraints to improve reliability and security.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418906",
    "comment": ""
  },
  {
    "title": "AirMochi: a tool for remotely controlling iOS devices",
    "author": "Luki\\'{c}, Nikola and Talebipour, Saghar and Medvidovi\\'{c}, Nenad",
    "abstract": "This paper presents AirMochi, a tool that provides remote access and control of apps by leveraging a mobile platform's publicly exported accessibility features. While AirMochi is designed to be platform-independent, we discuss its iOS implementation. We show that AirMochi places no restrictions on apps, is able to handle a variety of scenarios, and imposes a negligible performance overhead. https://youtu.be/rhPz2Hs4Ius https://github.com/nkllkc/air_mochi",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415304",
    "comment": ""
  },
  {
    "title": "Botsing, a search-based crash reproduction framework for Java",
    "author": "Derakhshanfar, Pouria and Devroey, Xavier and Panichella, Annibale and Zaidman, Andy and van Deursen, Arie",
    "abstract": "Approaches for automatic crash reproduction aim to generate test cases that reproduce crashes starting from the crash stack traces. These tests help developers during their debugging practices. One of the most promising techniques in this research field leverages search-based software testing techniques for generating crash reproducing test cases. In this paper, we introduce Botsing, an open-source search-based crash reproduction framework for Java. Botsing implements state-of-the-art and novel approaches for crash reproduction. The well-documented architecture of Botsing makes it an easy-to-extend framework, and can hence be used for implementing new approaches to improve crash reproduction. We have applied Botsing to a wide range of crashes collected from open source systems. Furthermore, we conducted a qualitative assessment of the crash-reproducing test cases with our industrial partners. In both cases, Botsing could reproduce a notable amount of the given stack traces.Demo. video: https://www.youtube.com/watch?v=k6XaQjHqe48Botsing website: https://stamp-project.github.io/botsing/",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415299",
    "comment": ""
  },
  {
    "title": "EXPRESS: an energy-efficient and secure framework for mobile edge computing and blockchain based smart systems",
    "author": "Xu, Jia and Liu, Xiao and Li, Xuejun and Zhang, Lei and Yang, Yun",
    "abstract": "As most smart systems such as smart logistic and smart manufacturing are delay sensitive, the current mainstream cloud computing based system architecture is facing the critical issue of high latency over the Internet. Meanwhile, as huge amount of data is generated by smart devices with limited battery and computing power, the increasing demand for energy-efficient machine learning and secure data communication at the network edge has become a hurdle to the success of smart systems. To address these challenges with using smart UAV (Unmanned Aerial Vehicle) delivery system as an example, we propose EXPRESS, a novel energy-efficient and secure framework based on mobile edge computing and blockchain technologies. We focus on computation and data (resource) management which are two of the most prominent components in this framework. The effectiveness of the EXPRESS framework is demonstrated through the implementation of a real-world UAV delivery system. As an open-source framework, EXPRESS can help researchers implement their own prototypes and test their computation and data management strategies in different smart systems. The demo video can be found at https://youtu.be/r3U1iU8tSmk.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415294",
    "comment": ""
  },
  {
    "title": "Edge4Real: a cost-effective edge computing based human behaviour recognition system for human-centric software engineering",
    "author": "Shao, Di and Liu, Xiao and Cheng, Ben and Wang, Owen and Hoang, Thuong",
    "abstract": "Recognition of human behaviours including body motions and facial expressions plays a significant role in human-centric software engineering. However, due to the data and computation intensive nature of human behaviour recognition through video analytics, expensive powerful machines are often required, which could hinder the research and application in human-centric software engineering. To address such an issue, this paper proposes a cost-effective human behaviour recognition system named Edge4Real which can be easily deployed in an edge computing environment with commodity machines. Compared with existing centralised solutions, Edge4Real has three major advantages including cost-effectiveness, easy-to-use, and real-time. Specifically, Edge4Real adopts a distributed architecture where components such as motion capturing, human behaviour recognition, data decoding and extraction, and the application of the recognition result, can be deployed on separated end devices and edge nodes in an edge computing environment. Using a virtual reality application which can capture a user's motion and translate into the motion of a 3D avatar in real time, we successfully validate the effectiveness of the system and demonstrate its promising value to the research and application of human-centric software engineering. The demo video can be found at https://youtu.be/tnEshD8j-kA.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415297",
    "comment": ""
  },
  {
    "title": "FILO: FIx-LOcus localization for backward incompatibilities caused by Android framework upgrades",
    "author": "Mobilio, Marco and Riganelli, Oliviero and Micucci, Daniela and Mariani, Leonardo",
    "abstract": "Mobile operating systems evolve quickly, frequently updating the APIs that app developers use to build their apps. Unfortunately, API updates do not always guarantee backward compatibility, causing apps to not longer work properly or even crash when running with an updated system. This paper presents FILO, a tool that assists Android developers in resolving backward compatibility issues introduced by API upgrades. FILO both suggests the method that needs to be modified in the app in order to adapt the app to an upgraded API, and reports key symptoms observed in the failed execution to facilitate the fixing activity. Results obtained with the analysis of 12 actual upgrade problems and the feedback produced by early tool adopters show that FILO can practically support Android developers. FILO can be downloaded from https://gitlab.com/learnERC/filo, and its video demonstration is available at https://youtu.be/WDvkKj-wnlQ.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415290",
    "comment": ""
  },
  {
    "title": "GUI2WiRe: rapid wireframing with a mined and large-scale GUI repository using natural language requirements",
    "author": "Kolthoff, Kristian and Bartelt, Christian and Ponzetto, Simone Paolo",
    "abstract": "High-fidelity Graphical User Interface (GUI) prototyping is a well-established and suitable method for enabling fruitful discussions, clarification and refinement of requirements formulated by customers. GUI prototypes can help to reduce misunderstandings between customers and developers, which may occur due to the ambiguity comprised in informal Natural Language (NL). However, a disadvantage of employing high-fidelity GUI prototypes is their time-consuming and expensive development. Common GUI prototyping tools are based on combining individual GUI components or manually crafted templates. In this work, we present GUI2WiRe, a tool that enables users to retrieve GUI prototypes from a semiautomatically created large-scale GUI repository for mobile applications matching user requirements specified in Natural Language (NLR). We extract multiple text segments from the GUI hierarchy data and employ various Information Retrieval (IR) models and Automatic Query Expansion (AQE) techniques to achieve ad-hoc GUI retrieval from NLR. Retrieved GUI prototypes mined from applications can be inserted in the graphical editor of GUI2WiRe to rapidly create wireframes. GUI components are extracted automatically from the GUI screenshots and basic editing functionality is provided to the user. Finally, a preview of the application is created from the wireframe to allow interactive exploration of the current design. We evaluated the applied IR and AQE approaches for their effectiveness in terms of GUI retrieval relevance on a manually annotated collection of NLR and discuss our planned user studies. Video presentation of GUI2WiRe: https://youtu.be/2nN-Xr2Hk7I",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415289",
    "comment": ""
  },
  {
    "title": "HomoTR: online test recommendation system based on homologous code matching",
    "author": "Zhu, Chenqian and Sun, Weisong and Liu, Qin and Yuan, Yangyang and Fang, Chunrong and Huang, Yong",
    "abstract": "A growing number of new technologies are used in test development. Among them, automatic test generation, a promising technology to improve the efficiency of unit testing, currently performs not satisfactory in practice. Test recommendation, like code recommendation, is another feasible technology for supporting efficient unit testing and gets increasing attention. In this paper, we develop a novel system, namely HomoTR, which implements online test recommendations by measuring the homology of two methods. If the new method under test shares homology with an existing method that has test cases, HomoTR will recommend the test cases to the new method. The preliminary experiments show that HomoTR can quickly and effectively recommend test cases to help the developers improve the testing efficiency. Besides, HomoTR has been integrated into the MoocTest platform successfully, so it can also execute the recommended test cases automatically and visualize the testing results (e.g., Branch Coverage) friendly to help developers understand the process of testing. The demo video of HomoTR can be found at https://youtu.be/_227EfcUbus.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415296",
    "comment": ""
  },
  {
    "title": "ImpAPTr: a tool for identifying the clues to online service anomalies",
    "author": "Wang, Hao and Rong, Guoping and Xu, Yangchen and You, Yong",
    "abstract": "As a common IT infrastructure, APM (Application Performance Management) systems have been widely adopted to monitor call requests to an on-line service. Usually, each request may contain multi-dimensional attributes (e.g., City, ISP, Platform, etc.), which may become the reason for a certain anomaly regarding DSR (Declining Success Rate) of service calls either solely or as a combination. Moreover, each attribute may also have multiple values (e.g., ISP could be T-Mobile, Vodafone, CMCC, etc.), rendering intricate root causes and huge challenges to identify the root causes. In this paper, we propose a prototype tool, ImpAPTr (Impact Analysis based on Pruning Tree), to identify the combination of dimensional attributes as the clues to dig out the root causes of anomalies regarding DSR of a service call in a timely manner. ImpAPTr has been evaluated in MeiTuan, one of the biggest on-line service providers. Performance regarding the accuracy outperforms several previous tools in the same field.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415301",
    "comment": ""
  },
  {
    "title": "OSLDetector: identifying open-source libraries through binary analysis",
    "author": "Zhang, Dan and Luo, Ping and Tang, Wei and Zhou, Min",
    "abstract": "Using open-source libraries can provide rich functions and reduce development cost. However, some critical issues have also been caused such as license conflicts and vulnerability risks. In this paper, we design and implement an open-source libraries detection tool OSLDetector which uses methods of matching features to detect third-party libraries for multi-platform software in binaries. We took a series of methods such as filtering features and novelty building an internal clone forest to cope with the challenge of feature duplication. The tool can also provide the conflict of licenses and identify possible corresponding vulnerabilities, so these potential risks can be resolved and avoided. To evaluate the efficiency of OSLDetector, we collect 5K libraries containing 9K versions and manage their respective license type and existing vulnerabilities. The experimental results with a precision of 96\\% and recall of 92.3\\% show that OSLDetector is effective and outperforms similar tools.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415303",
    "comment": ""
  },
  {
    "title": "Sosed: a tool for finding similar software projects",
    "author": "Bogomolov, Egor and Golubev, Yaroslav and Lobanov, Artyom and Kovalenko, Vladimir and Bryksin, Timofey",
    "abstract": "In this paper, we present Sosed, a tool for discovering similar software projects. We use fastText to compute the embeddings of subtokens into a dense space for 120,000 GitHub projects in 200 languages. Then, we cluster embeddings to identify groups of semantically similar subtokens that reflect topics in source code. We use a dataset of 9 million GitHub projects as a reference search base. To identify similar projects, we compare the distributions of clusters among their subtokens. The tool receives an arbitrary project as input, extracts subtokens in 16 most popular programming languages, computes cluster distribution, and finds projects with the closest distribution in the search base. We labeled subtoken clusters with short descriptions to enable Sosed to produce interpretable output.Sosed is available at https://github.com/JetBrains-Research/sosed/. The tool demo is available at https://www.youtube.com/watch?v=LYLkztCGRt8. The multi-language extractor of subtokens is available separately at https://github.com/JetBrains-Research/buckwheat/.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415291",
    "comment": ""
  },
  {
    "title": "WASim: understanding WebAssembly applications through classification",
    "author": "Romano, Alan and Wang, Weihang",
    "abstract": "WebAssembly is a new programming language built for better performance in web applications. It defines a binary code format and a text representation for the code. At first glance, WebAssembly files are not easily understandable to human readers, regardless of the experience level. As a result, distributed third-party WebAssembly modules need to be implicitly trusted by developers as verifying the functionality requires significant effort. To this end, we develop an automated classification tool WASim for identifying the purpose of WebAssembly programs by analyzing features at the module-level. It assigns purpose labels to a module in order to assist developers in understanding the binary module. The code for WASim is available at https://github.com/WASimilarity/WASim and a video demo is available at https://youtu.be/usfYFIeTy0U.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415293",
    "comment": ""
  },
  {
    "title": "MetPurity: a learning-based tool of pure method identification for automatic test generation",
    "author": "Yu, Runze and Zhang, Youzhe and Xuan, Jifeng",
    "abstract": "In object-oriented programming, a method is pure if calling the method does not change object states that exist in the pre-states of the method call. Pure methods are widely-used in automatic techniques, including test generation, compiler optimization, and program repair. Due to the source code dependency, it is infeasible to completely and accurately identify all pure methods. Instead, existing techniques such as ReImInfer are designed to identify a subset of accurate results of pure method and mark the other methods as unknown ones. In this paper, we designed and implemented MetPurity, a learning-based tool of pure method identification. Given all methods in a project, MetPurity labels a training set via automatic program analysis and builds a binary classifier (implemented with the random forest classifier) based on the training set. This classifier is used to predict the purity of all the other methods (i.e., unknown ones) in the same project. Preliminary evaluation on four open-source Java projects shows that MetPurity can provide a list of identified pure methods with a low error rate. Applying MetPurity to EvoSuite can increase the number of generated assertions for regression testing in test generation by EvoSuite.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415292",
    "comment": ""
  },
  {
    "title": "STIFA: crowdsourced mobile testing report selection based on text and image fusion analysis",
    "author": "Cao, Zhenfei and Wang, Xu and Yu, Shengcheng and Yun, Yexiao and Fang, Chunrong",
    "abstract": "Crowdsourced mobile testing has been widely used due to its convenience and high efficiency [10]. Crowdsourced workers complete testing tasks and record results in test reports. However, the problem of duplicate reports has prevented the efficiency of crowdsourced mobile testing from further improving. Existing crowdsourced testing report analysis techniques usually leverage screenshots and text descriptions independently, but fail to recognize the link between these two types of information. In this paper, we present a crowdsourced mobile testing report selection tool, namely STIFA, to extract image and text feature information in reports and establish an image-text-fusion bug context. Based on text and image fusion analysis results, STIFA performs cluster analysis and report selection. To evaluate, we employed STIFA to analyze 150 reports from 2 apps. The results show that STIFA can extract, on average, 95.23\\% text feature information and 84.15\\% image feature information. Besides, STIFA reaches an accuracy of 87.64\\% in detecting duplicate reports. The demo can be found at https://youtu.be/Gw6ptqyQbQY.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415300",
    "comment": ""
  },
  {
    "title": "JITBot: an explainable just-in-time defect prediction bot",
    "author": "Khanan, Chaiyakarn and Luewichana, Worawit and Pruktharathikoon, Krissakorn and Jiarpakdee, Jirayus and Tantithamthavorn, Chakkrit and Choetkiertikul, Morakot and Ragkhitwetsagul, Chaiyong and Sunetnanta, Thanwadee",
    "abstract": "Just-In-Time (JIT) defect prediction is a classification model that is trained using historical data to predict bug-introducing changes. However, recent studies raised concerns related to the explainability of the predictions of many software analytics applications (i.e., practitioners do not understand why commits are risky and how to improve them). In addition, the adoption of Just-In-Time defect prediction is still limited due to a lack of integration into CI/CD pipelines and modern software development platforms (e.g., GitHub). In this paper, we present an explainable Just-In-Time defect prediction framework to automatically generate feedback to developers by providing the riskiness of each commit, explaining why such commit is risky, and suggesting risk mitigation plans. The proposed framework is integrated into the GitHub CI/CD pipeline as a GitHub application to continuously monitor and analyse a stream of commits in many GitHub repositories. Finally, we discuss the usage scenarios and their implications to practitioners. The VDO demonstration is available at https://jitbot-tool.github.io/",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415295",
    "comment": ""
  },
  {
    "title": "Speeding up GUI testing by on-device test generation",
    "author": "Borges, Nataniel P. and Rau, Jenny and Zeller, Andreas",
    "abstract": "When generating GUI tests for Android apps, it typically is a separate test computer that generates interactions, which are then executed on an actual Android device. While this approach is efficient in the sense that apps and interactions execute quickly, the communication overhead between test computer and device slows down testing considerably. In this work, we present DD-2, a test generator for Android that tests other apps on the device using Android accessibility services. In our experiments, DD-2 has shown to be 3.2 times faster than its computer-device counterpart, while sharing the same source code.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415302",
    "comment": ""
  },
  {
    "title": "PerfCI: a toolchain for automated performance testing during continuous integration of Python projects",
    "author": "Javed, Omar and Dawes, Joshua Heneage and Han, Marta and Franzoni, Giovanni and Pfeiffer, Andreas and Reger, Giles and Binder, Walter",
    "abstract": "Software performance testing is an essential quality assurance mechanism that can identify optimization opportunities. Automating this process requires strong tool support, especially in the case of Continuous Integration (CI) where tests need to run completely automatically and it is desirable to provide developers with actionable feedback. A lack of existing tools means that performance testing is normally left out of the scope of CI. In this paper, we propose a toolchain - PerfCI - to pave the way for developers to easily set up and carry out automated performance testing under CI. Our toolchain is based on allowing users to (1) specify performance testing tasks, (2) analyze unit tests on a variety of python projects ranging from scripts to full-blown flask-based web services, by extending a performance analysis framework (VyPR) and (3) evaluate performance data to get feedback on the code. We demonstrate the feasibility of our toolchain by using it on a web service running at the Compact Muon Solenoid (CMS) experiment at the world's largest particle physics laboratory --- CERN.Package. Source code, example and documentation of PerfCI are available: https://gitlab.cern.ch/omjaved/perfci. Tool demonstration can be viewed on YouTube: https://youtu.be/RDmXMKA1v7g. We also provide the data set used in the analysis: https://gitlab.cern.ch/omjaved/perfci-dataset.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415288",
    "comment": ""
  },
  {
    "title": "SmartBugs: a framework to analyze solidity smart contracts",
    "author": "Ferreira, Jo\\~{a}o F. and Cruz, Pedro and Durieux, Thomas and Abreu, Rui",
    "abstract": "Over the last few years, there has been substantial research on automated analysis, testing, and debugging of Ethereum smart contracts. However, it is not trivial to compare and reproduce that research. To address this, we present SmartBugs, an extensible and easy-to-use execution framework that simplifies the execution of analysis tools on smart contracts written in Solidity, the primary language used in Ethereum. SmartBugs is currently distributed with support for 10 tools and two datasets of Solidity contracts. The first dataset can be used to evaluate the precision of analysis tools, as it contains 143 annotated vulnerable contracts with 208 tagged vulnerabilities. The second dataset contains 47,518 unique contracts collected through Etherscan. We discuss how SmartBugs supported the largest experimental setup to date both in the number of tools and in execution time. Moreover, we show how it enables easy integration and comparison of analysis tools by presenting a new extension to the tool SmartCheck that improves substantially the detection of vulnerabilities related to the DASP10 categories Bad Randomness, Time Manipulation, and Access Control (identified vulnerabilities increased from 11\\% to 24\\%).",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415298",
    "comment": ""
  },
  {
    "title": "RepoSkillMiner: identifying software expertise from GitHub repositories using natural language processing",
    "author": "Kourtzanidis, Stratos and Chatzigeorgiou, Alexander and Ampatzoglou, Apostolos",
    "abstract": "A GitHub profile is becoming an essential part of a developer's resume enabling HR departments to extract someone's expertise, through automated analysis of his/her contribution to open-source projects. At the same time, having clear insights on the technologies used in a project can be very beneficial for resource allocation and project maintainability planning. In the literature, one can identify various approaches for identifying expertise on programming languages, based on the projects that developer contributed to. In this paper, we move one step further and introduce an approach (accompanied by a tool) to identify low-level expertise on particular software frameworks and technologies apart, relying solely on GitHub data, using the GitHub API and Natural Language Processing (NLP)---using the Microsoft Language Understanding Intelligent Service (LUIS). In particular, we developed an NLP model in LUIS for named-entity recognition for three (3) .NET technologies and two (2) front-end frameworks. Our analysis is based upon specific commit contents, in terms of the exact code chunks, which the committer added or changed. We evaluate the precision, recall and f-measure for the derived technologies/frameworks, by conducting a batch test in LUIS and report the results. The proposed approach is demonstrated through a fully functional web application named RepoSkillMiner.Tool Links:Video, Code Repo, Application, Validation Dataset",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3415305",
    "comment": ""
  },
  {
    "title": "A machine learning based approach to autogenerate diagnostic models for CNC machines",
    "author": "Masalimov, Kamil Adipovich",
    "abstract": "This article presents a description of a system for the automatic generation of predictive diagnostic models of CNC machine tools. This system allows machine tool maintenance specialists to select and operate models based on LSTM neural networks to determine the state of elements of CNC machines. Examples of changes in the accuracy of the models used during operation are given to determine the state of the cutting tool (more than 95\\%) and the bearings of electric motors (more than 91\\%).",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418915",
    "comment": ""
  },
  {
    "title": "A program verification based approach to find data race vulnerabilities in interrupt-driven program",
    "author": "Feng, Haining",
    "abstract": "The data race problem is common in the interrupt-driven program, and it is difficult to find as a result of complicated interrupt interleaving. Static analysis is a mainstream technology to detect those problems, however, the synchronization mechanism of interrupt is hard to be processed by the existing method, which brings many false alarms. Eliminating false alarms in static analysis is the main challenge for precisely data race detection.In this paper, we present a framework of static analysis combined with program verification, which performs static analysis to find all potential races, and then verifies every race to eliminate false alarms. The experiment results on related race benchmarks show that our implementation finds all race bugs in the phase of static analysis, and eliminates all false alarms through program verification.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418925",
    "comment": ""
  },
  {
    "title": "A unified framework to learn program semantics with graph neural networks",
    "author": "Liu, Shangqing",
    "abstract": "Program semantics learning is a vital problem in various AI for SE applications e.g., clone detection, code summarization. Learning to represent programs with Graph Neural Networks (GNNs) has achieved state-of-the-art performance in many applications e.g., vulnerability identification, type inference. However, currently, there is a lack of a unified framework with GNNs for distinct applications. Furthermore, most existing GNN-based approaches ignore global relations with nodes, limiting the model to learn rich semantics. In this paper, we propose a unified framework to construct two types of graphs to capture rich code semantics for various SE applications.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418924",
    "comment": ""
  },
  {
    "title": "Anti-patterns for Java automated program repair tools",
    "author": "Wu, Yi",
    "abstract": "Prior study has identified common anti-patterns in automated repair for C programs. In this work, we study if the same problems exist in Java programs. We performed a manual inspection on the plausible patches generated by Java automated repair tools. We integrated anti-patterns in jGenProg2 and evaluated on Defects4J benchmark. The result shows that the average repair time is reduced by 22.6 \\% and the number of generated plausible patches is reduced from 67 to 29 for 14 bugs in total. Our study provided evidence about the effectiveness of applying anti-patterns in future Java automated repair tools.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418919",
    "comment": ""
  },
  {
    "title": "CrossPriv: user privacy preservation model for cross-silo federated software",
    "author": "Diddee, Harshita and Kansra, Bhrigu",
    "abstract": "The design and implementation of artificial intelligence driven software that keeps user data private is a complex yet necessary requirement in the current times. Developers must consider several ethical and legal challenges while developing services which relay massive amount of private information over a network grid which is susceptible to attack from malicious agents. In most cases, organizations adopt a traditional model training approach where publicly available data, or data specifically collated for the task is used to train the model. Specifically in the healthcare section, the operation of deep learning algorithms on limited local data may introduce a significant bias to the system and the accuracy of the model may not be representative due to lack of richly covariate training data. In this paper, we propose CrossPriv, a user privacy preservation model for cross-silo Federated Learning systems to dictate some preliminary norms of SaaS based collaborative software. We discuss the client and server side characteristics of the software deployed on each side. Further, We demonstrate the efficacy of the proposed model by training a convolution neural network on distributed data of two different silos to detect pneumonia using X-Rays whilst not sharing any raw data between the silos.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418911",
    "comment": ""
  },
  {
    "title": "Discovering UI display issues with visual understanding",
    "author": "Liu, Zhe",
    "abstract": "GUI complexity posts a great challenge to the GUI implementation. According to our pilot study of crowdtesting bug reports, display issues such as text overlap, blurred screen, missing image always occur during GUI rendering on difference devices due to the software or hardware compatibility. They negatively influence the app usability, resulting in poor user experience. To detect these issues, we propose a novel approach, OwlEye, based on deep learning for modelling visual information of the GUI screenshot. Therefore, OwlEye can detect GUIs with display issues and also locate the detailed region of the issue in the given GUI for guiding developers to fix the bug. We manually construct a large-scale labelled dataset with 4,470 GUI screenshots with UI display issues. We develop a heuristics-based data augmentation method and a GAN-based data augmentation method for boosting the performance of our OwlEye. At present, the evaluation demonstrates that our OwlEye can achieve 85\\% precision and 84\\% recall in detecting UI display issues, and 90\\% accuracy in localizing these issues.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418917",
    "comment": ""
  },
  {
    "title": "Dynamic algorithm selection for SMT",
    "author": "Pimpalkhare, Nikhil",
    "abstract": "We describe an online approach to SMT solver selection using nearest neighbor classification and runtime estimation. We implement and evaluate our approach with MedleySolver, finding that it makes nearly optimal selections and evaluates a dataset of queries three times faster than any indivdual solver.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418922",
    "comment": ""
  },
  {
    "title": "FLUX: from SQL to GQL query translation tool",
    "author": "Sharma, Chandan",
    "abstract": "With the influx of Web 3.0 the focus in Big Data Analytics has shifted towards modelling highly interconnected data and analysing relationships between them. Graph databases befit the requirements of Big Data Analytics yet organizations still depend on relational databases. A major roadblock in the industry wide adoption of graph databases is that a standard query language is still in its inception stage hence withholding interoperability between the two technologies. In this research we propose a tool FLUX for translating relational database queries to graph database queries.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418916",
    "comment": ""
  },
  {
    "title": "Finding ethereum smart contracts security issues by comparing history versions",
    "author": "Chen, Jiachi",
    "abstract": "Smart contracts are Turing-complete programs running on the blockchain. They cannot be modified, even when bugs are detected. The Selfdestruct function is the only way to destroy a contract on the blockchain system and transfer all the Ethers on the contract balance. Thus, many developers use this function to destroy a contract and redeploy a new one when bugs are detected. In this paper, we propose a deep learning-based method to find security issues of Ethereum smart contracts by finding the updated version of a destructed contract. After finding the updated versions, we use open card sorting to find security issues.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418923",
    "comment": ""
  },
  {
    "title": "Formal verification of masking countermeasures for arithmetic programs",
    "author": "Gao, Pengfei",
    "abstract": "Cryptographic algorithms are widely used to protect data privacy in many aspects of daily lives from smart card to cyber-physical systems. Unfortunately, programs implementing cryptographic algorithms may be vulnerable to practical power side-channel attacks, which may infer private data via statistical analysis of the correlation between power consumptions of an electronic device and private data. To thwart these attacks, several masking schemes have been proposed. However, programs that rely on secure masking schemes are not secure a priori. Although some techniques have been proposed for formally verifying masking countermeasures and for quantifying masking strength, they are currently limited to Boolean programs and suffer from low accuracy. In this work, we propose an approach for formally verifying masking countermeasures of arithmetic programs. Our approach is more accurate for arithmetic programs and more scalable for Boolean programs comparing to the existing approaches. We have implemented our methods in a verification tool QMVerif which has been extensively evaluated on cryptographic benchmarks including full AES, DES and MAC-Keccak. The experimental results demonstrate the effectiveness and efficiency of our approach, especially for compositional reasoning.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418920",
    "comment": ""
  },
  {
    "title": "Identifying mutation subsumption relations",
    "author": "Souza, Beatriz",
    "abstract": "One recent promising direction in reducing costs of mutation analysis is to identify redundant mutations. We propose a technique to discover redundant mutations by proving subsumption relations among method-level mutation operators using weak mutation testing. We conceive and encode a theory of subsumption relations in Z3 for 40 mutation targets (mutations of an expression or statement). Then we prove a number of subsumption relations using the Z3 theorem prover, and reduce the number of mutations in a number of mutation targets. MuJava-M includes some subsumption relations in MuJava. We apply MuJava and MuJava-M to 187 classes of 17 projects. Our approach correctly discards mutations in 74.97\\% of the cases, and reduces the number of mutations by 72.52\\%.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418921",
    "comment": ""
  },
  {
    "title": "Scalability and precision improvement of neural program synthesis",
    "author": "Zhang, Yating",
    "abstract": "Mosts of the neural synthesis construct encoder-decoder models to learn a probability distribution over the space of programs. Two drawbacks in such neural program synthesis are that the synthesis scale is relatively small and the correctness of the synthesis result cannot be guaranteed. We address these problems by constructing a framework, which analyzes and solves problems from three dimensions: program space description, model architecture, and result processing. Experiments show that the scalability and precision of synthesis are improved in every dimension.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418912",
    "comment": ""
  },
  {
    "title": "The classification and propagation of program comments",
    "author": "Xu, Xiangzhe",
    "abstract": "Natural language comments are like bridges between human logic and software semantics. Developers use comments to describe the function, implementation, and property of code snippets. This kind of connections contains rich information, like the potential types of a variable and the pre-condition of a method, among other things. In this paper, we categorize comments and use natural language processing techniques to extract information from them. Based on the semantics of programming languages, different rules are built for each comment category to systematically propagate comments among code entities. Then we use the propagated comments to check the code usage and comments consistency. Our demo system finds 37 bugs in real-world projects, 30 of which have been confirmed by the developers. Except for bugs in the code, we also find 304 pieces of defected comments. The 12 of them are misleading and 292 of them are not correct. Moreover, among the 41573 pieces of comments we propagate, 87 comments are for private native methods which had neither code nor comments. We also conduct a user study where we find that propagated comments are as good as human-written comments in three dimensions of consistency, naturalness, and meaningfulness.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418913",
    "comment": ""
  },
  {
    "title": "Source code and binary level vulnerability detection and hot patching",
    "author": "Xu, Zhengzi",
    "abstract": "This paper presents a static vulnerability detection and patching framework at both source code and binary level. It automatically identifies and collects known vulnerability information to build the signature. It matches vulnerable functions with similar signatures and filters out the ones that have been patched in the target program. For the vulnerable functions, the framework tries to generate hot patches by learning from the source code.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418914",
    "comment": ""
  },
  {
    "title": "When deep learning meets smart contracts",
    "author": "Gao, Zhipeng",
    "abstract": "Ethereum has become a widely used platform to enable secure, Blockchain-based financial and business transactions. However, many identified bugs and vulnerabilities in smart contracts have led to serious financial losses, which raises serious concerns about smart contract security. Thus, there is a significant need to better maintain smart contract code and ensure its high reliability.In this research: (1) Firstly, we propose an automated deep learning based approach to learn structural code embeddings of smart contracts in Solidity, which is useful for clone detection, bug detection and contract validation on smart contracts. We apply our approach to more than 22K solidity contracts collected from the Ethereum blockchain, results show that the clone ratio of solidity code is at around 90\\%, much higher than traditional software. We collect a list of 52 known buggy smart contracts belonging to 10 kinds of common vulnerabilities as our bug database. Our approach can identify more than 1000 clone related bugs based on our bug databases efficiently and accurately. (2) Secondly, according to developers' feedback, we have implemented the approach in a web-based tool, named SmartEmbed, to facilitate Solidity developers for using our approach. Our tool can assist Solidity developers to efficiently identify repetitive smart contracts in the existing Ethereum blockchain, as well as checking their contract against a known set of bugs. which can help to improve the users' confidence in the reliability of the contract. We optimize the implementations of SmartEmbed which is sufficient in supporting developers in real-time for practical uses. The Ethereum ecosystem as well as the individual Solidity developer can both benefit from our research.SmartEmbed website: http://www.smartembed.toolsDemo video: https://youtu.be/o9ylyOpYFq8Replication package: https://github.com/beyondacm/SmartEmbed",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3324884.3418918",
    "comment": ""
  },
  {
    "title": "Zimbabwean non-uptake of protective point-of-sale behaviours: is this a risk homeostasis response?",
    "author": "Musarurwa, Alfred and Renaud, Karen and Sch\\\"{u}rmann, Tim",
    "abstract": "In a world which is increasingly relying on debit and credit cards to effect transactions, people are entering PINs in a wide range of situations and contexts. We all know we ought to shield PIN entry, and check for skimmers if we are using magnetic stripe cards. Yet previous studies have found that a minority of card users shield their PINs at Points of Sale (PoS). Previous studies into the incidence of PIN shielding have taken place in Europe, with stable currencies and relative wealth.Zimbabwe, in 2019, presented us with a unique opportunity to carry out a replication study that is essentially a \"natural experiment\" i.e. we can study behaviours in interesting contexts which happen by chance, not by design. The context of interest is one where the country's currency is devaluing steeply, and creating a great deal of uncertainty and hardship. This occurred because Zimbabwe introduced a number of currency reforms in a short period of time. Protection Motivation Theory (PMT) suggests that people engage in a calculus based on their threat and coping appraisals. The devaluing currency ought to heighten threat appraisals (loss being much harder to bear) and the protective action's cost is relatively low (using a hand to shield a PIN). We ought, therefore, to see a higher incidence of protective behaviours in Zimbabwe.Our observation and interview study surprisingly found lower levels of PIN shielding at Points of Sale (PoS) than in previous European studies. We also found that those participants who did not take protective behaviours tended to know how to recover from card fraud.The low incidence we observed contradicted our PMT-based predictions. A possible explanation is that we are observing a risk homeostasis response, which suggests that having a \"safety net\" (being able to get your money back) might make people decide not to make the effort to take protective actions.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3422150",
    "comment": ""
  },
  {
    "title": "Designing a serious game: teaching developers to embed privacy into software systems",
    "author": "Arachchilage, Nalin Asanka Gamagedara and Hameed, Mumtaz Abdul",
    "abstract": "Software applications continue to challenge user privacy when users interact with them. Privacy practices (e.g. Data Minimisation (DM), Privacy by Design (PbD) or General Data Protection Regulation (GDPR)) and related \"privacy engineering\" methodologies exist and provide clear instructions for developers to implement privacy into software systems they develop that preserve user privacy. However, those practices and methodologies are not yet a common practice in the software development community. There has been no previous research focused on developing \"educational\" interventions such as serious games to enhance software developers' coding behaviour. Therefore, this research proposes a game design framework as an educational tool for software developers to improve (secure) coding behaviour, so they can develop privacy-preserving software applications that people can use. The elements of the proposed framework were incorporated into a gaming application scenario that enhances the software developers' coding behaviour through their motivation. The proposed work not only enables the development of privacy-preserving software systems but also helping the software development community to put privacy guidelines and engineering methodologies into practice.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3422149",
    "comment": ""
  },
  {
    "title": "Vulnerability discovery strategies used in software projects",
    "author": "Bhuiyan, Farzana Ahamed and Rahman, Akond and Morrison, Patrick",
    "abstract": "Malicious users can exploit undiscovered software vulnerabilities i.e., undiscovered weaknesses in software, to cause serious consequences, such as large-scale data breaches. A systematic approach that synthesizes strategies used by security testers can aid practitioners to identify latent vulnerabilities. The goal of this paper is to help practitioners identify software vulnerabilities by categorizing vulnerability discovery strategies using open source software bug reports. We categorize vulnerability discovery strategies by applying qualitative analysis on 312 OSS bug reports. Next, we quantify the frequency and evolution of the identified strategies by analyzing 1,632 OSS bug reports collected from five software projects spanning across 2009 to 2019. The five software projects are Chrome, Eclipse, Mozilla, OpenStack, and PHP.We identify four vulnerability discovery strategies: diagnostics, malicious payload construction, misconfiguration, and pernicious execution. For Eclipse and OpenStack, the most frequently used strategy is diagnostics, where security testers inspect source code and build/debug logs. For three web-related software projects namely, Chrome, Mozilla, and PHP, the most frequently occurring strategy is malicious payload construction i.e., creating malicious files, such as malicious certificates and malicious videos.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3422153",
    "comment": ""
  },
  {
    "title": "An informed consent model for managing the privacy paradox in smart buildings",
    "author": "Pathmabandu, Chehara and Grundy, John and Chhetri, Mohan Baruwal and Baig, Zubair",
    "abstract": "Smart Buildings are defined as the \"buildings of the future\" and use the latest Internet of Things (IoT) technologies to automate building operations and services. This is to both increase operational efficiency as well as maximize occupant comfort and environmental impact. However, these \"smart devices\" - typically used with default settings - also enable the capture and sharing of a variety of sensitive and personal data about the occupants. Given the non-intrusive nature of most IoT devices, individuals have little awareness of what data is being collected about them and what happens to it downstream. Even if they are aware, convenience overrides any privacy concerns, and they do not take sufficient steps to control the data collection, thereby exacerbating the privacy paradox. At the same time, IoT-based building automation systems are revealing highly sensitive insights about the building occupants by synthesizing data from multiple sources and this can be exploited by the device vendors and unauthorised third parties. To address the tension between privacy and convenience in an increasingly connected world, we propose a user-centric informed consent model to foster an accurate user discretion process for privacy choice in IoT-enabled smart buildings. The proposed model aims to (a) inform and increase user awareness about how their data is being collected and used, (b) provide fine-grained visibility into privacy compliance and infringement by IoT devices, and (c) recommend corrective actions through nudges (or soft notifications). We illustrate how our proposed consent model works through a use case scenario of a voice-activated smart office.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3422180",
    "comment": ""
  },
  {
    "title": "Characterizing co-located insecure coding patterns in infrastructure as code scripts",
    "author": "Bhuiyan, Farzana Ahamed and Rahman, Akond",
    "abstract": "Context: Insecure coding patterns (ICPs), such as hard-coded passwords can be inadvertently introduced in infrastructure as code (IaC) scripts, providing malicious users the opportunity to attack provisioned computing infrastructure. As performing code reviews is resource-intensive, a characterization of co-located ICPs, i.e., ICPs that occur together in a script can help practitioners to prioritize their review efforts and mitigate ICPs in IaC scripts. Objective: The goal of this paper is to help practitioners in prioritizing code review efforts for infrastructure as code (IaC) scripts by conducting an empirical study of co-located insecure coding patterns in IaC scripts. Methodology: We conduct an empirical study with 1613, 2764 and 2845 Puppet scripts respectively collected from three organizations namely, Mozilla, Openstack, and Wikimedia. We apply association rule mining to identify co-located ICPs in IaC scripts. Results: We observe 17.9\\%, 32.9\\%, and 26.7\\% of the scripts to include co-located ICPs respectively, for Mozilla, Openstack, and Wikimedia. The most frequent co-located ICP category is hard-coded secret and suspicious comment. Conclusion: Practitioners can prioritize code review efforts for IaC scripts by reviewing scripts that include co-located ICPs.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3422154",
    "comment": ""
  },
  {
    "title": "Exploring the requirements of pandemic awareness systems: a case study of COVID-19 using social media data",
    "author": "Shakeri, Esmaeil and Far, Behrouz H.",
    "abstract": "With the exponential growth of social media platforms like Twitter, a seemingly vast amount of data has become available for mining to draw conclusions about various topics, including awareness systems requirements. The exchange of health-related information on social media has been heralded as a new way to explore information-seeking behaviour during pandemics and design and develop awareness systems that address the public's information needs. Online datasets such as Twitter, Google Trends and Reddit have several advantages over traditional data sources, including real-time data availability, ease of access, and reduced cost.In this paper, to explore the pandemic awareness systems (PAS)' requirements, we utilize data from the large accessible database of tweets and Reddit's posts to explore the contextual patterns and temporal trends in Canadians' information-seeking behaviour during the COVID-19 pandemic. To validate our inferences and to understand how Google searches regarding COVID-19 were distributed throughout the course of the pandemic in Canada, we complement our Twitter and Reddit data with that collected through Google Trends, which tracks the popularity of specific search terms on Google. Our results show that Social media content contains useful technical information and can be used as a source to explore the requirements of pandemic awareness systems.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3422151",
    "comment": ""
  },
  {
    "title": "mHealth4U: designing for health and wellbeing self-management",
    "author": "Sellak, Hamza and Grobler, Marthie",
    "abstract": "Many individuals find it difficult to meet their personal health and wellbeing goals. To address this, mobile health (mHealth) apps aimed at improving individuals' diet, physical activity, sleep and mental health, are emerging at an increasing pace. These modern digital health interventions are a promising solution to promote behaviour change and help people maintain better health while controlling rising healthcare expenditures. However, the real-life effects of mHealth apps are often overshadowed by high dropout rates, with the loss of participants during the intervention seeming to be the rule rather than the exception. We designed the mHealth4U model as a sample-based study of user requirement and design preferences to enable more targeted health and wellbeing self-management. This model is aimed at understanding how life-changing digital health interventions can be designed and what software design components might increase consumers' acceptance, adherence and continuous engagement. We put forward three hypotheses in terms of designing an mHealth app that is consumer-centred: consumers prefer (1) self-management mHealth apps that target multiple key health and wellbeing dimensions, (2) intelligent recommendations, and (3) behaviour change support delivered precisely where, when and how it is needed most. We design the mHealth4U model around the 3U cyber security design components (user, usage and usability) and validate the hypotheses through a randomised sampling test with 114 participants. The results of this research will inform the design of a next-generation of digital health interventions capable of supporting the end-users to achieve the healthy lifestyle they deserve.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3422179",
    "comment": ""
  },
  {
    "title": "Reducing delay penalty of multiple concurrent software projects based on overtime planning",
    "author": "Zhang, Wei and Yang, Yun and Liu, Xiao",
    "abstract": "For software projects, significant delays can result in heavy penalty which may end up with project costs exceeding their budgets. As a consequence, employees, i.e., software developers, are often requested to work overtime in order to reduce or even eliminate the delays. By doing so, overtime payment may often be introduced and excessive overtime payment can also easily swallow company profit which may even lead to serious overdraft. Hence software manager needs to decide who should work overtime and how much overtime they would take in order to control the cost. This means that it is important to investigate how to reduce or eliminate the overall penalties by taking multiple concurrent software projects into account. In practice, there is normally a number of available employees with same or similar skills and domain knowledge from other similar concurrent projects. In addition, they have different skill proficiency. So rescheduling those employees with appropriate overtime may be feasible to find a solution which can reduce or eliminate the penalties of delayed software projects. Since this kind of scheduling is a typical NP-hard problem, a novel generic strategy is proposed to help select appropriate employees and determine how much overtime to be assigned to the delayed activities. The new strategy combines the features of Ant Colony Optimization algorithm and Tabu strategy and includes four rules to reduce the search space. A set of comprehensive generic experiments is carried out in order to evaluate the performance of the proposed strategy in a general manner. In addition, three real world software project instances are also utilized to evaluate our strategy. The results demonstrate that our strategy is effective which outperforms the other representative strategies which are applied successfully at software project scheduling.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3422152",
    "comment": ""
  },
  {
    "title": "Towards better understanding of agile teams through behavior change models",
    "author": "Madampe, Kashumi and Hoda, Rashina and Grundy, John",
    "abstract": "Agile software development welcomes changes throughout software development - but this implies that agile teams face several dilemmas. When to respond to a change; how to respond; how to manage the change. Our current understanding and support for agile teams during such change management is very limited. Psychological behavioral change models can be used to better understand the behavior of agile teams. Combining our understanding of agile teams and practices with a review of behavior change models, we propose several avenues for studying behavior and behavioral changes in agile teams. Our proposed interdisciplinary approach provides a much needed avenue to acknowledge and address the psychological and behavioral aspects of the humans central to the software engineering process, ultimately assisting with their well-being and productivity.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3422148",
    "comment": ""
  },
  {
    "title": "A vision to mitigate bioinformatics software development challenges",
    "author": "Rahman, Akond and Bhuiyan, Farzana Ahamed",
    "abstract": "Developers construct bioinformatics software to automate crucial analysis and research related to biological science. However, challenges while developing bioinformatics software can prohibit advancement in biological science research. Through a human-centric systematic analysis, we can identify challenges related to bioinformatics software development and envision future research directions. From our qualitative analysis with 221 Stack Overflow questions, we identify six categories of challenges: file operations, searching genetic entities, defect resolution, configuration management, sequence alignment, and translation of genetic information. To mitigate the identified challenges we envision three research directions that require synergies between bioinformatics and automated software engineering: (i) automated configuration recommendation using optimization algorithms, (ii) automated and comprehensive defect categorization, and (iii) intelligent task assistance with active and reinforcement learning.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3422155",
    "comment": ""
  },
  {
    "title": "A framework for the automatic execution of measurement-based experiments on Android devices",
    "author": "Malavolta, Ivano and Grua, Eoin Martino and Lam, Cheng-Yu and de Vries, Randy and Tan, Franky and Zielinski, Eric and Peters, Michael and Kaandorp, Luuk",
    "abstract": "Conducting measurement-based experiments is fundamental for assessing the quality of Android apps in terms of, e.g., energy consumption, CPU, and memory usage. However, orchestrating such experiments is not trivial as it requires large boilerplate code, careful setup of measurement tools, and the adoption of various empirical best practices scattered across the literature. All together, those factors are slowing down the scientific advancement and harming experiments' replicability in the mobile software engineering area.In this paper we present Android Runner (AR), a framework for automatically executing measurement-based experiments on native and web apps running on Android devices. In AR, an experiment is defined once in a descriptive fashion, and then its execution is fully automatic, customizable, and replicable. AR is implemented in Python and it can be extended with third-party profilers.AR has been used in more than 25 scientific studies primarily targeting performance and energy efficiency.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3422184",
    "comment": ""
  },
  {
    "title": "Ecosystem evolution analysis and trend prediction of projects in Android application framework",
    "author": "Fan, Zhehao and Feng, Zhiyong and Xue, Xiao and Chen, Shizhan and Wu, Hongyue",
    "abstract": "The application framework layer in the Android system consists of numerous project repositories, which rely on each other to form a co-evolving software ecosystem. Android's application framework layer provides many useful APIs to millions of Android Apps, so its evolution will affect the robustness and stability of Android Apps. Code dependency analysis technology is a common way to analyze software ecosystems. However, the code size of projects in the Android application framework layer is so huge that ordinary analysis methods are unacceptable due to the excessive resources required.In this paper, we propose an approach for evolution analysis and trend prediction based on the subgraph of code dependency network graph, in order to realize the effective analysis of large-scale software ecosystem. Based on the source code data of the application framework collected from AOSP, our proposed approach is verified. The prediction results of our model show that the average values of precision and recall are 90.0\\% and 90.4\\% respectively, which proves that our approach can well is effective.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3422185",
    "comment": ""
  },
  {
    "title": "KnowledgeZooClient: constructing knowledge graph for Android",
    "author": "Li, Li and Gao, Jun and Kong, Pingfan and Wang, Haoyu and Huang, Mengyu and Li, Yuan-Fang and Bissyand\\'{e}, Tegawend\\'{e} F.",
    "abstract": "In this work, we describe the design and implementation of a reusable tool named KnowledgeZooClient targeting the construction, as a crowd-sourced effort, of a knowledge graph for Android apps. KnowledgeZooClient is made up of two modules: (1) the Metadata Extraction Module (MEM), which aims at extracting metadata from Android apps and (2) the Metadata Integration Module (MIM) for importing and integrating extracted metadata into a graph database. The usefulness of KnowledgeZooClient is demonstrated via an exclusive knowledge graph called KnowledgeZoo, which contains information on over 500,000 apps already and still keeps growing. Interested users can already benefit from KnowledgeZoo by writing advanced search queries so as to collect targeted app samples.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3422187",
    "comment": ""
  },
  {
    "title": "Market-level analysis of government-backed COVID-19 contact tracing apps",
    "author": "Wang, Huiyi and Wang, Liu and Wang, Haoyu",
    "abstract": "To help curb the spread of the COVID-19 pandemic, governments and public health authorities around the world have launched a number of contact-tracing apps. Although contact tracing apps have received extensive attentions from the research community, no existing work has characterized the users' adoption of contact tracing apps from the app market level. In this work, we perform the first market-level analysis of contact tracing apps. We perform a longitudinal empirical study (over 4 months) of eight government-backed COVID-19 contact tracing apps in iOS app store. We first collect all the daily meta information (e.g., app updates, app rating, app comments, etc.) of these contact tracing apps from their launch to 2020-07-31. Then we characterize them from release practice, app popularity, and mobile users' feedback. Our study reveals various issues related to contact tracing apps from the users' perspective, hoping to help improve the quality of contact tracing apps and thus achieving a high level of adoption in the population.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3422186",
    "comment": ""
  },
  {
    "title": "Enforcing green code with Android lint",
    "author": "Goa\\\"{e}r, Olivier Le",
    "abstract": "Nowadays, energy efficiency is recognized as a core quality attribute of applications (apps) running on Android-powered devices constrained by their battery. Indeed, energy hogging apps are a liability to both the end-user and software developer. Yet, there are very few tools available to help developers increase the quality of their native code by ridding it of energy-related bugs. Android Studio is the official IDE for millions of developers worldwide and there's no better place to enforce green coding rules in everyday projects. Indeed, Android Studio provides a code scanning tool called Android lint that can be extended with lacking green checks in order to foster the design of more eco-responsible apps.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3422188",
    "comment": ""
  },
  {
    "title": "Boosting component-based synthesis with API usage knowledge",
    "author": "Liu, Jiaxin and Dong, Wei and Liu, Binbin",
    "abstract": "Component-based synthesis is one of the hottest research areas in automated software engineering. It aims to generate programs from a collection of components like Java library. However, the program space constituted by all the components in the library is fairly large, which leads to a vast number of candidate programs generated for a long time. The intractability of the program space affects the synthesis efficiency of the program and the size of the program generated. In this paper, we propose Itas, a framework of iterative program synthesis via API usage knowledge from the Internet, which can significantly improve the efficiency of program synthesis. Itas aims to constrain the program space by combining two main ideas. First, narrow down the program space from the outside via the guidance of API usage knowledge. Second, expand the program space from the inside via iterative strategy based on knowledge. For evaluation, we collect a set of programming tasks and compare our approach with a program synthesis tool on synthesizing these tasks. The experiment results show that Itas can significantly improve the efficiency of program synthesis, which can reduce the synthesis time by 97.1\\% than the original synthesizer.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3423370",
    "comment": ""
  },
  {
    "title": "Collective intelligence for smarter neural program synthesis",
    "author": "Wang, Daiyan and Dong, Wei and Zhang, Yating",
    "abstract": "We study the problem of automatically generating source code from different forms of user intents. Existing methods treating this problem as a language generating task of the neural network, known as Neural Program Synthesis (NPS). Most of these methods struggle with achieving high generating accuracy, one reason for that is the incompleteness and inaccuracy of user intents for a specific programming task. Inspired by the Swarm Intelligence (SI) and Collective Intelligence (CI) techniques, we proposed an automatic task-specific user intent merging framework combining both the bio-inspired algorithm in SI and CI merged from multiple developers. Empirically, we show that our approach is able to provide more accurate and adequate input for NPS, and our experiment on CI indicates that knowledge merging among isolated software developers in our approach has a significant influence on NPS.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3423371",
    "comment": ""
  },
  {
    "title": "Predicting software design patterns from text using neural embedding",
    "author": "Wijerathna, Laksri and Aleti, Aldeida",
    "abstract": "Software design patterns are solutions to common software problems that are proven to work adequately in particular scenarios. Deciding which design pattern to use for a given software problem often requires practical knowledge acquired with experience in a similar domain and can be highly subjective and error-prone. Further, for novice programmers, an automated approach would be of tremendous help as it would provide practical knowledge required for deciding which design pattern to use for a particular software problem. The majority of research in software design pattern prediction involves using software structure and features in determining which design pattern to implement. However, there are circumstances where software engineers would prefer to know which design pattern to be used by looking at the design problem during or before the implementation phase. Existing design pattern prediction tools cannot be utilized in this scenario due to the absence of code and class structures. To address this issue, this paper proposes a new approach that analyzes the context of the software problem from text and predicts a suitable design pattern for the given problem context using feature learning, neural embedding, and classification. We evaluate our approach on a case study from Stack Overflow with more than 66,000 questions that discuss problems and consequences related to 23 design patterns. Results show that our approach can predict design patterns from the text with 82\\% overall accuracy.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3423372",
    "comment": ""
  },
  {
    "title": "NLP-based enhancement of information security in ITO: a diffusion of innovation theory perspective",
    "author": "Bhatti, Baber Majid and Mubarak, Sameera and Nagalingam, Sev",
    "abstract": "Information technology outsourcing (ITO) has grown significantly in recent decades and is now over a USD trillion-dollar industry. Service provider organisations are striving to improve the efficiencies of their service deliveries. Natural language processing (NLP) provides an opportunity to bring efficiencies through automation in understanding and processing information. Since information security risk management (ISRM) in ITO is a growing concern of both, client and service provider organisations, they are adopting to improve ISRM in ITO using NLP. This paper explores those ISRM improvement scenarios. It also investigates the information security risks (ISRs) that result from the use of NLP in ITO and proposes strategies to manage those ISRs. To gain insights into the problem, a qualitative research approach is followed using the case study method. Six semi-structured interviews were conducted from participants in three organisations in the ICT industry, engaged in an ITO relationship. To the best of our knowledge, it is the first study to investigate the use of NLP for enhancing ISRM in ITO.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3423373",
    "comment": ""
  },
  {
    "title": "AutoEPRS-20: extracting business process redesign suggestions from natural language text",
    "author": "Mustansir, Amina and Shahzad, Khurram and Malik, Muhammad Kamran",
    "abstract": "In this paper, we have defined an NLP task, for the automatic extraction of business process redesign suggestions from natural language text. In particular, we have employed a systematic protocol to define the task, which is composed of three elements and three sub-tasks. The elements are: a) a real-world process model, b) actual feedback in natural language text, and c) three-level classification of the feedback. The task is composed of two binary and one multi-class classification sub-tasks. The evaluation of the AutoEPRS-20 task is performed using six traditional supervised learning techniques. The results show that the third sub-task is more challenging that the two binary sub-tasks.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3423374",
    "comment": ""
  },
  {
    "title": "Emotion detection in Roman Urdu text using machine learning",
    "author": "Majeed, Adil and Mujtaba, Hasan and Beg, Mirza Omer",
    "abstract": "Emotion detection is playing a very important role in our life. People express their emotions in different ways i.e face expression, gestures, speech, and text. This research focuses on detecting emotions from the Roman Urdu text. Previously, A lot of work has been done on different languages for emotion detection but there is limited work done in Roman Urdu. Therefore, there is a need to explore Roman Urdu as it is the most widely used language on social media platforms for communication. One major issue for the Roman Urdu is the absence of benchmark corpora for emotion detection from text because language assets are essential for different natural language processing (NLP) tasks. There are many useful applications of the emotional analysis of a text such as improving the quality of products, dialog systems, investment trends, mental health. In this research, to focus on the emotional polarity of the Roman Urdu sentence we develop a comprehensive corpus of 18k sentences that are gathered from different domains and annotate it with six different classes. We applied different baseline algorithms like KNN, Decision tree, SVM, and Random Forest on our corpus. After experimentation and evaluation, the results showed that the SVM model achieves a better F-measure score.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3423375",
    "comment": ""
  },
  {
    "title": "Mapping textual feedback to process model elements",
    "author": "Ahmed, Sanam and Mustansir, Amina",
    "abstract": "In this paper, we have proposed novel concept of mapping natural language customer feedback text to relevant business process model elements. Customer feedback mapped over business process model will provide augmented business process having customer perception. More specifically, in this work, we have proposed systematic approach for mapping feedback comment to relevant process model elements which comprises a)process model generation, b) preparation of real-world customer feedback corpus, c) BPRI framework based mapping guidelines and d) first novel human annotated customer feedback process model element mapping dataset. We have evaluated the effectiveness of six traditional text similarity measures for automatic mapping of customer feedback to process model elements. Based on the results, we concluded that automatic mapping identification is challenging task as six traditional similarity measures resulted zero recall score.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3423376",
    "comment": ""
  },
  {
    "title": "Roman Urdu reviews dataset for aspect based opinion mining",
    "author": "Zahid, Rabail and Idrees, Muhammad Owais and Mujtaba, Hasan and Beg, Mirza Omer",
    "abstract": "Social media, today, demonstrates the rapid growth of modern society as it becomes the main platform for Internet users to communicate and express themselves. People around the world, use a number of devices and resources to access the Internet, set up social networks, conduct online business, e-commerce, e-surveys, etc. Currently, social media is not only a technology that provides information to consumers, it also encourages users to connect and share their views and perspectives. It leads to an increase in inspiration towards Opinion Mining (OM), which is important for both customers and companies in making decisions. Individuals like to see the opinions provided by other customers about a particular product or a service. Companies need to analyze their customer's feedback to strengthen their business decisions. A lot of research has been performed in various languages in the field of Aspect Based OM (ABOM). However, there are still certain languages that need to be explored, such as Roman Urdu (RU). This paper presents a proposed reviews data-set (a RU data-set) of mobile reviews that has been manually annotated with multi-aspect sentiment labels at the sentence-level. It presents base-line results using different Machine Learning (ML) algorithms. The results demonstrate 71\\% F1-score for aspect detection and 64\\% for aspect-based polarity.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3423377",
    "comment": ""
  },
  {
    "title": "Energy efficiency in robotics software: a systematic literature review",
    "author": "Swanborn, Stan and Malavolta, Ivano",
    "abstract": "Nowadays, robots are widely used in many areas of our lifes, such as autonomous storage, self-driving vehicles, drones, industrial automation, etc. Energy is a critical factor for robotic systems, especially for mobile robots where energy is a finite resource (e.g., surveillance autonomous rovers). Since software is becoming the central focus of modern robotic systems, it is important to understand how it influences the energy consumption of the entire system. However, there is no systematic study of the state of the art in energy efficiency of robotics software that could guide research or practitioners in finding solutions and tools to develop robotic systems with energy efficiency in mind.The goal of this paper is to present a review of existing research on energy efficiency in robotics software. Specifically, we investigate on (i) the used metrics for energy efficiency, (ii) the application domains within the robotics area covered by research on energy efficiency, (iii) the identified major energy consumers within a robotic system, (iv) how existing approaches are evaluated, (v) the used energy models, (vi) the techniques supporting the development of energy-efficient robotics software, and (vii) which quality attributes tend to be traded off when dealing with energy efficiency in robotics. We also provide a replication package to assess, extend, and/or replicate the study.The results of this work can guide researchers and practitioners in robotics and software engineering in better reasoning and contributing to energy-efficient robotics software.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3422997",
    "comment": ""
  },
  {
    "title": "Gesture driven smart home solution for bedridden people",
    "author": "Jayaweera, Nimna and Gamage, Binura and Samaraweera, Mihiri and Liyanage, Sachintha and Lokuliyana, Shashika and Kuruppu, Thilmi",
    "abstract": "Conversion of ordinary houses into smart homes has been a rising trend for past years. Smart house development is based on the enhancement of the quality of the daily activities of normal people. But many smart homes have not been designed in a way that is user friendly for differently-abled people such as immobile, bedridden (disabled people with at least one hand movable). Due to negligence and forgetfulness, there are cases where the electrical devices are left switched on, regardless of any necessity. It is one of the most occurred examples of domestic energy wastage. To overcome those challenges, this research represents the improved smart home design: MobiGO that uses cameras to capture gestures, smart sockets to deliver gesture-driven outputs to home appliances, etc. The camera captures the gestures done by the user and the system processes those images through advanced gesture recognition and image processing technologies. The commands relevant to the gesture are sent to the specific appliance through a specific IoT device attached to them. The basic literature survey content, which contains technical words, is analyzed using Deep Learning, Convolutional Neural Network (CNN), Image Processing, Gesture recognition, smart homes, IoT. Finally, the authors conclude that the MobiGO solution proposes a smart home system that is safer and easier for people with disabilities.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3422998",
    "comment": ""
  },
  {
    "title": "Energy wars - Chrome vs. Firefox: which browser is more energy efficient?",
    "author": "de Macedo, Jo\\~{a}o and Alo\\'{\\i}sio, Jo\\~{a}o and Gon\\c{c}alves, Nelson and Pereira, Rui and Saraiva, Jo\\~{a}o",
    "abstract": "This paper presents a preliminary study on the energy consumption of two popular web browsers. In order to properly measure the energy consumption of both environments, we simulate the usage of various applications, which the goal to mimic typical user interactions and usage.Our preliminary results show interesting findings based on observation, such as what type of interactions generate high peaks of energy consumption, and which browser is overall the most efficient. Our goal with this preliminary study is to show to users how very different the efficiency of web browsers can be, and may serve with advances in this area of study.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3423000",
    "comment": ""
  },
  {
    "title": "Sustainability in migrating workloads to public clouds",
    "author": "Pathania, Priyavanshi and Mithani, Rajan Dilavar",
    "abstract": "In recent times, there has been a considerable increase in Cloud-Based applications and infrastructure. This has led to quicker innovations, agile businesses, availability of new services over the internet, improved collaboration, and better security. With the growth of new technologies like blockchain, quantum computing, mobility-focused applications, and edge computing, there has been an increased interest in adopting cloud services. In this paper, we highlight the different sustainability metrics and benefits while migrating workloads from the on-prem data center to the public clouds. Also, the clouds are elastic, scalable, cost-efficient, robust, and overall a better alternative to host the client applications and services. We present how the major Cloud Service Providers (CSPs) are continuously working on improving their infrastructure for a more energy efficient cloud. But with so many factors like the cost of cloud services, the location of the data center to name a few, it becomes quite a tedious task for the clients to select a cloud service provider when moving from their on-premise data center(s). Hence, we also briefly propose our solution that we are currently working on. The final goal is to have a cross-platform advisory that based on a wide-range of client-based inputs and a rich repository of current energy efficient clouds and their sustainability metrics, aims to provide them a detailed recommendation about their preferred cloud service provider. In case the client does not provide any such preference, the advisory should also recommend an ideal cloud service provider for their particular workload. This suggested action will be able to fulfill the client's constraints as well as provide them an energy efficient cloud along with a sustainability score. This score is indicative of how much improvement in the energy consumed and carbon footprint can be achieved through this migration to the suggested cloud.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3423001",
    "comment": ""
  },
  {
    "title": "E-Debitum: managing software energy debt",
    "author": "Maia, Daniel and Couto, Marco and Saraiva, Jo\\~{a}o and Pereira, Rui",
    "abstract": "This paper extends previous work on the concept of a new software energy metric: Energy Debt. This metric is a reflection on the implied cost, in terms of energy consumption over time, of choosing an energy flawed software implementation over a more robust and efficient, yet time consuming, approach.This paper presents the implementation a SonarQube tool called E-Debitum which calculates the energy debt of Android applications throughout their versions. This plugin uses a robust, well defined, and extendable smell catalog based on current green software literature, with each smell defining the potential energy savings. To conclude, an experimental validation of E-Debitum was executed on 3 popular Android applications with various releases, showing how their energy debt fluctuated throughout releases.",
    "year": "2020",
    "publication": "ase",
    "link": "https://doi.org/10.1145/3417113.3422999",
    "comment": ""
  }
]